This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: .specstory/**/*.md, .venv/**, _private/**, CLEANUP.txt, **/*.json, *.lock
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/
  rules/
    0project.mdc
    cleanup.mdc
    filetree.mdc
    quality.mdc
.github/
  workflows/
    push.yml
    release.yml
docs/
  research/
    people-api.md
    web-search-api.md
examples/
  check_llm_plugins.py
  example_people_script.py
  example_serp_script.py
  funchain_example.py
  run_action_config_example.py
src/
  twat_llm/
    __init__.py
    mallmo.py
    twat_llm.py
tests/
  test_mallmo.py
  test_twat_llm.py
.gitignore
.pre-commit-config.yaml
CHANGELOG.md
cleanup.py
LICENSE
llms.txt
LOG.md
pyproject.toml
README.md
TODO.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/0project.mdc">
---
description: About this project
globs:
---
# About this project

`twat-fs` is a file system utility library focused on robust and extensible file upload capabilities with multiple provider support. It provides:

- Multi-provider upload system with smart fallback (catbox.moe default, plus Dropbox, S3, etc.)
- Automatic retry for temporary failures, fallback for permanent ones
- URL validation and clean developer experience with type hints
- Simple CLI: `python -m twat_fs upload_file path/to/file.txt`
- Easy installation: `uv pip install twat-fs` (basic) or `uv pip install 'twat-fs[all,dev]'` (all features)

## Development Notes
- Uses `uv` for Python package management
- Quality tools: ruff, mypy, pytest
- Clear provider protocol for adding new storage backends
- Strong typing and runtime checks throughout
</file>

<file path=".cursor/rules/cleanup.mdc">
---
description: Run `cleanup.py` script before and after changes
globs:
---
Before you do any changes or if I say "cleanup", run the `cleanup.py update` script in the main folder. Analyze the results, describe recent changes in @LOG.md and edit @TODO.md to update priorities and plan next changes. PERFORM THE CHANGES, then run the `cleanup.py status` script and react to the results.

When you edit @TODO.md, lead in lines with empty GFM checkboxes if things aren't done (`- [ ] `) vs. filled (`- [x] `) if done.
</file>

<file path=".cursor/rules/quality.mdc">
---
description: Quality
globs:
---
- **Verify Information**: Always verify information before presenting it. Do not make assumptions or speculate without clear evidence.
- **No Apologies**: Never use apologies.
- **No Whitespace Suggestions**: Don't suggest whitespace changes.
- **No Inventions**: Don't invent major changes other than what's explicitly requested.
- **No Unnecessary Confirmations**: Don't ask for confirmation of information already provided in the context.
- **Preserve Existing Code**: Don't remove unrelated code or functionalities. Pay attention to preserving existing structures.
- **No Implementation Checks**: Don't ask the user to verify implementations that are visible in the provided context.
- **No Unnecessary Updates**: Don't suggest updates or changes to files when there are no actual modifications needed.
- **No Current Implementation**: Don't show or discuss the current implementation unless specifically requested.
- **Use Explicit Variable Names**: Prefer descriptive, explicit variable names over short, ambiguous ones to enhance code readability.
- **Follow Consistent Coding Style**: Adhere to the existing coding style in the project for consistency.
- **Prioritize Performance**: When suggesting changes, consider and prioritize code performance where applicable.
- **Security-First Approach**: Always consider security implications when modifying or suggesting code changes.
- **Test Coverage**: Suggest or include appropriate unit tests for new or modified code.
- **Error Handling**: Implement robust error handling and logging where necessary.
- **Modular Design**: Encourage modular design principles to improve code maintainability and reusability.
- **Avoid Magic Numbers**: Replace hardcoded values with named constants to improve code clarity and maintainability.
- **Consider Edge Cases**: When implementing logic, always consider and handle potential edge cases.
- **Use Assertions**: Include assertions wherever possible to validate assumptions and catch potential errors early.
</file>

<file path=".github/workflows/push.yml">
name: Build & Test
on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:
permissions:
  contents: write
  id-token: write
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"
      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"
      - name: Run Mypy type checking
        run: |
          uv pip install --system mypy ".[dev]" # Ensure mypy and any type stubs from dev deps are present
          uv run mypy src/twat_llm tests
  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}
      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"
      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/twat_llm --cov=tests tests/
      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml
  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true
      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs
      - name: Build distributions
        run: uv run python -m build --outdir dist
      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5
</file>

<file path=".github/workflows/release.yml">
name: Release
on:
  push:
    tags: ["v*"]
permissions:
  contents: write
  id-token: write
jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/twat-llm
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true
      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs
      - name: Build distributions
        run: uv run python -m build --outdir dist
      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)
      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}
      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

<file path="examples/check_llm_plugins.py">
"""Check which LLM plugins are installed and display results in a table."""
⋮----
@dataclass
class PackageResult
⋮----
"""Result of checking a package installation."""
name: str
status: str
duration: float
details: str = ""
def check_package(package: str) -> PackageResult
⋮----
"""Check if a package can be imported.
    Args:
        package: Name of package to check
    Returns:
        PackageResult containing status and timing information
    """
start_time = time.time()
⋮----
duration = time.time() - start_time
⋮----
def create_results_table(results: Sequence[PackageResult]) -> Table
⋮----
"""Create formatted table of package check results.
    Args:
        results: Sequence of package check results
    Returns:
        Rich Table object with formatted results
    """
table = Table(show_header=True, header_style="bold magenta")
⋮----
status_color = "green" if result.status == "SUCCESS" else "red"
⋮----
def check_llm_plugins(packages: Sequence[str] | None = None) -> None
⋮----
"""Check which LLM plugins are installed and display results in a table.
    Args:
        packages: Optional sequence of package names to check. If None, checks default LLM plugins.
    """
⋮----
packages = [
results = [check_package(pkg) for pkg in packages]
table = create_results_table(results)
console = Console()
</file>

<file path="examples/run_action_config_example.py">
"""
Example script to demonstrate using ActionConfig with process_data from twat_llm.
This script shows how to:
- Configure person enrichment and web search actions.
- Execute these actions using `process_data`.
- Handle API keys via environment variables (loaded by ApiKeySettings).
Requirements:
- Ensure the `twat-llm` package is installed.
- For person enrichment: Set PROXYCURL_API_KEY environment variable.
- For web search: Set SEARCH_API_KEY (e.g., for Brave Search) environment variable.
  (Create a .env file in your project root or set them in your shell)
  Example .env:
  PROXYCURL_API_KEY="your_proxycurl_key"
  SEARCH_API_KEY="your_search_key"
- Ensure LLM models are configured for the `llm` library for summarization.
"""
⋮----
# Configure basic logging for the example
⋮----
logger = logging.getLogger(__name__)
⋮----
ApiKeySettings, # To illustrate API key loading messages
⋮----
def main_demonstration()
⋮----
"""Demonstrates person enrichment and web search actions."""
⋮----
# --- Example 1: Enrich Person ---
⋮----
# For this example, using a public profile.
# Replace with a relevant LinkedIn URL for your tests.
enrich_params = PersonEnrichmentParams(
⋮----
linkedin_profile_url="https://www.linkedin.com/in/satyanadella/", # Example
name="Satya Nadella", # Optional, for context
# email="satya.nadella@example.com" # Optional
⋮----
enrich_config = ActionConfig(
⋮----
enrich_result = process_data(enrich_config, debug=True)
⋮----
print("\nPerson Enrichment Succeeded:") # noqa: T201
print(f"  Summary: {enrich_result.get('summary')}") # noqa: T201
⋮----
print(f"\nPerson Enrichment Failed: {enrich_result.get('details')}") # noqa: T201
⋮----
print(f"\nPerson Enrichment Error: {e}") # noqa: T201
⋮----
print(f"\nUnexpected Person Enrichment Error: {e}") # noqa: T201
# --- Example 2: Web Search ---
⋮----
search_params = WebSearchParams(
search_config = ActionConfig(
⋮----
search_result = process_data(search_config, debug=True)
⋮----
print("\nWeb Search Succeeded:") # noqa: T201
print(f"  Query: {search_result.get('query')}") # noqa: T201
print(f"  Summary: {search_result.get('summary')}") # noqa: T201
⋮----
print(f"\nWeb Search Failed: {search_result.get('details')}") # noqa: T201
⋮----
print(f"\nWeb Search Error: {e}") # noqa: T201
⋮----
print(f"\nUnexpected Web Search Error: {e}") # noqa: T201
⋮----
# Illustrate API key loading messages (optional, for clarity)
loaded_api_keys = ApiKeySettings()
⋮----
print( # noqa: T201
⋮----
print("Also ensure your `llm` library is configured with models for summarization.") # noqa: T201
# To run this example:
# 1. Make sure twat-llm is installed (`pip install .` or `pip install twat-llm`)
# 2. Create a .env file in the same directory as this script (or project root)
#    with your API keys:
#    PROXYCURL_API_KEY="your_proxycurl_key_here"
#    SEARCH_API_KEY="your_search_api_key_here" (e.g., Brave Search API token)
# 3. Run `python examples/run_action_config_example.py`
</file>

<file path=".gitignore">
*_autogen/
.DS_Store
__version__.py
__pycache__/
_Chutzpah*
_deps
_NCrunch_*
_pkginfo.txt
_Pvt_Extensions
_ReSharper*/
_TeamCity*
_UpgradeReport_Files/
!?*.[Cc]ache/
!.axoCover/settings.json
!.vscode/extensions.json
!.vscode/launch.json
!.vscode/settings.json
!.vscode/tasks.json
!**/[Pp]ackages/build/
!Directory.Build.rsp
.*crunch*.local.xml
.axoCover/*
.builds
.cr/personal
.fake/
.history/
.ionide/
.localhistory/
.mfractor/
.ntvs_analysis.dat
.paket/paket.exe
.sass-cache/
.vs/
.vscode
.vscode/*
.vshistory/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
[Bb]in/
[Bb]uild[Ll]og.*
[Dd]ebug/
[Dd]ebugPS/
[Dd]ebugPublic/
[Ee]xpress/
[Ll]og/
[Ll]ogs/
[Oo]bj/
[Rr]elease/
[Rr]eleasePS/
[Rr]eleases/
[Tt]est[Rr]esult*/
[Ww][Ii][Nn]32/
*_h.h
*_i.c
*_p.c
*_wpftmp.csproj
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl
*- [Bb]ackup.rdl
*.[Cc]ache
*.[Pp]ublish.xml
*.[Rr]e[Ss]harper
*.a
*.app
*.appx
*.appxbundle
*.appxupload
*.aps
*.azurePubxml
*.bim_*.settings
*.bim.layout
*.binlog
*.btm.cs
*.btp.cs
*.build.csdef
*.cab
*.cachefile
*.code-workspace
*.coverage
*.coveragexml
*.d
*.dbmdl
*.dbproj.schemaview
*.dll
*.dotCover
*.DotSettings.user
*.dsp
*.dsw
*.dylib
*.e2e
*.exe
*.gch
*.GhostDoc.xml
*.gpState
*.ilk
*.iobj
*.ipdb
*.jfm
*.jmconfig
*.la
*.lai
*.ldf
*.lib
*.lo
*.log
*.mdf
*.meta
*.mm.*
*.mod
*.msi
*.msix
*.msm
*.msp
*.ncb
*.ndf
*.nuget.props
*.nuget.targets
*.nupkg
*.nvuser
*.o
*.obj
*.odx.cs
*.opendb
*.opensdf
*.opt
*.out
*.pch
*.pdb
*.pfx
*.pgc
*.pgd
*.pidb
*.plg
*.psess
*.publishproj
*.publishsettings
*.pubxml
*.pyc
*.rdl.data
*.rptproj.bak
*.rptproj.rsuser
*.rsp
*.rsuser
*.sap
*.sbr
*.scc
*.sdf
*.sln.docstates
*.sln.iml
*.slo
*.smod
*.snupkg
*.so
*.suo
*.svclog
*.tlb
*.tlh
*.tli
*.tlog
*.tmp
*.tmp_proj
*.tss
*.user
*.userosscache
*.userprefs
*.vbp
*.vbw
*.VC.db
*.VC.VC.opendb
*.VisualState.xml
*.vsp
*.vspscc
*.vspx
*.vssscc
*.xsd.cs
**/[Pp]ackages/*
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.HTMLClient/GeneratedArtifacts
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
*~
~$*
$tf/
AppPackages/
artifacts/
ASALocalRun/
AutoTest.Net/
Backup*/
BenchmarkDotNet.Artifacts/
bld/
BundleArtifacts/
ClientBin/
cmake_install.cmake
CMakeCache.txt
CMakeFiles
CMakeLists.txt.user
CMakeScripts
CMakeUserPresets.json
compile_commands.json
coverage*.info
coverage*.json
coverage*.xml
csx/
CTestTestfile.cmake
dlldata.c
DocProject/buildhelp/
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/*.HxC
DocProject/Help/*.HxT
DocProject/Help/html
DocProject/Help/Html2
ecf/
FakesAssemblies/
FodyWeavers.xsd
Generated_Code/
Generated\ Files/
healthchecksdb
install_manifest.txt
ipch/
Makefile
MigrationBackup/
mono_crash.*
nCrunchTemp_*
node_modules/
nunit-*.xml
OpenCover/
orleans.codegen.cs
Package.StoreAssociation.xml
paket-files/
project.fragment.lock.json
project.lock.json
publish/
PublishScripts/
rcf/
ScaffoldingReadMe.txt
ServiceFabricBackup/
StyleCopReport.xml
Testing
TestResult.xml
UpgradeLog*.htm
UpgradeLog*.XML
x64/
x86/
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Distribution / packaging
!dist/.gitkeep

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
.ruff_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
__version__.py
_private

# Generated files
CLEANUP.txt
REPO_CONTENT.txt
</file>

<file path=".pre-commit-config.yaml">
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.5.0 # Updated Ruff version
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.11.0 # Use a recent version of mypy
    hooks:
      - id: mypy
        args: [--install-types, --non-interactive]
        # additional_dependencies are managed by hatch/uv in the main env
        # but for pre-commit, specify necessary types here if not picked up automatically
        # For example:
        # additional_dependencies: ['types-Pillow', 'types-tenacity']
        # However, it's often better to rely on mypy finding these from the project's venv
        # if pre-commit is run within an activated venv or if mypy is configured project-wide.
        # For now, keeping it simple. MyPy will use project's pyproject.toml for config.
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.6.0 # Updated pre-commit-hooks version
    hooks:
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf]
      - id: end-of-file-fixer
      - id: trailing-whitespace
</file>

<file path="CHANGELOG.md">
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased] - YYYY-MM-DD

### Added
- This `CHANGELOG.md` file to track changes.
- `PLAN.md` outlining the streamlining strategy for v1.0 MVP.
- `TODO.md` providing a task list for the streamlining effort.

### Changed
- Condensed `docs/research/people-api.md` to a summary focused on Proxycurl integration for MVP.
- Condensed `docs/research/web-search-api.md` to a summary focused on Brave Search API (as an example) integration for MVP.
- Rewrote `examples/example_people_script.py` to use `process_data`, `ActionConfig`, and `PersonEnrichmentParams` from `twat_llm`, and to guide on API key environment setup.
- Rewrote `examples/example_serp_script.py` to use `process_data`, `ActionConfig`, and `WebSearchParams` from `twat_llm`, and to guide on API key environment setup.
- Updated `examples/funchain_example.py` to use the correct import path for `ask_chain`, added a `main()` function, logging, and print statements for clarity.
- Moved demonstration logic from `main()` in `src/twat_llm/twat_llm.py` to a new example script `examples/run_action_config_example.py`.
- Updated `src/twat_llm/__init__.py` to export key public interfaces from `twat_llm.py` and `mallmo.py`.
- Streamlined dependencies in `pyproject.toml` for MVP by removing `opencv-python-headless`.
- Adjusted `src/twat_llm/mallmo.py` to remove video processing capabilities (dependent on `opencv-python-headless`), focusing only on image media types for MVP.

### Removed
- Removed detailed internal review files from `docs/research/`:
    - `review-copilot.md`
    - `review-cursor.md`
    - `review-o3.md`
    - `review-trae.md`
- Removed `docs/research/people-api-tldr.md` to reduce redundancy and focus documentation on implemented MVP features.

### Fixed
-   *(Placeholder for bug fixes during streamlining)*

## [v0.0.1] - 2025-02-15

### Added

- Initial release of the project
- Created `mallmo.py` with LLM interaction functionality:
  - Core `ask()` function for LLM prompting with media support
  - `ask_batch()` for parallel processing of multiple prompts
  - `ask_chain()` for chaining multiple prompts or functions
  - Support for multiple fallback models
  - Media file processing (images and video frames)
  - CLI interface
- Added `llm_plugins.py` for checking installed LLM plugins (Note: This file is `examples/check_llm_plugins.py` in current structure)
- Created `funchain.py` as a simple example of chain functionality (Note: This file is `examples/funchain_example.py` in current structure)
- Basic project structure with Python package setup

### Changed

- Moved `twat_llm.py` to `src/twat_llm/` directory
- Enhanced code quality with type hints and modern Python features
- Improved error handling and logging

### Fixed

- Added missing newline at end of files
- Updated `.gitignore` to exclude `_private` directory

---
*Note: The initial content for v0.0.1 is based on the existing `LOG.md`. `LOG.md` might be superseded by this `CHANGELOG.md` or integrated into it.*
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="llms.txt">
This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/
  rules/
    0project.mdc
    cleanup.mdc
    filetree.mdc
    quality.mdc
.github/
  workflows/
    push.yml
    release.yml
.specstory/
  history/
    .what-is-this.md
    reviewing-the-reviewers-a-critical-analysis.md
    writeup-review-and-rating-analysis.md
docs/
  research/
    people-api-tldr.md
    people-api.md
    review-copilot.md
    review-cursor.md
    review-o3.md
    review-trae.md
    web-search-api.md
examples/
  check_llm_plugins.py
  example_people_script.py
  example_serp_script.py
  funchain_example.py
src/
  twat_llm/
    __init__.py
    mallmo.py
    twat_llm.py
tests/
  test_mallmo.py
  test_twat_llm.py
.gitignore
.pre-commit-config.yaml
cleanup.py
LICENSE
LOG.md
pyproject.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/0project.mdc">
---
description: About this project
globs:
---
# About this project

`twat-fs` is a file system utility library focused on robust and extensible file upload capabilities with multiple provider support. It provides:

- Multi-provider upload system with smart fallback (catbox.moe default, plus Dropbox, S3, etc.)
- Automatic retry for temporary failures, fallback for permanent ones
- URL validation and clean developer experience with type hints
- Simple CLI: `python -m twat_fs upload_file path/to/file.txt`
- Easy installation: `uv pip install twat-fs` (basic) or `uv pip install 'twat-fs[all,dev]'` (all features)

## Development Notes
- Uses `uv` for Python package management
- Quality tools: ruff, mypy, pytest
- Clear provider protocol for adding new storage backends
- Strong typing and runtime checks throughout
</file>

<file path=".cursor/rules/cleanup.mdc">
---
description: Run `cleanup.py` script before and after changes
globs: 
---
Before you do any changes or if I say "cleanup", run the `cleanup.py update` script in the main folder. Analyze the results, describe recent changes in @LOG.md and edit @TODO.md to update priorities and plan next changes. PERFORM THE CHANGES, then run the `cleanup.py status` script and react to the results.

When you edit @TODO.md, lead in lines with empty GFM checkboxes if things aren't done (`- [ ] `) vs. filled (`- [x] `) if done.
</file>

<file path=".cursor/rules/filetree.mdc">
---
description: File tree of the project
globs: 
---
[ 896]  .
├── [  64]  .benchmarks
├── [  96]  .cursor
│   └── [ 224]  rules
│       ├── [ 821]  0project.mdc
│       ├── [ 516]  cleanup.mdc
│       ├── [1.7K]  filetree.mdc
│       └── [2.0K]  quality.mdc
├── [  96]  .github
│   └── [ 128]  workflows
│       ├── [2.7K]  push.yml
│       └── [1.4K]  release.yml
├── [3.5K]  .gitignore
├── [ 470]  .pre-commit-config.yaml
├── [  96]  .specstory
│   └── [ 160]  history
│       ├── [2.0K]  .what-is-this.md
│       ├── [4.0K]  reviewing-the-reviewers-a-critical-analysis.md
│       └── [4.4K]  writeup-review-and-rating-analysis.md
├── [ 987]  CLEANUP.txt
├── [1.0K]  LICENSE
├── [1.5K]  LOG.md
├── [ 706]  README.md
├── [173K]  REPO_CONTENT.txt
├── [   7]  VERSION.txt
├── [ 13K]  cleanup.py
├── [ 160]  dist
├── [ 384]  docs
│   ├── [6.6K]  people-api-tldr.md
│   ├── [ 67K]  people-api.md
│   ├── [ 189]  people.py
│   ├── [2.6K]  review-copilot.md
│   ├── [3.1K]  review-cursor.md
│   ├── [3.1K]  review-o3.md
│   ├── [2.3K]  review-trae.md
│   ├── [ 350]  serp.py
│   └── [ 50K]  web-search-api.md
├── [ 426]  package.toml
├── [7.2K]  pyproject.toml
├── [ 224]  src
│   ├── [ 279]  funchain.py
│   ├── [3.2K]  llm_plugins.py
│   ├── [8.7K]  mallmo.py
│   └── [ 128]  twat_llm
│       └── [1.6K]  twat_llm.py
└── [ 128]  tests
    └── [ 148]  test_twat_llm.py

13 directories, 34 files
</file>

<file path=".cursor/rules/quality.mdc">
---
description: Quality
globs: 
---
- **Verify Information**: Always verify information before presenting it. Do not make assumptions or speculate without clear evidence.
- **No Apologies**: Never use apologies.
- **No Whitespace Suggestions**: Don't suggest whitespace changes.
- **No Inventions**: Don't invent major changes other than what's explicitly requested.
- **No Unnecessary Confirmations**: Don't ask for confirmation of information already provided in the context.
- **Preserve Existing Code**: Don't remove unrelated code or functionalities. Pay attention to preserving existing structures.
- **No Implementation Checks**: Don't ask the user to verify implementations that are visible in the provided context.
- **No Unnecessary Updates**: Don't suggest updates or changes to files when there are no actual modifications needed.
- **No Current Implementation**: Don't show or discuss the current implementation unless specifically requested.
- **Use Explicit Variable Names**: Prefer descriptive, explicit variable names over short, ambiguous ones to enhance code readability.
- **Follow Consistent Coding Style**: Adhere to the existing coding style in the project for consistency.
- **Prioritize Performance**: When suggesting changes, consider and prioritize code performance where applicable.
- **Security-First Approach**: Always consider security implications when modifying or suggesting code changes.
- **Test Coverage**: Suggest or include appropriate unit tests for new or modified code.
- **Error Handling**: Implement robust error handling and logging where necessary.
- **Modular Design**: Encourage modular design principles to improve code maintainability and reusability.
- **Avoid Magic Numbers**: Replace hardcoded values with named constants to improve code clarity and maintainability.
- **Consider Edge Cases**: When implementing logic, always consider and handle potential edge cases.
- **Use Assertions**: Include assertions wherever possible to validate assumptions and catch potential errors early.
</file>

<file path=".github/workflows/push.yml">
name: Build & Test

on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"

      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"

      - name: Run Mypy type checking
        run: |
          uv pip install --system mypy ".[dev]" # Ensure mypy and any type stubs from dev deps are present
          uv run mypy src/twat_llm tests

  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}

      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"

      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/twat_llm --cov=tests tests/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml

  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5
</file>

<file path=".github/workflows/release.yml">
name: Release

on:
  push:
    tags: ["v*"]

permissions:
  contents: write
  id-token: write

jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/twat-llm
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

<file path=".specstory/history/.what-is-this.md">
# SpecStory Artifacts Directory

This directory is automatically created and maintained by the SpecStory extension to preserve your Cursor composer and chat history.

## What's Here?

- `.specstory/history`: Contains markdown files of your AI coding sessions
- Each file represents a separate chat or composer session
- Files are automatically updated as you work

## Valuable Uses

- Capture: Keep your context window up-to-date when starting new Chat/Composer sessions via @ references
- Search: For previous prompts and code snippets
- Learn: Meta-analyze your patterns and learn from your past experiences

## Version Control

We recommend keeping this directory under version control to maintain a history of your AI interactions. However, if you prefer not to version these files, you can exclude them by adding this to your `.gitignore`:

```
.specstory/**
```

## Searching Your Codebase

When searching your codebase in Cursor, search results may include your previous AI coding interactions. To focus solely on your actual code files, you can exclude the AI interaction history from search results.

To exclude AI interaction history:

1. Open the "Find in Files" search in Cursor (Cmd/Ctrl + Shift + F)
2. Navigate to the "files to exclude" section
3. Add the following pattern:

```
.specstory/*
```

This will ensure your searches only return results from your working codebase files.

## Notes

- Auto-save only works when Cursor/sqlite flushes data to disk. This results in a small delay after the AI response is complete before SpecStory can save the history.
- Auto-save does not yet work on remote WSL workspaces.

## Settings

You can control auto-saving behavior in Cursor:

1. Open Cursor → Settings → VS Code Settings (Cmd/Ctrl + ,)
2. Search for "SpecStory"
3. Find "Auto Save" setting to enable/disable

Auto-save occurs when changes are detected in Cursor's sqlite database, or every 2 minutes as a safety net.
</file>

<file path=".specstory/history/reviewing-the-reviewers-a-critical-analysis.md">
## SpecStory

## Reviewing the Reviewers: A Critical Analysis (20/02/2025, 23:02:54)

_**User**_

Read the original writeups@web-search-api.md and then read three reviews: "by Copilot" @review-copilot.md, "by Cursor" @review-cursor.md and "by Trae" @review-trae.md. In a new document "review-o3.md", write a comprehensive REVIEW OF THE REVIEWS, judging their structure, quality, and how well they reflect the original material. Give each REVIEWER (Copilot, Cursor and Trae) one to 5 stars, and critically & perhaps sarcastically review them.

---

_**Assistant**_

```diff
-
+ # Comprehensive Review of the Reviews on Web Search API Writeups
+
+ This document critically examines the three reviews produced for the original Web Search API writeups. We analyze the structure, clarity, depth, and overall quality of the reviews by Copilot, Cursor, and Trae, and assess how well they capture the essence of the original material. At the end, each reviewer is awarded an overall star rating from 1 to 5.
+
+ ---
+
+ ## Copilot Review Analysis
+
+ The Copilot review is organized by breaking down the original writeups into several sections (o3 mini, Perplexity, Phind, Google DR, and Grok 3) with individual star ratings for each. Its structure is methodical and the use of star ratings lends an air of simplicity and quick judgment. However, the commentary sometimes comes across as terse and overly reliant on numerical scores, leaving some nuances of the original detailed analysis underexplored. It gives off the vibe of someone racing against the clock – effective for quick insights but lacking in a deeper critique.
+
+ **Overall, Copilot's review earns a solid 3.5/5 stars.**
+
+ ---
+
+ ## Cursor Review Analysis
+
+ Cursor's review stands out for its clarity and balanced critique. It methodically reviews each section (o3 mini, Perplexity, Phind, Google DR, and Grok 3) with distinct commentary that covers both strengths and shortcomings. The language is direct and informative, with a neatly organized TLDR section summarizing the best insights. Although it sometimes misses a few subtleties regarding price tiers or implementation details, its structured approach and thoughtful evaluations make it a very reliable appraisal of the original writeups.
+
+ **Overall, Cursor's review receives a high 4.5/5 stars.**
+
+ ---
+
+ ## Trae Review Analysis
+
+ Trae injects personality and wit into the review, offering a narrative that is as entertaining as it is informative. His review of Google DR, for instance, is laced with sarcasm and colorful commentary – it even suggests that the document is a cure for insomnia! Trae focuses sharply on the key strengths and weaknesses (especially of Google DR and Phind), delivering actionable insights and market perspectives. However, the review sometimes feels a bit uneven as it doesn't cover all items as consistently as the others, and its less formal style may not suit every technical reader.
+
+ **Overall, Trae's review is awarded 4/5 stars.**
+
+ ---
+
+ ## Summary
+
+ In summary, the three reviews each offer a unique perspective on the original Web Search API writeups:
+
+ - **Copilot:** A timely, if somewhat superficial, assessment with quick star ratings and concise commentary – 3.5/5 stars.
+ - **Cursor:** A meticulous and structured critique with balanced insights and a clear TLDR – 4.5/5 stars.
+ - **Trae:** A refreshingly candid and sarcastic review that delivers real insights, albeit with a less uniform approach – 4/5 stars.
+
+ Each reviewer brings something different to the table; whether you prefer brevity, structure, or personality, there's merit in all their approaches. The best combined insight, however, is that while the original writeups are dense and detailed, these reviews help distill the key points – even if one of them makes you long for a 'Skip to Conclusion' button.
+
+ *End of Review of the Reviews*
```

---

_****_

I've created a new document "docs/review-o3.md" with the comprehensive review of the reviews. You can review the file below.

---

_****_



---



---
</file>

<file path=".specstory/history/writeup-review-and-rating-analysis.md">
## SpecStory

## Writeup Review and Rating Analysis (20/02/2025, 22:46:39)

_**User**_

@web-search-api.md Analyze this file and in the same folder write, as `review-cursor.md`, a review of all writeups. Rate them from 1 to 5 stars, critically and maybe sarcastically sum up. Then  write a TLDR that combines the best out of all these. You may use the @Web

---

_**Assistant**_

I'll help analyze the web search API writeups and create a review. First, I'll create the review file.

---

_****_

```diff
-
+ # Web Search API Writeups Review
+
+ ## o3 mini ⭐⭐⭐
+ A decent tabular comparison that gets straight to the point. The formatting is clean and the data is well-organized. However, it lacks depth in the analysis and misses some crucial pricing details. The "Final Thoughts" section feels a bit superficial. Props for including privacy considerations though.
+
+ ## Perplexity ⭐⭐⭐⭐
+ A solid, well-structured analysis with good attention to detail on pricing tiers. The summary table is particularly useful. Loses a star for some redundant information and overly formal language. The citations are a nice touch, even if they're mostly just numbered placeholders.
+
+ ## Phind ⭐⭐
+ Started strong with key points but quickly devolved into bullet point hell. The formatting is inconsistent and there's significant repetition. While it contains useful information, the presentation is messy and hard to follow. At least it tried to highlight surprising details about Bing's price surge.
+
+ ## Google DR ⭐⭐⭐⭐⭐
+ The most comprehensive and well-researched analysis of the bunch. Excellent structure, detailed pricing breakdowns, and thoughtful comparisons. The methodology section adds credibility, and the citations are properly formatted. Could be shorter, but if you want the full picture, this is your guy.
+
+ ## Grok 3 ⭐⭐⭐½
+ A concise analysis that gets to the point quickly. Good focus on cost comparison and key features. The "Surprising Detail" about Bing's price increase is well-highlighted. Loses points for being a bit too brief and missing some important details about terms and conditions.
+
+ ## TLDR: The Best Combined Analysis
+
+ ### Pricing Sweet Spots:
+ - Brave & Exa are the cost leaders at $0.0025/query for keyword searches
+ - Google charges $0.005/query after free tier
+ - Bing is now the expensive one at $0.01/query
+ - DuckDuckGo has no real search API (just instant answers)
+
+ ### Best For:
+ - General Purpose: Google (expensive but reliable)
+ - Privacy Focus: Brave (independent index)
+ - AI/LLM Integration: Exa (semantic search)
+ - Developer Tools: Phind (technical focus)
+ - Enterprise: Microsoft/Bing (if you're made of money)
+
+ ### Free Tiers:
+ - Google: 100 queries/day
+ - Brave: 2,000-5,000 queries/month
+ - Exa: $10 credit
+ - Others: Various limited offerings
+
+ ### Key Takeaways:
+ 1. The market is shifting toward AI-enhanced search APIs
+ 2. Privacy-focused options are becoming more competitive
+ 3. Traditional players (Google, Bing) are generally more expensive
+ 4. Most APIs now offer semantic search capabilities
+ 5. Watch out for hidden costs in result limits and feature tiers
+
+ ### Pro Tips:
+ - Start with Brave or Exa for cost-effective testing
+ - Use Google if you need reliable, branded search
+ - Avoid Bing unless you really need Microsoft ecosystem integration
+ - Consider hybrid approaches (different APIs for different needs)
+ - Read the fine print on rate limits and result counts
```

---

_****_



I've created a review of all the writeups in `review-cursor.md`. The review:
1. Rates each writeup on a 5-star scale with critical commentary
2. Provides a sarcastic but fair assessment of each analysis's strengths and weaknesses
3. Includes a comprehensive TLDR that combines the best insights from all sources

The TLDR section is particularly useful as it breaks down the information into clear categories:
- Pricing Sweet Spots
- Best Use Cases
- Free Tier Comparisons
- Key Market Takeaways
- Pro Tips for Implementation

I've tried to maintain a balance between being critical and constructive, while adding a touch of humor (like the Bing "if you're made of money" comment). The review should help readers quickly understand which analysis to trust for different aspects of web search APIs, while also providing a consolidated view of the most important information.

Would you like me to modify any part of the review or focus on specific aspects in more detail?

---

_****_



---



---
</file>

<file path="docs/research/people-api-tldr.md">
# Comprehensive Guide to Person Profile APIs and Tools

- https://hunter.io/email-finder
- https://hunter.io/verify
- https://nubela.co/proxycurl/people-api
- https://www.api-ninjas.com/api/hobbies
- https://app.hubspot.com/
- https://app.prospeo.io/
- https://app.getprospect.com/last-step
- https://app.abstractapi.com/dashboard
- https://app.abstractapi.com/api/company-enrichment/tester
- https://app.voilanorbert.com/#/prospecting/manual
- 

This guide explores the various APIs, tools, and services available for constructing detailed profiles of individuals based on their name and email address, with a focus on discovering their profession, workplace, languages, and interests. We'll emphasize solutions that offer free tiers or are cost-effective.

## 1. Data Enrichment APIs

### 1.1. Basic Profile Enrichment

These APIs provide fundamental profile information using email addresses as the primary lookup method.

#### 1.1.1. Hunter.io

- Offers email verification and basic company information
- Free tier: 25 requests/month
- Primarily focused on B2B email discovery
- Good for initial company domain validation

**Links:**

- [Hunter.io API Documentation](https://hunter.io/api)
- [Hunter.io Email Finder](https://hunter.io/email-finder)

#### 1.1.2. Clearbit

- Provides comprehensive person and company data enrichment
- Limited free tier available
- Extensive business and professional information
- Good accuracy for work email addresses

**Links:**

- [Clearbit Enrichment API](https://clearbit.com/docs#enrichment-api)
- [Clearbit API Documentation](https://dashboard.clearbit.com/docs)

### 1.2. Professional Network Data

These services specifically focus on professional network information.

#### 1.2.1. Proxycurl

- Specializes in LinkedIn profile data
- Pay-as-you-go pricing
- Comprehensive professional details
- High accuracy for current employment

**Links:**

- [Proxycurl API Documentation](https://nubela.co/proxycurl/docs)
- [Proxycurl People API](https://nubela.co/proxycurl/people-api)

## 2. Language Detection

### 2.1. Cloud Provider APIs

Major cloud providers offer language detection services with generous free tiers.

#### 2.1.1. Google Cloud Natural Language API

- Detects language from text samples
- Free tier: 5, 000 requests/month
- High accuracy across many languages
- Easy integration with other Google services

**Links:**

- [Google Cloud Natural Language API](https://cloud.google.com/natural-language)
- [Language Detection Documentation](https://cloud.google.com/natural-language/docs/detecting-languages)

#### 2.1.2. Azure Cognitive Services

- Text Analytics API includes language detection
- Free tier: 5, 000 transactions/month
- Supports 120+ languages
- Good documentation and samples

**Links:**

- [Azure Text Analytics API](https://azure.microsoft.com/services/cognitive-services/text-analytics/)
- [Language Detection Documentation](https://docs.microsoft.com/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-language-detection)

### 2.2. Open Source Solutions

Free, self-hosted alternatives for language detection.

#### 2.2.1. Langdetect

- Python library based on Google's language detection
- Completely free and open source
- Easy to integrate into existing applications
- Good for batch processing

**Links:**

- [Langdetect GitHub Repository](https://github.com/Mimino666/langdetect)
- [PyPI Package](https://pypi.org/project/langdetect/)

## 3. Interest Analysis Tools

### 3.1. Topic Modeling

Free and open-source tools for discovering interests through content analysis.

#### 3.1.1. Gensim

- Python library for topic modeling
- Free and open source
- Includes implementations of LDA, LSI, and other algorithms
- Good for processing large text collections

**Links:**

- [Gensim Documentation](https://radimrehurek.com/gensim/)
- [Topic Modeling Tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html)

### 3.2. Social Media Analysis

APIs for analyzing public social media content.

#### 3.2.1. Twitter API v2

- Access to public tweets and user information
- Free tier available with Essential access
- Good for analyzing public interests and engagement
- Requires application approval

**Links:**

- [Twitter API Documentation](https://developer.twitter.com/en/docs/twitter-api)
- [Essential Access Information](https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api)

## 4. Ethical Considerations and Best Practices

### 4.1. Legal Compliance

- Always check and comply with data protection regulations (GDPR, CCPA)
- Obtain necessary consents when required
- Document your data collection and processing activities
- Implement data retention policies

### 4.2. Privacy Protection

- Only collect necessary information
- Use secure storage and transmission methods
- Provide transparency about data usage
- Allow opt-out options where applicable

## 5. Integration Strategy

### 5.1. Recommended Workflow

1. Start with basic email validation and enrichment using Hunter.io
2. Enrich professional data using Clearbit or Proxycurl
3. Analyze available text content using language detection tools
4. Process public social media content for interests using topic modeling
5. Combine and verify information from multiple sources

### 5.2. Cost-Effective Implementation

To maximize free tiers and minimize costs:

1. Cache results to avoid redundant API calls
2. Implement rate limiting to stay within free tier limits
3. Use open-source alternatives where possible
4. Batch process requests when applicable

## 6. Additional Resources

### 6.1. API Testing Tools

- [Postman](https://www.postman.com/) - API testing and documentation
- [Insomnia](https://insomnia.rest/) - REST client
- [HTTPie](https://httpie.io/) - Command-line HTTP client

### 6.2. Development Libraries

- [Requests](https://docs.python-requests.org/) - Python HTTP library
- [aiohttp](https://docs.aiohttp.org/) - Async HTTP client/server
- [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) - Web scraping library

## 7. Conclusion

Building comprehensive person profiles requires a combination of different APIs and tools. By leveraging free tiers and open-source solutions, it's possible to create a robust profile enrichment system without significant cost. The key is to combine multiple data sources while respecting privacy and maintaining ethical practices.

Remember to always:

- Prioritize privacy and consent
- Validate data from multiple sources
- Use rate limiting and caching
- Document your processes
- Stay updated with API changes and terms of service

This approach allows for effective profile enrichment while maintaining cost-effectiveness and ethical compliance.
</file>

<file path="docs/research/people-api.md">
# People APIs

Let's say I know a person's name and e-mail address. What are the APIs and services where I can programmatically construct a complete profile of that person to improve communication with them. Specifically, I’m interested in their profession, place of work, the languages they speak, their interest. 

# ChatGPT o3

Several providers offer APIs that take a name and email address as input and return enriched profiles covering professional details, workplace, languages, and interests. Here are some leading options:

**Clearbit Enrichment API**  
Clearbit’s API can take an email (or domain) and return detailed information such as job title, company, role, and even social profiles—all of which help you understand a person’s professional background and interests.

**FullContact API**  
FullContact specializes in contact enrichment. Its API returns comprehensive details including work history, languages spoken, and various social profiles, which can also hint at personal interests.

**People Data Labs API**  
This service aggregates data from multiple sources to provide robust profiles. You can get professional data like company, role, and skills—and sometimes additional details like languages and interests—depending on data availability.

**Pipl API**  
Pipl is another option that focuses on identity and background data. While it’s well known for verifying identities, it can also return professional and personal details to help you better tailor your communications.

Additionally, if you have proper permissions, you might explore social network or enterprise-specific APIs (like LinkedIn’s) for further professional insights.

> **Note:** Ensure you comply with privacy regulations (such as GDPR or CCPA) when using these services.

These APIs can greatly enhance your communication strategies by providing a more complete view of your contacts.

# Phind Sonnet

I'll help you understand the available options for building a person's profile programmatically, while also addressing the ethical and legal considerations involved.

### 0.1. Legal and Ethical Considerations

Before discussing technical solutions, it's important to note several key points:

* Collecting and aggregating personal data without explicit consent may violate privacy laws like GDPR, CCPA, and other regulations
* Using certain APIs or services to build profiles without the subject's knowledge could raise ethical concerns
* Some methods of data collection might violate terms of service of various platforms

### 0.2. Available Professional APIs and Services

Several legitimate APIs can be used with proper authorization and consent:

* LinkedIn API
  - Provides professional information when users authorize access
  - Can retrieve work history, skills, and professional connections
  - Requires business justification and developer program approval [3]

* Social Media APIs
  - Twitter/X, Facebook, and Instagram APIs can provide public profile information
  - Must be used in accordance with their terms of service
  - Requires user authentication and consent [3]

### 0.3. Public Data Sources

Some legitimate public data sources include:

* Professional Directory APIs
  - Industry-specific directories
  - Academic institution directories
  - Professional association databases

* Company APIs
  - Corporate employee directories (with proper authorization)
  - Professional networking platforms
  - Business registration databases

### 0.4. Best Practices

When collecting profile information:

1. Always obtain explicit consent from the individual
2. Be transparent about data collection and usage
3. Provide opt-out options
4. Store collected data securely
5. Follow data protection regulations
6. Document all data sources and collection methods

### 0.5. Alternative Approach

Instead of building profiles automatically, consider:

* Creating a self-service profile system where users can input their own information
* Using authenticated OAuth flows for connecting to professional networks
* Implementing a consent-based system for data collection
* Building relationships that encourage voluntary information sharing

Remember that the most ethical and effective approach is to gather information directly from the individual with their explicit consent and knowledge of how their data will be used.

# Google Flash

You're asking about a fascinating but also ethically sensitive area: building a detailed profile of a person programmatically using their name and email address.  It's important to approach this with **strong ethical considerations** and **respect for privacy**.  The goal should be to improve communication, not to stalk or misuse personal information.

Here's a breakdown of APIs and services, categorized by the type of information they might help you uncover:

**I. Professional & Work-Related Information:**

* **LinkedIn APIs (Official LinkedIn API, Third-party LinkedIn APIs):**
    * **What they offer:**  This is the goldmine for professional data. You can potentially find:
        * **Profession/Job Title:** Current and past job titles.
        * **Place of Work/Company:** Current and past employers.
        * **Skills:**  Listed skills and endorsements.
        * **Education:**  Schools and degrees.
        * **Languages:**  Sometimes listed in profiles.
        * **Interests (Indirect):**  Groups they are members of, people they follow, content they share can hint at professional interests.
        * **Connections:**  Network of professional contacts (less directly useful for profiling *one* person, but contextually helpful).
    * **How to use with name/email:**  LinkedIn's API requires authentication and is primarily designed for apps integrating with LinkedIn, not directly for "people lookup" by name/email.  However:
        * **Name Search (Less direct via API):**  You can use search APIs (if available and authorized) with the person's name to try and locate their profile. Email is less directly searchable via the official API.
        * **Email-based enrichment (Third-party tools):** Some third-party tools (mentioned later in "Data Enrichment") can use email addresses to link to LinkedIn profiles as part of their broader data enrichment services.
    * **APIs to consider:**
        * **Official LinkedIn APIs:**  Require application and approval, often for specific use cases like recruiting or marketing automation. Access is more restricted for general profile enrichment.
        * **Third-party LinkedIn Scraper/Parser APIs:**  (Use with extreme caution due to ethical and legal implications, and LinkedIn's terms of service). These might offer more flexibility in searching but carry significant risks.
    * **Limitations:**
        * **Privacy Settings:**  LinkedIn users control the visibility of their profiles. Data availability depends on their privacy settings.
        * **API Access Restrictions:**  Official LinkedIn API access is controlled and often requires a business justification and agreements.
        * **Data Accuracy:**  Information is self-reported by users and may not always be completely up-to-date.
        * **Ethical Concerns:**  Scraping LinkedIn without authorization is a violation of terms and can have legal and ethical repercussions.

* **Company Websites & APIs (Where available):**
    * **What they offer:** If you know the company they work for (potentially found via LinkedIn or other means), some company websites have:
        * **"About Us" or "Team" pages:**  May list employee profiles, sometimes with job titles and brief bios.
        * **Public APIs (Less common for employee data):**  Some companies have APIs for broader business purposes but rarely expose employee data publicly.
    * **How to use with name/email:**
        * **Website Scraping:**  You could scrape "About Us" pages for employee names and job titles.
        * **Email Domain Matching:** If you have the email address domain (e.g., `@company.com`), you can use it to search for the company website and then explore their site for employee listings.
    * **APIs/Tools:**
        * **Web Scraping Libraries (Python - Beautiful Soup, Scrapy; Node.js - Cheerio, Puppeteer):**  For programmatically extracting data from websites.
    * **Limitations:**
        * **Data Availability:**  Not all companies list employee profiles publicly.
        * **Website Structure Variations:**  Scraping scripts need to be adapted for each website's unique structure.
        * **Terms of Service:**  Always check the website's terms of service and robots.txt before scraping.

* **Crunchbase API:**
    * **What they offer:** Focuses on company and investment data, but can also have:
        * **People Profiles:**  For executives and key employees of companies.
        * **Company Information:**  Industry, size, location, funding, etc. (Contextually helpful).
    * **How to use with name/email:**
        * **Name Search:**  Search for people by name.  Email is less directly searchable.
        * **Company Search:**  If you know their company (from LinkedIn or other sources), you can find employee profiles associated with that company.
    * **APIs:**
        * **Crunchbase API:**  Requires an API key, offers different pricing tiers based on usage.
    * **Limitations:**
        * **Focus on Business Professionals:**  Data is more geared towards business leaders and investors than all employees.
        * **Data Coverage:**  Coverage may be less complete for smaller companies or non-executive roles.

**II. Data Enrichment APIs (General Profile Information):**

These services are specifically designed to enrich contact data. They often use email addresses and names as primary identifiers.

* **Clearbit Enrichment API:**
    * **What they offer:**  One of the most popular data enrichment services, providing a wide range of information:
        * **Professional Data:** Job title, company, industry, role, seniority, LinkedIn profile URL, Twitter profile URL.
        * **Company Data:** Company size, industry, location, funding.
        * **Location Data:**  Geographic location based on IP or other data.
        * **Demographic Data (Less precise and ethically sensitive - use cautiously):**  Sometimes inferred demographic data (e.g., gender based on name, location).
        * **Social Media Profiles:** Links to social media profiles (LinkedIn, Twitter, etc.).
    * **How to use with name/email:**  Provide name and/or email address as input to the API.
    * **APIs:**
        * **Clearbit Enrichment API:**  Requires an API key, priced based on usage (requests).
    * **Limitations:**
        * **Cost:**  Clearbit is a paid service.
        * **Data Accuracy:**  While generally good, data may not always be 100% accurate or up-to-date.
        * **Privacy:**  Data is aggregated from various public sources, but still important to consider privacy implications and usage guidelines.

* **Hunter.io (Email Finder & Verifier, Company Data):**
    * **What they offer:** Primarily known for finding email addresses associated with companies, but also offers some profile enrichment:
        * **Professional Data:** Job titles associated with emails found.
        * **Company Data:**  Company name, website, industry.
        * **LinkedIn Profile URLs (Sometimes):**  May link to LinkedIn profiles associated with found email addresses.
    * **How to use with name/email:**  Use name and company domain (if known) to find email addresses and potentially associated profile data. Or, if you have the email, use it to look up company information.
    * **APIs:**
        * **Hunter.io API:**  Offers free and paid plans, based on usage.
    * **Limitations:**
        * **Email Focus:**  More focused on email finding and verification than deep profile enrichment.
        * **Professional Data Focus:**  Primarily professional data linked to email addresses at companies.

* **FullContact Person API (Now part of Contact Network by TransUnion):**
    * **What they offer:**  Another established data enrichment provider, offering:
        * **Professional Data:** Job title, company, LinkedIn profile, other professional links.
        * **Social Media Profiles:** Links to various social media profiles (Facebook, Twitter, etc.).
        * **Demographic Data (Less precise and ethically sensitive):**  Inferred demographic data.
        * **Location Data:**  Geographic location.
        * **Interests (Inferred - Limited):**  Potentially inferred interests based on online activity.
    * **How to use with name/email:**  Provide name and/or email address as input.
    * **APIs:**
        * **FullContact Person API (Contact Network API):** Requires API key, priced based on usage.
    * **Limitations:**
        * **Cost:**  Paid service.
        * **Data Accuracy and Privacy:** Similar considerations to Clearbit regarding accuracy and ethical use of data.

* **Pipl API (People Data Platform):**
    * **What they offer:** Aggregates data from a very wide range of sources, aiming for comprehensive people profiles:
        * **Professional Data:** Job title, company, education, LinkedIn, etc.
        * **Social Media Profiles:** Extensive social media profile links.
        * **Contact Information:**  Phone numbers, addresses (Use with extreme caution and legality).
        * **Demographic Data (Sensitive):**  Potentially more detailed demographic data (use ethically and legally responsibly).
        * **Interests (Inferred - Potentially broader than others):**  Inferred interests based on wider online presence.
    * **How to use with name/email:**  Input name and/or email address.
    * **APIs:**
        * **Pipl API:**  Requires API key, often used for more in-depth people search and verification.  Be very mindful of terms of service and ethical use.
    * **Limitations:**
        * **Cost:**  Typically more expensive than basic data enrichment APIs.
        * **Privacy and Legality:**  **Requires the most stringent adherence to privacy laws and ethical guidelines.**  The depth of data available raises significant ethical concerns.  Use responsibly and ensure legal compliance.  May have restrictions on usage based on jurisdiction and purpose.

**III. Social Media APIs (Beyond LinkedIn - For Interests & Languages):**

* **Twitter API (X API):**
    * **What they offer:**
        * **Public Tweets:**  Analyze public tweets for language, topics of interest, sentiment.
        * **Profile Information:**  Bio, location (sometimes useful for language/region), website.
        * **Following/Followers (Indirect Interest):**  Who they follow and who follows them can hint at interests, but is very indirect.
    * **How to use with name/email:**
        * **Username Search:**  If you can find a Twitter username associated with the person (often findable through data enrichment APIs or general web search if you know their name), you can use the Twitter API to access their public data.  Email is not directly used to find Twitter profiles via the API.
    * **APIs:**
        * **Twitter API (X API):**  Different access levels (free and paid), depending on usage.
    * **Limitations:**
        * **Data Availability:**  Only public tweets are accessible.  Private accounts are not.
        * **Noise and Context:**  Tweets can be short and lack context.  Interest inference is challenging and noisy.
        * **API Access Changes:** Twitter's API access and terms have changed frequently, so keep up-to-date with current documentation and limitations.

* **Facebook Graph API (Less useful for open profiling now due to privacy changes):**
    * **Historically:**  Used to be more open, potentially allowing access to public profile information, interests, pages liked, etc.
    * **Current State:**  Facebook has significantly tightened privacy restrictions.  Public profile information is much more limited.  Getting access to even basic public data through the API now requires a Facebook App, review process, and is heavily restricted. **Generally, not a reliable source for open profiling anymore.**
    * **Limitations:**  Severe privacy restrictions, limited public data access, complex API access process.

* **Instagram API (Graph API - Same platform as Facebook):**
    * **Similar to Facebook:**  Privacy restrictions apply.  Public profile information is limited.
    * **Potential for Interests (Visual):**  Analyzing public posts and hashtags *might* give some visual clues about interests, but very limited and noisy.
    * **Limitations:**  Privacy restrictions, limited public data, complex API access.

**IV. Language Detection APIs:**

* **Google Translate API (Language Detection Feature):**
    * **What they offer:** Can detect the language of a text.
    * **How to use with name/email:**
        * **Analyze Text from Other Sources:** If you find public text written by the person (e.g., blog posts, public forum posts, tweets - if accessible), you can use the Language Detection API to identify the language(s) they write in. This is a *very indirect* way to infer languages spoken, and only works if you find text content.
    * **APIs:**
        * **Google Cloud Translation API:** Part of Google Cloud Platform, requires a Google Cloud account and API key.
    * **Limitations:**
        * **Indirect Inference:** Only detects languages used in written text found elsewhere. Doesn't directly say "this person speaks this language."
        * **Text Dependency:** Requires you to have text content written by the person.

* **Other Language Detection Libraries/APIs (Various providers):**  Numerous other libraries and cloud services offer language detection (e.g., Microsoft Text Analytics API, AWS Comprehend).  Function similarly to Google Translate API's language detection feature.

**V. General Search & Web Scraping (More Manual & Open-Ended):**

* **Google Custom Search API or Programmable Search Engine:**
    * **What they offer:**  Programmatically execute Google searches based on keywords (name, email, etc.).
    * **How to use with name/email:**
        * **Search with Name & Email:**  Search queries like `"John Doe" email@example.com`, `"John Doe" profession`, `"John Doe" interests`, etc.
        * **Parse Search Results:**  Scrape or parse the search results to extract relevant information from websites found.
    * **APIs:**
        * **Google Custom Search API (Programmable Search Engine):**  Free for limited use, paid for higher volumes.
    * **Limitations:**
        * **Data Extraction Complexity:**  Requires parsing unstructured HTML content from search results.
        * **Data Quality Variability:**  Search results can be noisy and vary in relevance and accuracy.
        * **Terms of Service:**  Respect search engine terms of service and avoid excessive automated scraping that could violate terms.

* **Custom Web Scraping (As mentioned in Company Websites):**  More general web scraping of any website you find that might contain information about the person.  Use web scraping libraries (Beautiful Soup, Scrapy, etc.).

**Ethical and Legal Considerations -****ABSOLUTELY CRITICAL** **:**

* **Privacy is paramount:**  Always prioritize ethical considerations and respect for personal privacy.  Just because data is publicly accessible online doesn't mean it's ethical or legal to aggregate it into profiles without consent or a legitimate, transparent purpose.
* **Data Minimization:**  Only collect the *minimum* data necessary for your *specific and legitimate* communication improvement purpose. Don't collect everything just because you can.
* **Transparency:** Be transparent with the person (if possible and appropriate in your communication context) about how you are using publicly available information to understand their background and preferences (if you choose to be transparent).
* **Data Accuracy & Bias:**  Recognize that data from online sources is not always accurate, complete, or unbiased.  Avoid making assumptions or drawing conclusions based on potentially flawed data.
* **Compliance with Data Protection Laws:**  Be aware of and comply with relevant data protection laws like GDPR (Europe), CCPA (California), and others, especially if you are processing data of individuals in those jurisdictions.  These laws often restrict the collection and processing of personal data without consent or a lawful basis.
* **Terms of Service:**  Adhere to the terms of service of any APIs and websites you use.  Avoid scraping if it violates terms of service.
* **Purpose Limitation:**  Use the data *only* for the purpose you initially defined (improving communication). Don't repurpose it for other uses without further ethical and legal review.

**In summary:**

Building a comprehensive profile programmatically using just a name and email is challenging and ethically nuanced.  LinkedIn and data enrichment APIs (Clearbit, Hunter.io, FullContact) are the most direct sources for professional data. Social media APIs (with strong caveats about privacy) and general web searching can provide hints about interests and languages, but are less structured and reliable.

**Recommendation:**

1. **Start with ethical considerations first.** Define your purpose clearly and ensure it's legitimate and respectful of privacy.
2. **Prioritize LinkedIn and data enrichment APIs** for professional data.
3. **Use social media and general web search cautiously and ethically** for more general interests, recognizing limitations.
4. **Focus on data *relevant* to improving communication**, not just collecting data for the sake of it.
5. **Be transparent (where appropriate) and responsible in your data use.**
6. **Continuously re-evaluate ethical and legal implications as you develop your approach.**

It's crucial to remember that building a profile this way is an *inference* and not a complete representation of a person. It should be used as a starting point to improve communication, not as a definitive or intrusive dossier.

# Phind Phind

I notice you're looking to gather detailed personal information about someone based on just their name and email address. Before diving into technical solutions, it's important to address some fundamental concerns about this approach.

### 0.6.  Privacy and Ethical Considerations

- Collecting personal information without explicit consent raises serious privacy concerns
- Many jurisdictions have strict regulations about gathering and storing personal data
- Using automated tools to compile profiles could violate data protection laws
- There's a risk of collecting inaccurate or outdated information

### 0.7.  Why Automated Profiling Isn't Recommended

1. **Accuracy Issues**  - Name and email alone are unreliable identifiers
  - Multiple people may share the same name
  - Information sources may be outdated or incorrect
  - Risk of confusing different individuals


2. **Legal Concerns**  - Violates GDPR in Europe 2:18
  - May breach CCPA in California
  - Could infringe local privacy laws
  - Exposes your organization to legal risks


3. **Better Alternatives**  - Ask the person directly for relevant information
  - Use legitimate professional networking platforms
  - Request information through proper channels
  - Build relationships organically



### 0.8.  Recommended Approach

Direct Communication- Reach out personally to ask about their background
- Be transparent about why you're interested
- Respect boundaries if they decline to share information

Professional Networks- Connect through established platforms like LinkedIn
- Join relevant industry groups
- Attend professional events

Consent-Based Information Gathering- Create opt-in forms for information sharing
- Provide clear privacy policies
- Give users control over their data

The most reliable and ethical way to build a relationship is through direct, honest communication rather than automated data collection. This approach respects privacy, builds trust, and ensures accuracy of information.

# Perplexity

To programmatically construct a complete profile of a person using their name and email address, you can leverage various APIs and services that provide information about individuals, including their profession, workplace, languages spoken, and interests. Here are some notable options:

## 1. APIs for Person Profile Enrichment

### 1.1. Google People API
The Google People API allows you to retrieve user profile information associated with Google accounts. You can access details such as names, email addresses, and other profile fields. However, this typically requires user authentication and permission to access their data.

- **Key Features**:
  - Access to basic profile information.
  - Requires OAuth2 authentication for user data.

### 1.2. Clearbit
Clearbit offers a Person API that enables you to find social information related to an email address. This includes the person's name, job title, workplace, social media profiles (LinkedIn, Twitter), and more.

- **Key Features**:
  - Retrieve comprehensive social profiles linked to an email.
  - Useful for sales and marketing insights.

### 1.3. CareerOneStop API
This API provides access to a wide range of career-related data, including job titles, educational backgrounds, and employment statistics. It can be useful for understanding the professional landscape of the individual.

- **Key Features**:
  - Access to employment and education data.
  - Quality-controlled datasets for accurate information.

### 1.4. TruDiligence Background Check API
TruDiligence offers a background check API that can provide detailed information about an individual's work history, education credentials, and more. This could help in verifying the professional background of the person.

- **Key Features**:
  - Integrates with applicant tracking systems for seamless data retrieval.
  - Comprehensive background checks including work history and certifications.

### 1.5. Language Detection APIs
For identifying languages spoken by a person, you can use language detection APIs such as those provided by Eden AI or other similar services. These APIs analyze text input to determine the language.

- **Key Features**:
  - Detects multiple languages from text inputs.
  - Useful for understanding multilingual capabilities.

## 2. Additional Considerations
- **Data Privacy**: When using these APIs, ensure compliance with data privacy regulations (e.g., GDPR) and obtain necessary permissions from individuals before accessing their data.
  
- **Integration**: Many of these APIs provide RESTful interfaces which can be easily integrated into your applications using standard programming languages like Python or JavaScript.

By combining these resources, you can effectively construct a detailed profile that enhances communication with the individual based on their professional background and interests.

Citations:
[1] https://developers.google.com/people/v1/profiles
[2] https://www.pcmag.com/picks/the-best-personal-data-removal-services
[3] https://clearbit.com/blog/hello-world
[4] https://www.careeronestop.org/Developers/WebAPI/web-api.aspx
[5] https://www.reddit.com/r/marketing/comments/7tmjkz/trying_to_get_job_titles_from_list_of_emails/
[6] https://www.trudiligence.com/background-check-integration/
[7] https://www.edenai.co/post/top-10-language-detection-apis
[8] https://www.onetcenter.org/IP.html
[9] https://www.api-ninjas.com/api/hobbies
[10] https://developers.google.com/people/api/rest/v1/people
[11] https://www.sscpnm.com/wp-content/uploads/2018/04/Privacy-Policy-SSCPNM.pdf
[12] https://prospeo.io/api/email-finder
[13] https://coresignal.com/solutions/employee-data-api/
[14] https://getprospect.com
[15] https://cfirst.io/background-check-apis/
[16] https://poeditor.com/docs/languages
[17] https://mohr.uoregon.edu/channels/identifying-your-interests-self-assessment/
[18] https://stackoverflow.com/questions/31227830/getting-contact-list-profile-pictures-and-emails-from-google-apis
[19] https://www.reddit.com/r/privacy/comments/19bm8vb/how_can_i_find_out_every_data_broker_site_that_is/
[20] https://developers.facebook.com/docs/workplace/reference/graph-api/
[21] https://www.devgold.com/best-email-lookup-api/
[22] https://segment.com/docs/unify/profile-api/
[23] https://www.signupanywhere.com
[24] https://stackoverflow.com/questions/52443126/access-user-profile-information-using-gmail-api-by-just-email-address
[25] https://developers.liveperson.com/profiles-api-methods-create-profile.html
[26] https://bravado.co/war-room/posts/is-there-a-tool-that-takes-personal-emails-and-enriches-them-with-name-role-and-company
[27] https://www.mailersend.com/blog/guide-to-using-email-apis
[28] https://www.captaindata.com/people-profile-api
[29] https://datagma.com
[30] https://www.twilio.com/en-us/blog/best-email-api
[31] https://docs.mparticle.com/developers/apis/profile-api/
[32] https://www.reddit.com/r/privacy/comments/1c18d4a/can_someone_get_my_personal_details_via_a_newly/
[33] https://checkr.com/our-technology/background-check-api
[34] https://www.elastic.co/guide/en/workplace-search/current/workplace-search-api-overview.html
[35] https://blog.datacaptive.com/job-position-contact-lists/
[36] https://certn.co/background-screening-api/
[37] https://peoplesafe.co.uk/blogs/the-role-of-apis-in-enhancing-workplace-safety/
[38] https://www.loopcv.pro
[39] https://rapidapi.com/collection/background-check-api
[40] https://www.notifytechnology.com/the-role-of-apis-for-safety-professionals/
[41] https://www.upwork.com/services/product/marketing-search-for-certain-people-job-titles-and-find-contact-info-1644383522457747456
[42] https://serpapi.com/use-cases/background-check-automation
[43] https://www.voya.com/voya-insights/what-are-apis-and-how-can-they-help-employers-and-employees
[44] https://www.bookyourdata.com/ready-made-lists/job-titles
[45] https://developer.vonage.com/en/blog/the-ultimate-list-of-fun-apis-for-your-next-coding-project
[46] https://opentelemetry.io/docs/languages/
[47] https://www.polyglossic.com/interests-reignite-language-fire/
[48] https://www.reddit.com/r/dataengineering/comments/1epvzf9/free_apis_for_personal_projects/
[49] https://developer.accuweather.com/localizations-by-language
[50] https://www.indeed.com/career-advice/finding-a-job/how-to-find-your-passion
[51] https://www.turing.com/kb/7-examples-of-apis
[52] https://developers.perspectiveapi.com/s/about-the-api-attributes-and-languages
[53] https://www.tilburguniversity.edu/students/career/your-career-path/interests-and-values
[54] https://github.com/public-apis/public-apis
[55] https://www.teradata.com/platform/clearscape-analytics/languages-apis
[56] https://www.ef.com/wwen/blog/language/ways-to-improve-your-cv/
[57] https://www.kaspr.io/blog/email-finder-tools
[58] https://www.reddit.com/r/languagelearning/comments/153zfiq/what_hobbies_do_you_have_besides_languages_if_any/
[59] https://dev.to/mukeshkuiry/12-free-and-fun-api-for-your-next-project-5eem
[60] https://www.reddit.com/r/devops/comments/tyu9rl/rest_api_development_what_language_do_you_prefer/

---
Answer from Perplexity: pplx.ai/share

# Perplexity R1

To construct a comprehensive profile of a person based on their name and email address, you can utilize various APIs and services that provide detailed professional and personal information. Here are some notable options:

## 3. APIs for Profile Enrichment

### 3.1. Proxycurl APIs
Proxycurl offers a suite of APIs specifically designed for enriching people and company profiles. Key features include:

- **Person Profile Endpoint**: Provides structured data such as profile picture, job history, education, languages spoken, interests, and more. It can deliver up to 45 data points about an individual, including personal and work-related information[1].

### 3.2. Google People API
The Google People API allows access to merged information from various sources. It can return details like:

- Name, email addresses, occupation, organization, interests, skills, locations, and more.
- This API is useful for integrating contact management functionalities into applications[2].

### 3.3. Coresignal Employee Data API
Coresignal provides access to a large-scale database of professional profiles. It allows you to:

- Search for individuals based on job title, company name, location, and industry.
- Enrich existing data with updated records that include job titles, locations, skills, and educational background[3].

### 3.4. LinkedIn API
The LinkedIn API offers extensive access to professional data. Key functionalities include:

- Fetching public profile information such as experience, skills, education, and recommendations.
- Note that access requires OAuth authentication and permission from the user whose data you wish to retrieve[4].

### 3.5. Microsoft Cognitive Services
For language-related data, Microsoft provides APIs that can help identify languages spoken by a person through text analysis. This can be useful if you have text samples or communications from the individual[5].

## 4. Considerations for Use
When using these APIs:
- **Data Privacy**: Ensure compliance with data privacy regulations such as GDPR when handling personal information.
- **Permission**: Some services (like LinkedIn) require explicit permission from users to access their data.
- **Integration**: Most APIs provide documentation to assist with integration into your existing systems or applications.

By leveraging these APIs, you can gather a well-rounded profile of individuals that includes their profession, workplace information, language capabilities, and personal interests.

Citations:
[1] https://nubela.co/blog/ultimate-guide-to-all-21-proxycurl-apis-enrich-people-company-profiles/
[2] https://developers.google.com/people/api/rest/v1/people
[3] https://coresignal.com/solutions/employee-data-api/
[4] https://www.getphyllo.com/post/linkedin-api-to-get-profile-information
[5] https://blogs.windows.com/windowsdeveloper/2017/02/15/cognitive-services-apis-language/
[6] https://www.smartling.com/blog/lsp-language-service-provider
[7] https://doc.batch.com/api/profile/update/
[8] https://www.onetcenter.org/IP.html
[9] https://3cloudsolutions.com/resources/cognitive-services-showcase-api-language-tools/
[10] https://docs.oracle.com/en/cloud/saas/human-resources/24d/farws/op-talentpersonprofiles-post.html
[11] https://bravado.co/war-room/posts/is-there-a-tool-that-takes-personal-emails-and-enriches-them-with-name-role-and-company
[12] https://learn.microsoft.com/en-us/graph/people-insights-overview
[13] https://www.reddit.com/r/jobs/comments/6fpf8g/creating_a_professional_email_the_ones_with_my/
[14] https://www.captaindata.com/people-profile-api
[15] https://www.amitree.com/resources/blog/50-ideas-for-creating-professional-email-addresses-2021/
[16] https://nubela.co/proxycurl/people-api
[17] https://docs.mparticle.com/developers/apis/profile-api/
[18] https://sparkmailapp.com/blog/create-professional-email-address-format-examples
[19] https://nobl.ai/apis/recommender/
[20] https://developer.webex.com/docs/api/v1/people/create-a-person
[21] https://workspace.google.com/solutions/business-email/
[22] https://www.indeed.com/career-advice/finding-a-job/job-search-tools
[23] https://www.elastic.co/guide/en/workplace-search/current/workplace-search-api-overview.html
[24] https://www.indeed.com/career-advice/finding-a-job/pay-someone-to-find-me-a-job
[25] https://developers.facebook.com/docs/workplace/reference/account-management-api/graph-api/
[26] https://peoplesafe.co.uk/blogs/the-role-of-apis-in-enhancing-workplace-safety/
[27] https://www.coursera.org/articles/best-job-search-engines
[28] https://www.captaindata.com/company-profile-api
[29] https://www.notifytechnology.com/the-role-of-apis-for-safety-professionals/
[30] https://nationalcareers.service.gov.uk/careers-advice/advertised-job-vacancies
[31] https://developers.google.com/my-business
[32] https://www.vergesense.com/resources/blog/how-do-apis-promote-workplace-efficiency
[33] https://hirelevel.com/recruitment-services-job-seekers/
[34] https://www.cloudskillsboost.google/course_templates/634
[35] https://poeditor.com/docs/languages
[36] https://www.atanet.org/directory/
[37] https://learn.microsoft.com/en-us/graph/api/resources/profile-example?view=graph-rest-beta
[38] https://developer.accuweather.com/localizations-by-language
[39] https://www.ethniccommunities.govt.nz/resources/language-services-support-and-information/
[40] https://www.reddit.com/r/devops/comments/tyu9rl/rest_api_development_what_language_do_you_prefer/
[41] https://developers.perspectiveapi.com/s/about-the-api-attributes-and-languages
[42] https://poeditor.com/blog/how-to-choose-language-service-provider/
[43] https://docs.systran.net/translateAPI/profiles/
[44] https://blog.axway.com/learning-center/apis/api-management/what-is-an-api-language
[45] https://www.linkedin.com/pulse/9-steps-choosing-right-language-service-provider-lsp-tarjama
[46] https://stackoverflow.com/questions/52443126/access-user-profile-information-using-gmail-api-by-just-email-address
[47] https://www.neo.space/blog/personal-email
[48] https://www.zoho.com/mail/how-to/choose-a-professional-email-address.html
[49] https://api.slack.com/methods/users.profile.set
[50] https://www.upwork.com/resources/best-job-search-website
[51] https://www.careeronestop.org/Developers/WebAPI/web-api.aspx
[52] https://www.findmyprofession.com/career-advice/agencies-that-help-find-jobs/
[53] https://developers.google.com/my-business/ref_overview
[54] https://www.voya.com/voya-insights/what-are-apis-and-how-can-they-help-employers-and-employees
[55] https://support.google.com/business/answer/6333473?hl=en
[56] https://www.helloglobo.com/blog/what-do-language-services-providers-do
[57] https://avantpage.com/blog/ways-to-improve-equity-language-access/

---
Answer from Perplexity: pplx.ai/share

# You Genius

Based on the research findings and analysis, here is a comprehensive compilation of ethical and legal ways to gather publicly available information to improve communication, focusing on professional networking platforms, public databases, language detection, and interest analysis tools, while emphasizing the importance of privacy and consent:

## 5. Overview

Gathering publicly available information can significantly enhance communication strategies, but it must be done ethically and legally. This report covers four key areas: professional networking platforms, public databases, language detection, and interest analysis tools. Each section outlines legal methods for data gathering and important privacy considerations.

![fig](https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/222secondStreet.jpg/500px-222secondStreet.jpg)

## 6. Ethical and Legal Guidelines for Public Information Gathering

| Category | Legal Methods | Privacy Considerations |
|----------|---------------|------------------------|
| Professional Networking APIs | • Use official LinkedIn Partner APIs with proper authentication<br>• Access GitHub API with rate limit compliance<br>• Respect API terms of service and usage policies | • Store only authorized user data<br>• Obtain explicit user permissions<br>• Follow platform privacy policies |
| Language Detection APIs | • Use established services (AWS, Google, Azure)<br>• Process text only with user consent<br>• Follow data retention policies | • Ensure secure data transmission<br>• Limit personal data storage<br>• Provide opt-out options |
| Public Database APIs | • Access open data portals (Data.gov, World Bank)<br>• Use academic APIs with proper attribution<br>• Follow API documentation guidelines | • Respect data usage restrictions<br>• Handle sensitive info appropriately<br>• Follow ethical guidelines |
| Interest/Topic Modeling | • Analyze public posts with consent<br>• Implement privacy protection measures<br>• Use transparent data collection methods | • Protect user anonymity<br>• Implement data security measures<br>• Be transparent about analysis methods |

## 7. Professional Networking Platforms

### 7.1. LinkedIn API

LinkedIn offers a variety of APIs for accessing professional networking data, but their use is highly regulated:

1. **Access and Permissions**: 
   - Developers must become LinkedIn Partners, which involves a rigorous approval process.
   - APIs include Profile API, Connections API, Share API, Invitation API, Organization API, UGC API, and Compliance API.

2. **Usage Policies**:
   - Data storage is only allowed for authenticated members with their permission.
   - Strict rate limits are enforced on API calls.
   - All usage must comply with LinkedIn's privacy policies.

3. **Privacy and Consent**:
   - Explicit user permissions are required before accessing their data.
   - Developers must ensure compliance with LinkedIn's privacy policies.

### 7.2. GitHub API

GitHub provides APIs for interacting with its platform's features and data:

1. **Access and Permissions**:
   - User API allows access to public and private information about authenticated users.
   - Repository API provides access to repository data, including issues, pull requests, and commits.

2. **Usage Policies**:
   - Developers must comply with GitHub's Acceptable Use Policies.
   - Personal information must be used in accordance with GitHub's Privacy Statement.
   - Rate limits are imposed to ensure fair access and prevent abuse.

## 8. Public Databases

Various APIs and services provide access to publicly available information:

1. **Scholarly and Academic Publications**:
   - APIs like arXiv, BioMed Central, Elsevier Scopus, IEEE Xplore, JSTOR Data for Research, and PubMed Central OAI-PMH service offer access to academic publications and metadata.

2. **Company Directories and Business Information**:
   - APIs such as OpenCorporates, Crunchbase, and Clearbit provide access to company data and business information.

3. **Government and Open Data**:
   - Platforms like Data.gov, World Bank Open Data, and UN Comtrade Web Services offer access to various datasets across different domains.

4. **Specialized Data Repositories**:
   - APIs from the Library of Congress, Digital Public Library of America (DPLA), and NASA Open Data Portal provide access to specific types of data.

### 8.1. Usage Policies and Access Considerations:
- Most APIs require an API key for access.
- Rate limits are often imposed to prevent abuse.
- Data usage restrictions may apply, especially for commercial purposes.
- Privacy and consent must be considered when accessing and using data.

## 9. Language Detection

Several language detection APIs and services are available for analyzing written content:

1. **Major Cloud Providers**:
   - Amazon Comprehend, Google Cloud Natural Language API, IBM Watson Natural Language Understanding, and Microsoft Azure Cognitive Services offer robust language detection capabilities.

2. **Specialized Services**:
   - MeaningCloud, ModernMT, NeuralSpace, NLPCloud, and One AI provide advanced language detection features.

### 9.1. Key Considerations:
- Ensure secure data transmission when using these services.
- Limit personal data storage and provide opt-out options for users.
- Follow data retention policies as specified by each service provider.

## 10. Interest Analysis Tools

Interest and topic modeling APIs can analyze public social media posts to identify areas of interest:

1. **Key Techniques**:
   - Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), and BERTopic are commonly used for topic modeling.

2. **Notable APIs and Services**:
   - Google Cloud Natural Language API, IBM Watson Natural Language Understanding, Microsoft Azure Text Analytics, and Gensim offer powerful tools for text analysis and topic modeling.

3. **Applications**:
   - These tools can be used for analyzing social media data, customer reviews, and other user-generated content to identify prevalent themes and interests.

### 10.1. Ethical and Legal Considerations:
- Ensure that the data being analyzed is publicly available and that users' privacy is respected.
- Implement robust data security measures to protect sensitive information.
- Be transparent about data collection and analysis methods, providing users with opt-out options when possible.

## 11. Conclusion

Gathering publicly available information can significantly enhance communication strategies when done ethically and legally. By following the guidelines outlined in this report, organizations can leverage professional networking platforms, public databases, language detection tools, and interest analysis APIs while respecting user privacy and complying with legal requirements. It is crucial to prioritize transparency, obtain necessary consents, and implement strong data security measures throughout the information gathering process.

# You Research

To construct a comprehensive profile of a person based on their name and email address, there are several APIs and services that can be utilized programmatically. These tools can help gather information about a person's profession, place of work, languages spoken, and interests. However, it's crucial to approach this task with careful consideration of legal and ethical guidelines. Here's a detailed overview of the available options:

## 12. Email Enrichment Services

Email enrichment services are a primary tool for gathering publicly available information based on email addresses. These services can provide valuable insights into a person's professional background and contact details.

1. **Tomba**: This service specializes in email enrichment by locating verified email addresses of professionals using their names. Tomba taps into an extensive public email database to accurately identify necessary contact details. It can be a good starting point for gathering basic professional information.

2. **Clearbit**: Clearbit offers real-time data enrichment and integrates with major CRM platforms like Salesforce. It provides demographic, technographic, and firmographic details, which can be used to enhance contact records and improve sales personalization. This could be particularly useful for understanding a person's profession and place of work.

3. **Hunter.io**: Known for finding and verifying professional email addresses, Hunter.io provides email discovery and enrichment services. It allows users to find emails and enrich contacts based on domain names. This can be helpful in confirming the validity of the email address and potentially uncovering additional professional information.

4. **Snov.io**: Snov.io offers email enrichment and verification services, using its database of over 260 million email addresses. It provides enriched lead profiles based on email addresses and integrates with CRM systems. This could be valuable for gathering a more comprehensive professional profile.

## 13. Professional Networking APIs

While professional networking APIs like LinkedIn's can provide rich data about a person's professional life, it's important to note that access to these APIs is often restricted and requires partnership agreements.

1. **LinkedIn API**: The LinkedIn API is a powerful tool that allows access to user profiles, connections, and company information. However, access is tightly controlled and primarily available to approved LinkedIn Partners. To use this API, you would need to:
   - Become a LinkedIn Partner, which involves a rigorous approval process.
   - Use OAuth 2.0 for user authorization and API authentication.
   - Comply with LinkedIn's strict terms of service and privacy guidelines.

![LinkedIn Office in Toronto](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/LinkedInOfficeToronto2.jpg/500px-LinkedInOfficeToronto2.jpg)

It's worth noting that unauthorized access or misuse of LinkedIn data can lead to account bans or legal action, so it's crucial to follow their guidelines carefully.

## 14. Language Detection APIs

To determine the languages a person speaks, you can analyze their written content using language detection APIs. These can be particularly useful if you have access to the person's public social media posts or other written communications.

1. **Google Cloud Language Detection API**: This widely-used API is part of Google's suite of language processing tools and can be accessed programmatically to determine the language of a document.

2. **AWS Language Detection API**: Part of Amazon Comprehend, this API can examine text to determine the dominant language, using identifiers from RFC 5646.

3. **Microsoft Azure Language Detection**: Azure AI services provide a language detection feature that evaluates text and returns a language identifier. This service is designed to be integrated into larger applications for seamless language processing.

## 15. Interest and Topic Modeling APIs

To infer a person's interests, you can use topic modeling and interest prediction APIs that analyze public content.

1. **Latent Dirichlet Allocation (LDA)**: This popular probabilistic model assumes documents are generated from a mixture of topics, and each topic is characterized by a distribution over words. It has been applied to various types of corpora, including social media data, to extract topics and infer user interests.

2. **Google's Topics API**: Part of Google's Privacy Sandbox initiative, this API is designed to replace third-party cookies with a privacy-preserving method of interest-based advertising. It classifies users' online behaviors into categories of interest.

3. **Social Media APIs**: Platforms like Twitter provide APIs that allow access to public data, which can be analyzed using topic modeling techniques to extract meaningful insights about a person's interests.

## 16. Legal and Ethical Considerations

When using these APIs and services to construct a profile of an individual, it's crucial to adhere to legal and ethical guidelines:

1. **Informed Consent**: Ensure that you have the right to access and use the person's data. Many jurisdictions require explicit consent for data collection and profiling.

2. **Data Protection Regulations**: Comply with relevant data protection laws such as GDPR in the EU or CCPA in California. These laws give individuals rights over their personal data, including the right to know what data is being collected and how it's being used.

3. **Transparency**: Be transparent about your data collection practices, including how data is collected, used, and shared. This builds trust and aligns with ethical standards.

4. **Data Minimization**: Collect only the data necessary for your intended purpose to avoid unnecessary privacy intrusions.

5. **Accountability**: Be prepared to address any breaches or misuse of data and ensure compliance with both legal and ethical standards.

In conclusion, while there are numerous APIs and services available to construct a comprehensive profile of a person based on their name and email address, it's essential to approach this task with caution and respect for privacy. Always prioritize ethical considerations and legal compliance when gathering and using personal data for communication purposes.

# Grok DR

### 16.1. Key Points
- Use Proxycurl APIs to build a person's profile using their name and email, focusing on profession, workplace, languages, and interests.
- Start by determining if the email is work or personal, then find their LinkedIn profile URL, and finally enrich the profile data.

### 16.2. Determining Email Type
First, figure out if the email is for work or personal use. Work emails usually have domains like `@company.com`, while personal emails often use domains like `@gmail.com` or `@yahoo.com`. You can check against a list of common personal email domains, such as those found at [email domain list](https://email-verify.my-addr.com/list-of-most-popular-email-domains.php).

### 16.3. Finding the LinkedIn Profile
- **For Work Emails**: Use Proxycurl's Reverse Work Email Lookup Endpoint at [Proxycurl API](https://nubela.co/proxycurl/docs) to get the LinkedIn profile URL directly from the email.
- **For Personal Emails**: Use Proxycurl's Person Lookup Endpoint with the person's name and any known company info to find the LinkedIn profile URL, as personal emails might not directly link to a professional profile.

### 16.4. Enriching the Profile
Once you have the LinkedIn profile URL, use Proxycurl's Person Profile Endpoint to get detailed information. This will include:
- **Profession**: Extracted from the job title.
- **Place of Work**: Taken from the current company field.
- **Languages Spoken**: Available in the languages field of the profile.
- **Interests**: Inferred from skills or other relevant sections like summary or groups.

### 16.5. Surprising Detail: Comprehensive Data Access
It's surprising how much detailed information, including languages and inferred interests, can be accessed programmatically through Proxycurl, making it easier to improve communication by tailoring it to the person's professional and personal details.

---

### 16.6. Comprehensive Analysis of APIs and Services for Profile Construction

This analysis explores how to programmatically construct a complete profile of a person using their name and email address, focusing on profession, place of work, languages spoken, and interests. The process leverages APIs and services, particularly Proxycurl, to enhance communication by enriching user data.

#### 16.6.1. Methodology and Approach

The initial step involves determining whether the provided email is a work or personal email, as this influences the method for finding the person's LinkedIn profile. Work emails typically have domains associated with companies (e.g., `@company.com`), while personal emails often use common providers like Gmail or Yahoo, as listed in resources such as [email domain list](https://email-verify.my-addr.com/list-of-most-popular-email-domains.php). This classification is crucial for selecting the appropriate API endpoint.

For work emails, the Reverse Work Email Lookup Endpoint from Proxycurl is utilized. This endpoint, detailed at [Proxycurl API](https://nubela.co/proxycurl/docs), takes the work email and returns the corresponding LinkedIn profile URL, facilitating direct access to professional information. For personal emails, where the domain suggests a personal account (e.g., `@gmail.com`), the Person Lookup Endpoint is employed. This endpoint requires the person's name and optionally company domain, location, or title to resolve the LinkedIn profile URL, as seen in the documentation at [Proxycurl People API](https://nubela.co/proxycurl/people-api).

Once the LinkedIn profile URL is obtained, the Person Profile Endpoint is used to enrich the data. This endpoint, also part of [Proxycurl API](https://nubela.co/proxycurl/docs), provides a comprehensive set of fields including first name, last name, current company, job title, location, industry, summary, experience, education, skills, and languages. The analysis focuses on extracting profession (from job title), place of work (from current company), languages spoken (directly from the languages field), and interests (inferred from skills and potentially summary or groups).

#### 16.6.2. Detailed Process and Considerations

1. **Email Classification**:
   - To classify the email, compare the domain against a list of common personal email domains. For instance, domains like `gmail.com`, `yahoo.com`, and `hotmail.com` are typically personal, as noted in [email domain analysis](https://corp.inntopia.com/email-domains/). If the domain is not in this list, it is assumed to be a work email.
   - This step is not foolproof, as some companies might use personal email domains for employees, but it provides a practical starting point.

2. **Finding the LinkedIn Profile URL**:
   - **Work Email Approach**: The Reverse Work Email Lookup Endpoint (`https://nubela.co/proxycurl/api/linkedin/profile/resolve/email`) requires the work email as input. An example request might look like:
     ```
     curl -G -H "Authorization: Bearer ${YOUR_API_KEY}" 'https://nubela.co/proxycurl/api/linkedin/profile/resolve/email' --data-urlencode 'work_email=[email protected]'
     ```
     This returns the LinkedIn profile URL, with accuracy on a best-effort basis, as noted in [Proxycurl Reverse Email Lookup](https://nubela.co/proxycurl/reverse-email-lookup).
   - **Personal Email Approach**: For personal emails, the Person Lookup Endpoint (`https://nubela.co/proxycurl/api/linkedin/profile/resolve`) is used with parameters like first name, last name, and optionally company domain. An example:
     ```
     curl -G -H "Authorization: Bearer ${YOUR_API_KEY}" 'https://nubela.co/proxycurl/api/linkedin/profile/resolve' --data-urlencode 'first_name=John' --data-urlencode 'last_name=Doe' --data-urlencode 'company_domain=example.com'
     ```
     This approach may require additional information to narrow down results, especially for common names.

3. **Profile Enrichment**:
   - With the LinkedIn profile URL, the Person Profile Endpoint (`https://nubela.co/proxycurl/api/linkedin/person-profile`) is queried. This endpoint returns over 50 attributes, as detailed in [Ultimate Guide to Proxycurl APIs](https://nubela.co/blog/ultimate-guide-to-all-21-proxycurl-apis-enrich-people-company-profiles/). Key fields include:
     - **Profession**: Extracted from `job_title`.
     - **Place of Work**: Taken from `current_company`.
     - **Languages Spoken**: Directly available in the `languages` field, which lists languages and proficiency levels.
     - **Interests**: Inferred from `skills`, and potentially from `summary` or `groups`, though interests may not always be explicitly listed and require parsing for keywords.

4. **Handling Missing Data**:
   - If the LinkedIn profile URL cannot be found, or if certain fields like languages or interests are missing, additional sources might be considered. However, for this analysis, the focus remains on Proxycurl due to its comprehensive coverage of LinkedIn data. The documentation at [Proxycurl Docs](https://nubela.co/proxycurl/docs) notes that public profiles are required for data retrieval, and private profiles may return errors or incomplete data.

#### 16.6.3. Extracted Information and Use Cases

The extracted information directly addresses the user's needs:
- **Profession**: The `job_title` field provides the current professional role, such as "Software Engineer" or "Marketing Manager".
- **Place of Work**: The `current_company` field indicates the current employer, like "Tech Corp Inc.".
- **Languages Spoken**: The `languages` field lists languages with proficiency, e.g., "English (Native), Spanish (Fluent)".
- **Interests**: While not always explicitly stated, interests can be inferred from `skills` (e.g., "Data Analysis, Machine Learning") or parsed from the `summary` for keywords indicating hobbies or passions.

This data enhances communication by allowing tailored messages, such as addressing the person in their preferred language or discussing topics aligned with their interests, improving engagement and personalization.

#### 16.6.4. Comparative Analysis and Alternatives

While Proxycurl is the primary service recommended, other APIs like Clearbit, ZoomInfo, and Hunter.io offer similar functionalities. For instance, Clearbit's [Person API](https://clearbit.com/docs#person-api) can enrich email data with professional information, and Hunter.io's [Email Finder](https://hunter.io/email-finder) can verify emails and find associated social profiles. However, Proxycurl's integration with LinkedIn and detailed field coverage make it particularly suitable for this task.

#### 16.6.5. Tables for Clarity

To organize the API endpoints and their uses, consider the following table:

| **API Endpoint**                          | **Purpose**                              | **Input**                     | **Output**                     |
|-------------------------------------------|------------------------------------------|-------------------------------|---------------------------------|
| Reverse Work Email Lookup                 | Find LinkedIn URL from work email        | Work email                    | LinkedIn profile URL           |
| Person Lookup Endpoint                    | Find LinkedIn URL using name and company | Name, company domain, etc.    | LinkedIn profile URL           |
| Person Profile Endpoint                   | Enrich profile data                     | LinkedIn profile URL          | Comprehensive profile data     |

Another table for the extracted fields:

| **Field**         | **Source**               | **Example**                     |
|-------------------|--------------------------|---------------------------------|
| Profession        | job_title                | Software Engineer               |
| Place of Work     | current_company          | Tech Corp Inc.                  |
| Languages Spoken  | languages                | English (Native), Spanish       |
| Interests         | skills, summary          | Data Analysis, Machine Learning |

#### 16.6.6. Conclusion

The process of constructing a complete profile using Proxycurl APIs is efficient and programmatic, leveraging the rich data available from LinkedIn profiles. By classifying the email type, finding the LinkedIn profile URL, and enriching the data, you can obtain profession, place of work, languages spoken, and inferred interests, enhancing communication strategies. This approach is particularly effective for professional networking and personalized outreach, with considerations for handling missing data through additional searches if necessary.

#### 16.6.7. Key Citations
- [List of most popular email domains by live emails](https://email-verify.my-addr.com/list-of-most-popular-email-domains.php)
- [Complete API documentation for Proxycurl's B2B data enrichment APIs](https://nubela.co/proxycurl/docs)
- [Proxycurl's People API for finding and enriching profiles](https://nubela.co/proxycurl/people-api)
- [Ultimate guide to all 21 Proxycurl APIs for enriching people and company profiles](https://nubela.co/blog/ultimate-guide-to-all-21-proxycurl-apis-enrich-people-company-profiles/)

# Grok Search

To programmatically construct a complete profile of a person using their name and email address—focusing on their profession, place of work, languages spoken, and interests—you can leverage **data enrichment APIs and services**. These tools aggregate publicly available information from various online sources, such as professional networks, social media, and company websites, to provide detailed insights. Below, I’ll outline the most effective APIs and services for this purpose, tailored to improving communication by understanding these specific attributes.

---

### 16.7. Recommended APIs and Services

#### 16.7.1. FullContact
- **Overview**: FullContact is a data enrichment platform that offers a **Person API** designed to provide comprehensive personal and professional details based on an email address.
- **Key Features**:
  - **Profession and Workplace**: Returns employment history, including job titles and company names, allowing you to identify their current role and employer.
  - **Languages**: Explicitly includes a "languages" field in the API response, detailing the languages the person speaks.
  - **Interests**: Under the "digitalFootprint" section, it lists topics of interest derived from their online activity, such as social media or public web presence.
  - **Social Media Profiles**: Provides links to profiles (e.g., LinkedIn, Twitter), which can offer additional context.
- **How to Use**: Submit the person’s email address via an API request, and receive a structured JSON response with the above details.
- **Why It’s Useful**: FullContact directly addresses all four requested attributes, making it a one-stop solution for building a detailed profile.

#### 16.7.2. Clearbit
- **Overview**: Clearbit is another data enrichment service with an **Enrichment API** that provides professional and personal insights based on an email address.
- **Key Features**:
  - **Profession and Workplace**: Delivers job title, company name, and employment details, pinpointing their professional role and workplace.
  - **Languages**: Does not explicitly list languages but includes a "geo" field (country, city), which can help infer likely languages (e.g., German for someone in Germany).
  - **Interests**: While not explicitly labeled as "interests," it provides social media handles (e.g., Twitter, LinkedIn), which can be analyzed further to deduce interests.
  - **Social Media Profiles**: Links to profiles that can enhance the profile with additional data.
- **How to Use**: Send the email address to the Clearbit Enrichment API endpoint, and parse the response for professional and social details.
- **Why It’s Useful**: Clearbit excels at professional data and provides a foundation that can be extended with other tools for languages and interests.

---

### 16.8. How These Services Work
- **Input**: You provide the person’s name and email address (though the email alone is often sufficient) through an API request.
- **Output**: The service returns a structured dataset containing:
  - Job title and company (profession and workplace).
  - Languages spoken (directly from FullContact; inferred from Clearbit’s location data).
  - Interests (from FullContact’s digital footprint; indirectly via Clearbit’s social links).
  - Additional links to social profiles for further exploration.
- **Integration**: These APIs are easy to integrate into your application or script using standard HTTP requests, with documentation available on their websites (fullcontact.com and clearbit.com).

---

### 16.9. Complementary Tools for Missing Details
If FullContact or Clearbit don’t fully cover languages or interests due to limited data on a specific individual, you can supplement them with these tools:

#### 16.9.1. Language Detection APIs
- **Purpose**: To determine the languages a person speaks by analyzing their public content (e.g., social media posts, blogs).
- **Options**:
  - **Google Cloud Translation API**: Detects the language of text samples, useful if you retrieve posts via social media links from FullContact or Clearbit.
  - **langdetect (Python Library)**: A free, lightweight alternative for language detection if you process content programmatically.
- **How to Use**: Extract text from linked social profiles or websites, then run it through the API to identify languages.

#### 16.9.2. Social Media APIs and NLP Tools
- **Purpose**: To extract interests from social media activity when enrichment APIs provide insufficient detail.
- **Options**:
  - **Twitter API**: If a Twitter handle is provided, fetch recent tweets and use natural language processing (NLP) to identify recurring topics or interests.
  - **Topic Modeling Tools**: Libraries like Gensim or APIs like IBM Watson Natural Language Understanding can analyze text from posts or bios to pinpoint interests.
- **How to Use**: Access public posts via social media APIs (where permitted) and apply NLP techniques to categorize interests.

---

### 16.10. Considerations
- **Accuracy**: The quality of the profile depends on the person’s online presence. Individuals with minimal public data may yield incomplete results.
- **Cost**: Both FullContact and Clearbit charge per API request, though they offer free tiers for limited use. Check their pricing pages for details.
- **Privacy and Ethics**: Ensure compliance with data protection laws (e.g., GDPR, CCPA) by using these services only for legitimate purposes, such as improving communication, and not for unsolicited marketing. These APIs typically rely on public data, but you should verify your use case aligns with their terms and applicable regulations.

---

### 16.11. Conclusion
To programmatically build a complete profile of a person—including their profession, place of work, languages, and interests—based on their name and email address, the most effective tools are **data enrichment APIs like FullContact and Clearbit**. FullContact stands out for directly providing languages and interests, while Clearbit excels at professional data and offers flexibility for further analysis. For cases where additional detail is needed, language detection APIs or social media analysis can enhance the profile. These services enable you to tailor communication by understanding the individual’s background and preferences efficiently and ethically.
</file>

<file path="docs/research/review-copilot.md">
# Review of Web Search API Analyses

## o3 mini (⭐⭐⭐)

Quick, clean table-based comparison focusing on pricing and features. Hits the main points 
but lacks depth on terms of service and real-world usage considerations. Points for 
conciseness, but misses some nuanced pricing details. The kind of analysis you'd get from 
someone who has deadlines to meet.

## Perplexity (⭐⭐⭐⭐)

A thorough breakdown with excellent attention to pricing tiers and feature sets. Good 
structure and clear categorization. However, feels a bit like it's throwing every possible 
detail at the wall to see what sticks. Still, solid research and well-presented 
information, even if it occasionally veers into "I must include everything" territory.

## Phind (⭐⭐⭐⭐½)

Strong technical focus with practical considerations and well-organized sections. The 
"Key Considerations" and "Best Practices" sections add real value. Could have gone deeper 
on API reliability and rate limits, but overall a strong analysis that someone could 
actually use to make decisions.

## Google DR (⭐⭐)

Oh my, someone got paid by the word here. Academically rigorous to the point of tedium, 
with more citations than a PhD thesis. While comprehensive, it's the kind of document 
that makes you wish for a "Skip to Conclusion" button. Points for thoroughness, but 
deductions for making simple comparisons feel like reading legal documents.

## Grok 3 (⭐⭐⭐⭐)

Refreshingly direct with clear cost comparisons and practical insights. The "Surprising 
Detail" about Bing's price surge adds valuable context. Good balance of concision and 
detail, though could use more specific implementation considerations.

## Combined Best Insights

### Top Picks

- **Budget Choice**: Brave/Exa @ $0.0025/query for keyword searches
- **Enterprise Standard**: Google @ $0.005/query with reliable service
- **Avoid**: Bing (recently 10x price hike to $0.01/query)

### Free Tier Champions

- Brave: 2,000-5,000 queries/month
- Google: 100 queries/day
- Exa: $10 free credits

### Key Decision Factors

1. **Cost-Effective**: Brave/Exa for basic keyword search
2. **AI Integration**: Exa/You.com for LLM-friendly features
3. **Privacy Focus**: Brave with no user tracking
4. **Reliability**: Google for stable, well-documented service

### Watch Out For

- Unclear pricing (You.com)
- Limited APIs (DuckDuckGo)
- Regional restrictions (Yandex/Baidu)

### Pro Tips

- Combine keyword search (cheaper) with semantic search (better accuracy) based on needs
- Consider privacy requirements before implementation
- Factor in rate limits and reliability for production use
- Get custom quotes for high-volume usage
</file>

<file path="docs/research/review-cursor.md">
# Web Search API Writeups Review

## o3 mini ⭐⭐⭐
A decent tabular comparison that gets straight to the point. The formatting is clean and the data is well-organized. However, it lacks depth in the analysis and misses some crucial pricing details. The "Final Thoughts" section feels a bit superficial. Props for including privacy considerations though.

## Perplexity ⭐⭐⭐⭐
A solid, well-structured analysis with good attention to detail on pricing tiers. The summary table is particularly useful. Loses a star for some redundant information and overly formal language. The citations are a nice touch, even if they're mostly just numbered placeholders.

## Phind ⭐⭐
Started strong with key points but quickly devolved into bullet point hell. The formatting is inconsistent and there's significant repetition. While it contains useful information, the presentation is messy and hard to follow. At least it tried to highlight surprising details about Bing's price surge.

## Google DR ⭐⭐⭐⭐⭐
The most comprehensive and well-researched analysis of the bunch. Excellent structure, detailed pricing breakdowns, and thoughtful comparisons. The methodology section adds credibility, and the citations are properly formatted. Could be shorter, but if you want the full picture, this is your guy.

## Grok 3 ⭐⭐⭐½
A concise analysis that gets to the point quickly. Good focus on cost comparison and key features. The "Surprising Detail" about Bing's price increase is well-highlighted. Loses points for being a bit too brief and missing some important details about terms and conditions.

## TLDR: The Best Combined Analysis
### Pricing Comparison
| Provider    | Cost per Query | Notes                                    |
|-------------|----------------|------------------------------------------|
| Brave/Exa   | $0.0025       | Most cost-effective for keyword search   |
| Google      | $0.005        | Standard rate after free tier            |
| Bing        | $0.01         | Premium pricing tier                     |
| DuckDuckGo  | N/A           | Instant answers API only                 |

*Prices current as of review date. Subject to change based on provider updates.*

### Best For:
- General Purpose: Google (expensive but reliable)
- Privacy Focus: Brave (independent index)
- AI/LLM Integration: Exa (semantic search)
- Developer Tools: Phind (technical focus)
- Enterprise: Microsoft/Bing (if you're made of money)

### Free Tiers:
- Google: 100 queries/day
- Brave: 2,000-5,000 queries/month
- Exa: $10 credit
- Others: Various limited offerings

### Key Takeaways:
1. The market is shifting toward AI-enhanced search APIs
2. Privacy-focused options are becoming more competitive
3. Traditional players (Google, Bing) are generally more expensive
4. Most APIs now offer semantic search capabilities
5. Watch out for hidden costs in result limits and feature tiers

### Pro Tips:
- Start with Brave or Exa for cost-effective testing
- Use Google if you need reliable, branded search
- Avoid Bing unless you really need Microsoft ecosystem integration
- Consider hybrid approaches (different APIs for different needs)
- Read the fine print on rate limits and result counts
</file>

<file path="docs/research/review-o3.md">
# Web Search API Reviews Analysis

This document analyzes three expert reviews of Web Search API writeups. Each review is evaluated on structure, clarity, depth, and overall quality, with ratings from 1-5 stars. Our assessment criteria focus on technical accuracy, insight depth, and practical value for implementers.
---

## Copilot Review Analysis

The Copilot review is organized by breaking down the original writeups into several sections (o3 mini, Perplexity, Phind, Google DR, and Grok 3) with individual star ratings for each. Its structure is methodical and the use of star ratings lends an air of simplicity and quick judgment. However, the commentary sometimes comes across as terse and overly reliant on numerical scores, leaving some nuances of the original detailed analysis underexplored. It gives off the vibe of someone racing against the clock – effective for quick insights but lacking in a deeper critique. 

**Overall, Copilot's review earns a solid 3.5/5 stars.**

---

## Cursor Review Analysis

Cursor's review stands out for its clarity and balanced critique. It methodically reviews each section (o3 mini, Perplexity, Phind, Google DR, and Grok 3) with distinct commentary that covers both strengths and shortcomings. The language is direct and informative, with a neatly organized TLDR section summarizing the best insights. Although it sometimes misses a few subtleties regarding price tiers or implementation details, its structured approach and thoughtful evaluations make it a very reliable appraisal of the original writeups. 

**Overall, Cursor's review receives a high 4.5/5 stars.**

---

## Trae Review Analysis

Trae injects personality and wit into the review, offering a narrative that is as entertaining as it is informative. His review of Google DR, for instance, is laced with sarcasm and colorful commentary – it even suggests that the document is a cure for insomnia! Trae focuses sharply on the key strengths and weaknesses (especially of Google DR and Phind), delivering actionable insights and market perspectives. However, the review sometimes feels a bit uneven as it doesn't cover all items as consistently as the others, and its less formal style may not suit every technical reader. 

**Overall, Trae's review is awarded 4/5 stars.**

---

## Summary
In summary, the three reviews each offer a unique perspective on the original Web Search API writeups:

- **Copilot (3.5/5):** A timely, if somewhat superficial, assessment with quick star ratings and concise commentary.
- **Cursor (4.5/5):** A meticulous and structured critique with balanced insights and a clear TLDR.
- **Trae (4/5):** A refreshingly candid and sarcastic review that delivers real insights, albeit with a less uniform approach.

Each reviewer brings something different to the table; whether you prefer brevity, structure, or personality, there's merit in all their approaches. The best combined insight, however, is that while the original writeups are dense and detailed, these reviews help distill the key points – even if one of them makes you long for a 'Skip to Conclusion' button.
*End of Review of the Reviews*
</file>

<file path="docs/research/review-trae.md">
# Web Search API Writeups Review

## Google DR (⭐⭐⭐⭐)
A scholarly tome that could double as a cure for insomnia. While impressively thorough and methodologically sound, it reads like a legal document had a baby with an academic paper. The detailed pricing breakdowns and citations are great, but did we really need a methodology section that rivals a doctoral dissertation? Points for completeness, deductions for making me wish I had a PhD in API Documentation Studies.

## Phind (⭐⭐⭐⭐½)
Finally, someone who understands that developers want practical insights, not a philosophical treatise on the nature of APIs. Strong technical focus, well-organized sections, and actually useful "Key Considerations" make this a standout. Could have dived deeper into reliability metrics and rate limits, but overall delivers what developers need without the fluff.

# TLDR: The Essential Guide to Web Search APIs 2025

## Top Picks by Use Case
- General Search: Google (expensive but rock-solid)
- Privacy-First: Brave (independent index, generous free tier)
- AI Integration: Exa (semantic search specialist)
- Technical Search: Phind (developer-focused)
- Enterprise: Bing (for those with Microsoft-sized budgets)

## Free Tier Breakdown
- Google: 100 queries/day
- Brave: 2,000-5,000 queries/month
- Exa: $10 credit starter

## Pro Tips
1. Mix and match: Use keyword search (cheaper) for basic queries, semantic search for complex ones
2. Watch the fine print on rate limits and result quotas
3. Privacy requirements should be considered before implementation
4. Most APIs now offer semantic search - use it wisely
5. Get custom quotes for high-volume usage - listed prices aren't the whole story

## Market Insights
- AI-enhanced search is becoming the norm
- Privacy-focused options are gaining ground
- Traditional players (Google, Bing) charge premium rates
- Hidden costs lurk in result limits and feature tiers

## Watch Out For
- Unclear pricing structures (especially You.com)
- Limited API functionality (DuckDuckGo)
- Regional restrictions (Yandex/Baidu)
- Sudden changes in rate limits or pricing

Remember: The best API is the one that fits your specific needs and budget. Don't get dazzled by features you'll never use, and always factor in reliability and support when making your choice.
</file>

<file path="docs/research/web-search-api.md">
# Search API

[You.com](https://api.you.com/plans)
[Brave Search API](https://brave.com/search/api/)
[Critique Labs](https://critique-labs.ai/en/pricing)
[Perplexity](https://docs.perplexity.ai/guides/pricing)
[Exa AI](https://exa.ai/pricing)
[SerpAPI](https://serpapi.com/pricing)
[Tavily](https://tavily.com/#pricing)
[Yandex](https://yandex.cloud/en/docs/overview/concepts/region)
[DataForSeo](https://dataforseo.com/)
[Brightdata Serp](https://brightdata.com/pricing/serp)
[TrajectData Scale Serp](https://trajectdata.com/serp/scale-serp-api/pricing/)
[Serpstack](https://serpstack.com/pricing)
[Zenserp](https://zenserp.com/pricing-plans/)
[AutomDev](https://autom.dev/subscribe/twardoch)
[APILayer](https://apilayer.com/marketplace/google_search-api)
[RapidAPI DDG](https://rapidapi.com/duckduckgo/api/duckduckgo-zero-click-info)
[RapidAPI Search Category](https://rapidapi.com/search/Search?sortBy=ByRelevance)




I want an API that does web searches. Research and compare the exact pricing and terms of Google search API, Brave, Exa, Phind, You.com, Perplexity.ai, DuckDuckGo, Bing Search and other providers.

--------------------------------------------------------------------------------

> # o3 mini

Below is a detailed comparison of several leading web search APIs that you might consider. Note that "web search APIs" can vary widely in how they index data, what exactly they return, and--critically--how much you pay per query. Here's an overview of eight major options:

## Comparison Overview

Provider                                           | Free Tier Details                                | Paid Pricing & Units                                                                                                                                                            | Key Terms & Notes
-------------------------------------------------- | ------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------
**Google Search API** (Custom Search JSON API)     | ~100 queries/day free (≈3, 000/month)            | About **$5 per 1, 000 queries**                                                                                                                                                 | Limits you to custom search results only; you cannot "crawl" the whole web. Usage must follow Google's API TOS. citeturn0search2
**Brave Search API**                               | Free: 1 query/second, up to 2, 000 queries/month | "Data for Search": Base at **$3 CPM** (i.e. $3 per 1, 000 queries) or Pro at **$5 CPM**; additional tiers for AI data, storage rights, and autosuggest are available            | Provides results from an independent index with strong privacy guarantees. Flexible, developer-first pricing. citeturn0search1
**Exa AI Web Search API**                          | Free up to 1, 000 queries                        | Starting at a minimum of **$50/month** for up to 25 results per query; higher plans (e.g. minimum $250/month) add more results with fees around **$10–$30 per 1, 000 searches** | Designed for "complex queries" with rich results. Pricing is usage‐ and result–count based and often negotiable. citeturn0search3
**Phind API**                                      | Free with limitations                            | Pro version typically around **$20/month**                                                                                                                                      | Tailored for technical queries (e.g. coding questions) with a focus on contextual, developer-friendly answers. citeturn0search3
**You.com API**                                    | Free tier available                              | YouPro subscription at about **$20/month** (or ~$180/year)                                                                                                                      | Offers a highly customizable, privacy‐conscious search experience with integrated AI tools. citeturn0search3
**Perplexity.ai API**                              | Currently free (for individual use)              | Commercial/API pricing isn't widely published (some reports hint at a Pro tier near **$20/month**)                                                                              | Presents answers conversationally with full source citations. Terms restrict commercial "wrapping" of their data. citeturn0search3
**DuckDuckGo API**                                 | The Instant Answer API is entirely free          | N/A – there is no paid tier for the Instant Answer API                                                                                                                          | Returns "instant answers" (not full search results) and emphasizes strict privacy (no tracking). citeturn0search9
**Bing Search API** (via Azure Cognitive Services) | Free tier: up to about 3, 000 transactions/month | Typically around **$3 per 1, 000 transactions** (S1 tier)                                                                                                                       | Provided through Microsoft Azure; requires an Azure subscription and adherence to Microsoft's TOS. citeturn0search4

--------------------------------------------------------------------------------

## Detailed Notes

### Google Search API

Google no longer offers a "Web Search API" that scrapes the live web. Instead, the **Custom Search JSON API** lets you query a predefined set of sites (or the entire web if you pay to configure it). It gives you a fixed number of free queries per day (around 100), then charges about $5 per 1, 000 queries. Note that you must abide by Google's usage policies and the results are limited to what the custom search engine can index.

### Brave Search API

Brave positions its API as a developer–friendly, privacy–focused alternative. In its "Data for Search" product, you can try 2, 000 queries per month for free (1 query/second), then scale up using a CPM (cost per thousand queries) pricing model. The Base plan is around $3 CPM and the Pro plan is around $5 CPM--with additional product tiers (for AI data, data with storage rights, or autosuggest functions) available as needed.

### Exa AI Web Search API

Exa is aimed at "complex" query use cases. The free tier is limited (up to 1, 000 monthly requests), and then pricing starts at about $50/month for a modest result set (around 25 results per search). Higher tiers--if you need more comprehensive results (up to 100 results per query)--require a higher minimum spend (around $250/month) and additional charges per 1, 000 queries (typically $10 for up to 25 results and up to $30 for 26–100 results).

### Phind API

Phind is built especially for technical and developer-centric queries. It's free to try but also offers a Pro subscription (commonly about $20/month) that unlocks advanced features such as extended context windows and more robust coding assistance.

### You.com API

You.com offers a robust free tier with its own AI-powered search experience. For advanced usage (called YouPro), a subscription is required--approximately $20/month or $180 billed annually. This plan unlocks more advanced models and unlimited queries, along with additional customization options.

### Perplexity.ai API

Perplexity.ai has become popular for its "conversational" search that provides concise, citation–backed answers. For now, individual use appears to be free, though commercial API access or a Pro version might be offered at around $20/month. Their TOS and usage limits emphasize that you must not "wrap" or resell their output without permission.

### DuckDuckGo API

DuckDuckGo provides a free Instant Answer API. It does not have a paid tier because its primary focus is on returning a single "instant" result (or snippet) rather than a full set of search results. Its strict privacy stance means no user tracking occurs.

### Bing Search API

Bing's web search API is available through Microsoft Azure. It offers a free tier (roughly 3, 000 transactions per month) and then charges about $3 per 1, 000 transactions in the Standard (S1) tier. Transactions include each call to the API, and you must follow Microsoft's pricing and usage policies.

--------------------------------------------------------------------------------

## Final Thoughts

Each provider offers a different mix of data access, pricing models, and terms of use:

- **Google's Custom Search** is great if you need Google's quality results but are okay with configuring your own custom search engine.
- **Brave Search API** is appealing if privacy and a truly independent index matter.
- **Exa AI** and **Phind** target more complex and technical use cases, with pricing that scales with the richness of the returned results.
- **You.com** and **Perplexity.ai** offer modern, conversational experiences that might suit creative or research–oriented applications.
- **DuckDuckGo** is best if you need a lightweight, privacy–first Instant Answer tool.
- **Bing's API** remains competitive through Azure, especially for enterprises already in the Microsoft ecosystem.

Before deciding, review each provider's full terms and pricing documentation (and consider contacting sales if your usage is high) to ensure their API aligns with your application's needs.

Feel free to ask for further details or clarification on any of these points!

--------------------------------------------------------------------------------

> # Perplexity

Here's a detailed comparison of various web search APIs, including their pricing and terms as of February 2025:

## Google Custom Search API

- **Free Tier**: 100 queries per day.

- **Paid Tier**: $5 per 1, 000 queries beyond the free limit, with a maximum of 10, 000 queries per day.

- **Notes**: This API allows you to create a custom search engine tailored to your needs. Billing must be enabled through the API console for additional queries[2].

## Brave Search API

- **Free Tier**: Up to 2, 000 queries per month (1 query per second).

- **Paid Plans**:

  - $3 per 1, 000 queries.
  - Additional features include web search, video clusters, and autosuggest capabilities.

- **Notes**: Brave emphasizes privacy and independence from traditional search engines. The API can handle up to 20 million queries per month with a rate limit of 20 queries per second[3].

## Exa Search API

- **Free Credits**: $10 in free credits for new users.

- **Pricing**:

  - $5 per 1, 000 requests for AI auto-decided searches or semantic searches.
  - $2.50 per 1, 000 requests for keyword searches.

- **Notes**: Offers various search types with different pricing based on the number of results returned (1-25 results or 26-100 results) and includes options for content retrieval and direct answers[4].

## Phind

- **Free Tier**: Basic access available.

- **Paid Plans**:

  - Phind Pro: $17/month (billed annually) for unlimited searches.
  - Business Plan: $40/month per user for advanced features and data privacy.

- **Notes**: Focuses on AI-driven search with interactive responses and integrates real-time web results. It's particularly useful for developers needing specific technical insights[5].

## You.com

- **Free Tier**: Basic access available.

- **Paid Plans**:

  - $15/month for annual billing or $20/month for monthly billing.

- **Notes**: Targets knowledge workers with complex query handling and offers a selection of AI models for enhanced search capabilities. It aims to provide summarized answers with citations[7].

## DuckDuckGo

DuckDuckGo does not offer a dedicated API like others listed here but provides a general search experience focused on privacy. They do not have a formal pricing structure for an API at this time.

## Bing Search API

Bing offers various tiers through Azure Cognitive Services:

- **Free Tier**: Limited free queries available.

- **Paid Plans**: Pricing varies based on usage but typically starts around $5 per 1, 000 transactions.

- **Notes**: It includes features like image search, video search, and news search capabilities.

## Summary Table

Provider   | Free Tier                  | Paid Plans                            | Notes
---------- | -------------------------- | ------------------------------------- | --------------------------------------
Google CSE | 100 queries/day            | $5 per 1, 000 queries                 | Customizable search engines
Brave      | Up to 2, 000 queries/month | $3 per 1, 000 queries                 | Privacy-focused
Exa        | $10 in free credits        | $5-$25 per 1k requests                | Multiple search types
Phind      | Basic access               | $17/month (Pro), $40/month (Business) | AI-driven interactive responses
You.com    | Basic access               | $15/month (annual), $20/month         | Focus on complex work queries
DuckDuckGo | None                       | N/A                                   | No formal API pricing
Bing       | Limited free queries       | Starts at ~$5 per 1k transactions     | Broad feature set including multimedia

This comparison should help you evaluate which web search API best fits your needs based on pricing and functionality.

Citations: [1] <https://www.reddit.com/r/googlecloud/comments/1i3cgq0/google_maps_api_pricing_change_after_march_1_2025/> [2] <https://stackoverflow.com/questions/43853432/google-custom-search-engine-pricing> [3] <https://brave.com/blog/search-api-launch/> [4] <https://exa.ai/pricing> [5] <https://findmyaitool.io/tool/phind/> [6] <https://www.capterra.com/p/211544/Search-API/> [7] <https://www.cnet.com/tech/services-and-software/you-com-wants-to-be-your-ai-search-engine-for-complex-work-queries/> [8] <https://team-gpt.com/blog/perplexity-pricing/> [9] <https://brightdata.com/products/serp-api/duckduckgo-search> [10] <https://www.itpro.com/business/business-strategy/370107/microsoft-hikes-bing-search-api-prices> [11] <https://ai.google.dev/pricing> [12] <https://www.googlecloudcommunity.com/gc/Cloud-Hub/Buying-Custom-Search-JSON-API-queries-for-5/m-p/648725> [13] <https://techcrunch.com/2025/01/21/perplexity-launches-sonar-an-api-for-ai-search/> [14] <https://www.phind.com/plans> [15] <https://developers.google.com/maps/documentation/places/web-service/usage-and-billing> [16] <https://ai.google.dev/gemini-api/docs/pricing> [17] <https://developers.google.com/custom-search/v1/site_restricted_api> [18] <https://developers.google.com/custom-search/v1/overview> [19] <https://hkmci.com/news/google-maps-api-2025-complete-guide-to-pricing-policies-business-strategy/> [20] <https://www.reddit.com/r/webscraping/comments/1f2s1nx/why_are_programmable_search_engine_custom_search/> [21] <https://mapsplatform.google.com/pricing-calculator/> [22] <https://support.google.com/programmable-search/thread/249304679/monthly-cost-of-using-custom-search-json-api?hl=en> [23] <https://mapsplatform.google.com/pricing/> [24] <https://cloud.google.com/generative-ai-app-builder/docs/migrate-from-cse> [25] <https://cloud.google.com/api-keys/docs/quotas> [26] <https://cloud.google.com/free/docs/free-cloud-features> [27] <https://portable.info.pl/brave-search-online/> [28] <https://search.brave.com> [29] <https://jsr.io/@tyr/brave-search> [30] <https://brave.com/ai/brave-search-api-vs-bing-api/> [31] <https://brave.com/ai/what-sets-brave-search-api-apart/> [32] <https://brave.com/ai/category/search-apis/> [33] <https://www.piloterr.com/library/brave-search> [34] <https://x.com/exaailabs> [35] <https://www.f6s.com/company-deals/exa.ai/1k-in-exa-ai-search-credits-14967> [36] <https://www.reddit.com/r/Rag/comments/1gr8jnr/which_search_api_should_i_use_between_tavilycom/> [37] <https://tavily.com> [38] <https://www.linkedin.com/posts/exa-ai_2024-was-just-the-2025-preview-stay-tuned-activity-7285063767485222913-lRGZ> [39] <https://sourceforge.net/software/compare/Exa-vs-Perplexity-AI/> [40] <https://www.linkedin.com/posts/exa-ai_2025-here-we-come-activity-7288333877855617025-ElO3> [41] <https://www.webpilot.ai/writeDetail/0cf82325-10c2-4f9c-80ec-e6560587e695> [42] <https://www.codedex.io/projects/build-a-custom-search-engine-with-exa-ai> [43] <https://aiagentstore.ai/ai-agent/exa-ai> [44] <https://sourceforge.net/software/product/Exa/> [45] <https://opentools.ai/tools/phind> [46] <https://www.phind.com> [47] <https://powerusers.ai/ai-tool/phind/> [48] <https://www.phind.com/search?cache=zl73ckwfaxfn7yl9uq35328w> [49] <https://www.reddit.com/r/ChatGPTPro/comments/1begq7r/which_is_the_best_to_pay_for_in_2024_chatgpt_pro/> [50] <https://insight7.io/ai-tools/productivity/phind/> [51] <https://www.phind.com/blog/phind-2-model-creation> [52] <https://deepinfra.com/Phind/Phind-CodeLlama-34B-v2/api> [53] <https://clickup.com/blog/ai-search-engines/> [54] <https://docs.perplexity.ai/guides/pricing> [55] <https://docs.perplexity.ai/home> [56] <https://aicoulddothat.net/tools/perplexity-ai-pricing-review-alternatives/> [57] <https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research> [58] <https://www.perplexity.ai/hub/faq/what-is-perplexity-pro> [59] <https://www.perplexity.ai/hub/faq/pplx-api> [60] <https://www.elegantthemes.com/blog/business/perplexity-ai> [61] <https://duckduckgo.com/terms> [62] <https://serpapi.com/duckduckgo-search-api> [63] <https://duckduckgo.com/duckduckgo-help-pages/get-duckduckgo/how-much-does-duckduckgo-cost/> [64] <https://duckduckgo.com/duckduckgo-help-pages/duckai/usage-limits/> [65] <https://stackoverflow.com/questions/37012469/duckduckgo-api-getting-search-results> [66] <https://duckduckgo.com> [67] <https://python.langchain.com/docs/integrations/tools/ddg/> [68] <https://azure.microsoft.com/en-us/pricing/details/cognitive-services/v5/search-api/> [69] <https://news.ycombinator.com/item?id=34832726> [70] <https://www.serphouse.com/blog/explore-bing-search-api-documentation/> [71] <https://community.openai.com/t/bing-search-now-in-chat-when-will-it-be/402844> [72] <https://brightdata.com/products/serp-api/bing-search> [73] <https://blogs.bing.com/search-quality-insights/october-2020/Bing-Search-APIS-are-Transitioning> [74] <https://www.thurrott.com/cloud/web-browsers/283850/brave-releases-its-search-api> [75] <https://sdtimes.com/api/brave-makes-its-search-api-available-for-developers/> [76] <https://search.brave.com/help/premium> [77] <https://appmaster.io/news/brave-launches-search-api> [78] <https://www.reddit.com/r/brave_browser/comments/163sqz3/dont_think_ill_be_using_brave_much_anymore_if_at/> [79] <https://brave.com/search/api/> [80] <https://news.ycombinator.com/item?id=35731705> [81] <https://aitools.fyi/exa> [82] <https://www.mexc.co/en-TR/price-prediction/exa> [83] <https://python.langchain.com/docs/integrations/tools/exa_search/> [84] <https://exa.ai> [85] <https://blog.wiseone.io/ai-search-engine-for-developers/> [86] <https://news.ycombinator.com/item?id=38092729> [87] <https://aitechfy.com/aitool/phind/> [88] <https://www.reddit.com/r/ChatGPTCoding/comments/1begxkg/which_is_the_best_to_pay_for_in_2024_chatgpt_pro/> [89] <https://aitools.inc/tools/phind> [90] <https://www.capterra.com/p/10015254/phind/> [91] <https://slashdot.org/software/p/You.com/integrations/> [92] <https://www.f6s.com/software/you-com> [93] <https://www.withorb.com/blog/perplexity-pricing> [94] <https://subscribed.fyi/perplexity/pricing/> [95] <https://docs.perplexity.ai/guides/usage-tiers> [96] <https://www.reddit.com/r/perplexity_ai/comments/1cptxxo/is_perplexity_pro_just_the_obvious_choice_for_a/> [97] <https://www.thekeyword.co/news/perplexity-launches-sonar-api-to-expand-ai-search-integration> [98] <https://www.youtube.com/watch?v=sl2YNoJbEcg> [99] <https://rapidapi.com/apiriot/api/duckduckgo-search-api/pricing> [100] <https://pypi.org/project/duckduckgo-search/> [101] <https://rapidapi.com/arsen1c/api/duckduckgo-image-search/pricing> [102] <https://zylalabs.com/api-marketplace/data/duckduckgo+search+api/3350> [103] <https://www.searchapi.io/pricing> [104] <https://apicontext.com/api-directory/search/duckduckgo/> [105] <https://www.reddit.com/r/duckduckgo/comments/1178k85/i_read_some_terrible_news_m_is_increasing_the/> [106] <https://www.reddit.com/r/bing/comments/12zfrng/search_api_cost/> [107] <https://www.computerworld.com/article/1618921/microsoft-more-than-triples-bing-search-api-prices-to-recoup-investments.html> [108] <https://www.rbn-media.com/blog/why-did-the-price-of-the-bing-web-search-api-increase-and-how-does-the-api-work/> [109] <https://searchengineland.com/bing-search-api-no-longer-free-118100> [110] <https://www.information-age.com/how-bing-api-prices-are-disrupting-search-123505076/> [111] <https://www.microsoft.com/en-us/bing/apis/pricing> [112] <https://azure.microsoft.com/en-us/pricing/details/cognitive-services/bing-entity-search-api/>

--------------------------------------------------------------------------------

> # Phind

Based on the search results, I'll compare the key search API providers and their pricing/terms. Here's a comprehensive breakdown:

### Search API Providers and Pricing

1. **Exa**

2. Starting at $50 per month

3. Offers API access for businesses and developers

4. Specializes in retrieval-augmented generation (RAG)
5. Provides real-time web data integration
6. Best for complex research queries [0]

7. **You.com (YouChat)**

8. $20 per month for YouPro

9. Includes access to premium AI models

10. Offers real-time web search capabilities
11. Integrates with content creation tools
12. Uses Google search for sources [0]

13. **Brave Search (Leo AI)**

14. $14.99 per month for premium

15. Privacy-focused search API

16. Includes multiple LLM options (Mixtral 8x7B, Claude Instant, Llama 2 13B)
17. No chat recording or user data retention [0]

18. **Microsoft Copilot/Bing**

19. $20/month for Copilot Pro

20. $30/month for commercial use

21. Includes citations and source links
22. Integrates with Microsoft 365 suite
23. Real-time Bing search integration [0]

### Additional Providers

1. **Phind**

2. $20/month for Pro

3. Specialized for technical and programming queries

4. Includes multiple AI model access
5. Real-time web search capabilities [0]

6. **Perplexity.ai**

7. Not explicitly stated in results, but offers search capabilities

8. Known for research and citation features

9. Real-time information retrieval [0]

### Key Considerations

1. **Privacy Features**

2. Brave and DuckDuckGo focus on privacy

3. Some providers like Brave don't store user data

4. Different levels of data collection across providers [0]

5. **Integration Capabilities**

6. Microsoft Copilot offers deep Microsoft 365 integration

7. Exa provides developer API access

8. You.com includes content creation tool integration [0]

9. **Search Quality**

10. Different providers use various search backends

11. Some use Google (You.com), others like Bing (Microsoft)

12. Quality and freshness of results vary [0]

### Best Practices

1. **Choose Based on Use Case**

2. Research: Exa or Perplexity

3. Privacy: Brave or DuckDuckGo

4. Enterprise: Microsoft Copilot
5. Development: Phind or Exa [0]

6. **Consider Pricing Structure**

7. Most professional services range from $14.99 to $50 per month

8. Enterprise pricing typically higher

9. Some offer free tiers with limitations [0]

10. **Evaluate Additional Features**

11. API access availability

12. Integration capabilities

13. Privacy features
14. Real-time data access [0]

--------------------------------------------------------------------------------

# Google DR

This report provides a detailed comparison of pricing and terms for various web search APIs, including Google Search API, Brave Search, Exa, Phind, You.com, Perplexity.ai, DuckDuckGo, Bing Search, and other providers. The analysis is based on extensive information gathered from the respective providers' websites and documentation, as well as reviews and comparisons from industry sources.

## **Google Search API**

Google offers the Custom Search JSON API for developers to programmatically access search results from the Programmable Search Engine. With this API, developers can send search queries and receive results without directly accessing Google's search engine1\. The API employs RESTful requests and returns results in JSON format1.

**Pricing:** 2

- Free tier: 100 search queries per day.
- Paid tier: $5 per 1000 queries, up to 10, 000 queries per day after the free tier is exhausted.

**Terms:**

- The terms of service for the Google Search Custom API are available on the Google Developers website2.

## **Brave Search API**

Brave Search API allows developers to integrate Brave Search's independent search index into their applications. Brave Search is one of the few independent global search providers and is the fastest growing since Bing3\. Unlike other search engines that may rely on third-party providers, Brave Search uses its own index, which is consistently refreshed with new data3\. This gives Brave Search a unique advantage in terms of data privacy and unbiased results3\. The API offers specialized searches beyond just web pages, allowing developers to access images, videos, news, and more3.

**Pricing:** 3

Brave Search API offers various pricing plans depending on usage and features:

Plan    | Queries/month | Queries/second | Price   | Features
:------ | :------------ | :------------- | :------ | :----------------------------------------------------------------------------------------------------------------------------------------------------
Free    | 2, 000        | 1              | Free    | Web search, Images, Videos, News, Goggles, Schema enriched web results, Discussions, Infobox, FAQ, Locations, Summarizer
Base    | 20M           | 20             | $3 CPM  | Web search, Images, Videos, News, Goggles, Schema enriched web results, Discussions, Infobox, FAQ, Locations
Pro     | Unlimited     | 50             | $5 CPM  | Web search, Images, Videos, News, Goggles, Schema enriched web results, Discussions, Infobox, FAQ, Locations
Base AI | 20M           | 20             | $5 CPM  | Web search, Goggles, news Cluster, videos cluster, Extra alternate snippets for AI
Pro AI  | Unlimited     | 50             | $9 CPM  | Web search, Goggles, news cluster, videos cluster, schema-enriched Web results, infobox, FAQ, Discussions, locations, Extra alternate snippets for AI
Base+   | 20M           | 20             | $26 CPM | Web search, Goggles, news cluster, videos cluster, Extra alternate snippets for AI
Pro+    | Unlimited     | 50             | $45 CPM | Web search, Goggles, news cluster, videos cluster, schema-enriched Web results, infobox, FAQ, Discussions, locations, Extra alternate snippets for AI

Autosuggest & Spellcheck: 3

- Free: 5 queries/second, up to 5, 000 queries/month.
- Pro: 100 queries/second, no monthly limit.

**Terms:**

- Brave Search API access is significantly cheaper than the Bing API6.
- Users can sign up for free and try the API, then choose the right plan once they assess the API's capabilities3.

## **Exa Search API**

Exa (formerly Metaphor Search) is a search engine designed for use by Large Language Models (LLMs)7\. Unlike keyword-based search engines like Google, Exa uses neural search capabilities to understand the meaning behind search queries and return more relevant results7\. For example, a search for "fascinating article about cats" on Google might return SEO-optimized listicles based on the keyword "fascinating, " while Exa would return articles that are actually about cats7.

**Pricing:** 8

- Free: $10 in free credits to get started.
- Direct answers backed by citations: $5 per query for both Exa and Exa-Pro models.

**Terms:** 9

- Exa offers a free trial with over 1000 requests per month.

## **Phind Search API**

Phind is an AI-powered search engine that provides answers, explanations, and examples for technical questions10\. It offers a Visual Studio Code extension for seamless integration into development workflows10.

**Pricing:** 10

- Free: Unlimited Phind-70B searches, 500+ daily GPT-4o uses, 500+ daily Claude 3.5 Sonnet uses, 10 daily Claude Opus uses.
- Pro Monthly: $20/month for unlimited Phind-70B searches, 500+ daily GPT-4o uses, 500+ daily Claude 3.5 Sonnet uses, 10 daily Claude Opus uses, multi-query search mode, image analysis with GPT-4, 32, 000 context length, data exclusion from training by opt-out, and the ability to run and test code in-browser.
- Pro Yearly: $17/month for the same features as Pro Monthly.
- Business: $40/month per user for the same features as Pro Monthly.

**Terms:**

No information available.

## **You.com Search API**

You.com is an AI-powered search engine that offers a personalized and efficient browsing experience11\. It prioritizes user control, allowing individuals to customize their search preferences and filter results based on their needs11.

**Pricing:** 12

- Free: Limited basic queries, real-time answers powered by live web search.
- Plus: $15/month for access to all AI models, including GPT-4o, OpenAI o1, and Claude 3.5 Sonnet, file uploads and GDrive integration up to 25MB per query, up to 64k context window, and access to Research and Custom Agents.
- Pro: $25/month for unlimited queries, including all AI models and Research and Custom Agents, unlimited file uploads and GDrive integration up to 50MB per query, up to 200k context window, zero data retention, and no-training for models.
- Team: $25/month per user for everything in Pro.
- Enterprise: Custom pricing for everything in Team, plus a dedicated account team and prioritized support, fine-grained control over permissions and access, and secure access to all your data from 100KB to 100TB+ with Private RAG.

**Terms:**

- You.com offers a free trial for up to 1, 000 monthly calls13.

## **Perplexity.ai API**

Perplexity.ai is an AI-powered search engine that provides accurate and detailed answers to complex questions14\. It uses multiple sources to ensure reliability, minimize bias, and offer a broad perspective14.

**Pricing:** 15

- Input tokens: $3/1M tokens.
- Search queries: $5/1000 searches.
- Output tokens: $15/1M tokens.

**Terms:**

- Perplexity Pro subscribers receive $5 in monthly credits16.

## **DuckDuckGo API**

DuckDuckGo is a privacy-focused search engine that does not track users or collect personal data17\. It offers unbiased search results and prioritizes user privacy17.

**Pricing:** 18

- DuckDuckGo Search, browser extensions, and DuckDuckGo Private Browser are free.
- Privacy Pro: $9.99 a month, or $99.99 a year.

**Terms:** 19

- DuckDuckGo has a strict no-logging policy.
- They use encryption and firewalls to protect user data.

## **Bing Search API**

The Bing Search API lets you add Bing search options to your app20\. It allows developers to access web, image, video, and news results, as well as related search and spelling suggestions20.

**Pricing:** 20

- Standard Pricing: Pay-as-you-go pricing for S1-S9 tiers.
- High Volume Pricing: For S10+ tiers.
- Enterprise Pricing: Custom pricing for high-volume users.

**Terms:** 20

- Bing offers a free trial for prototyping21.

## **Other Web Search API Providers**

In addition to the APIs mentioned above, several other providers offer web search APIs with varying features and pricing models. These include:

- **SerpDog API:** An all-inclusive SERP API that handles search engine tasks with one tool22.
- **SerpAPI:** A real-time SERP API that delivers structured data from various search engines, including Google Search, Google Shopping, Google Lens, YouTube, Amazon, and more22.
- **SEMrush API:** A robust set of features for SEO analysis, including backlink analysis, deep domain reports, competitor research, historical data tracking, and position tracking23.
- **Ahrefs API:** Offers similar SEO analysis tools to SEMrush, with additional features like anchor text analysis and historical refdomains data23.
- **Moz API:** Provides domain reports, keyword research, domain authority, competitor analysis, and information on anchor text and inbound and outbound links23.
- **SerpWow API:** Can query Bing, Yahoo, Baidu, and Yandex, and retrieve real-time data from Amazon and eBay23.
- **Scrapingdog API:** Offers dedicated endpoints for Amazon, LinkedIn, and Twitter, and allows populating a database using a Webhook URL23.
- **DataForSEO API:** Works with numerous search engines and has specialized search functions23.
- **ScaleSERP API:** Offers SEO tools with batch support and webhook integration23.

## **Comparison and Analysis**

When choosing a web search API, several factors should be considered, including:

- **Pricing:** The cost of API calls can vary significantly between providers. Some offer free tiers for limited usage, while others have pay-as-you-go or subscription-based models.
- **Features:** Different APIs offer different features, such as specialized searches, AI capabilities, and data parsing options.
- **Speed:** The response time of the API can be crucial for certain applications.
- **Accuracy:** The accuracy of the search results is essential for reliable information retrieval.
- **Data privacy:** Some APIs prioritize user privacy and do not track or collect personal data.

Based on the research conducted, here's a summary of the pros and cons of each API:

API Provider       | Pricing Model               | Key Features                                    | Pros                                          | Cons
:----------------- | :-------------------------- | :---------------------------------------------- | :-------------------------------------------- | :-----------------------------------
Google Search API  | Free and paid tiers         | RESTful requests, JSON responses                | Established provider, customizable search     | Limited queries per day
Brave Search API   | Free and paid tiers         | Independent search index, AI summarizer         | Privacy-focused, affordable                   | Limited features in lower tiers
Exa Search API     | Free trial and paid queries | Neural search, LLM-optimized                    | Understands natural language                  | Limited information on terms
Phind Search API   | Free and paid tiers         | VS Code extension, code generation              | Developer-focused, extensive documentation    | No API pricing information available
You.com Search API | Free and paid tiers         | Personalized search, AI agents                  | Customizable, supports various LLMs           | No API pricing information available
Perplexity.ai API  | Pay-as-you-go               | Accurate answers, source citations              | Reliable, transparent                         | Can be expensive for high usage
DuckDuckGo API     | Free and paid tiers         | Privacy-focused, unbiased results               | No tracking or data collection                | Limited API documentation
Bing Search API    | Free trial and paid tiers   | Comprehensive search options, statistics add-in | Established provider, various pricing options | Can be expensive for high usage

## **Conclusion**

This report has provided a comprehensive overview of the pricing and terms of various web search APIs. The choice of API will depend on the specific needs and priorities of the user. For those who prioritize privacy, Brave Search and DuckDuckGo are excellent options. For developers who need an API that understands natural language, Exa is a strong contender. For those who require accurate and detailed answers, Perplexity.ai is a good choice. And for those who need a comprehensive suite of SEO tools, SEMrush and Ahrefs are worth considering. By carefully evaluating the different options, users can choose the API that best meets their requirements.

### **Works cited**

1\. Google Search API: Everything You Need To Know - Medium, accessed February 20, 2025, <https://medium.com/@TerryFrederick/google-search-api-everything-you-need-to-know-fd1e24cfca45><br>
2\. Custom Search JSON API | Programmable Search Engine | Google ..., accessed February 20, 2025, <https://developers.google.com/custom-search/v1/overview><br>
3\. Brave Search API, accessed February 20, 2025, <https://brave.com/search/api/><br>
4\. Brave Releases its Search API - Thurrott.com, accessed February 20, 2025, <https://www.thurrott.com/cloud/web-browsers/283850/brave-releases-its-search-api><br>
5\. Brave releases its Search API, bringing independence and competition to the search landscape, accessed February 20, 2025, <https://brave.com/blog/search-api-launch/><br>
6\. Brave Search API vs the Bing API, accessed February 20, 2025, <https://brave.com/ai/brave-search-api-vs-bing-api/><br>
7\. Exa Search - ️ LangChain, accessed February 20, 2025, <https://python.langchain.com/v0.1/docs/integrations/tools/exa_search/><br>
8\. Pricing - Exa | Web API for AI, accessed February 20, 2025, <https://exa.ai/pricing><br>
9\. Exa: AI-Powered Search Engine for Intuitive and Efficient Web Exploration | Deepgram, accessed February 20, 2025, <https://deepgram.com/ai-apps/exa><br>
10\. Phind Features, Pricing, and Alternatives | AI Tools, accessed February 20, 2025, <https://aitools.inc/tools/phind><br>
11\. You.com's APIs make LLMs and search experiences more factual and up to date with real-time web data, accessed February 20, 2025, <https://you.com/business/api><br>
12\. Plans | You.com, accessed February 20, 2025, <https://you.com/plans><br>
13\. YOU API Frequently Asked Questions | LLM Search FAQs, accessed February 20, 2025, <https://api.you.com/faq><br>
14\. Perplexity AI Review: Ditch Google & ChatGPT For Good?, accessed February 20, 2025, <https://www.unite.ai/perplexity-ai-review/><br>
15\. Pricing - Perplexity, accessed February 20, 2025, <https://docs.perplexity.ai/guides/pricing><br>
16\. What is the API? - Perplexity, accessed February 20, 2025, <https://www.perplexity.ai/hub/faq/pplx-api><br>
17\. DuckDuckGo API - APImetrics API Directory - key data on 300+ top providers, accessed February 20, 2025, <https://apicontext.com/api-directory/search/duckduckgo/><br>
18\. How much does DuckDuckGo cost?, accessed February 20, 2025, <https://duckduckgo.com/duckduckgo-help-pages/get-duckduckgo/how-much-does-duckduckgo-cost/><br>
19\. DuckDuckGo Terms of Service, accessed February 20, 2025, <https://duckduckgo.com/terms><br>
20\. Web Search API | Microsoft Bing, accessed February 20, 2025, <https://www.microsoft.com/en-us/bing/apis/bing-web-search-api><br>
21\. document - Bing, accessed February 20, 2025, <https://www.bing.com/webmaster/content/developers/ADM_MIGRATION_GUIDE.docx><br>
22\. Top 14 Google Search APIs to Access Search Results - Smacient, accessed February 20, 2025, <https://smacient.com/top-google-search-engine-apis-3/><br>
23\. 10 Search Engine Results Page (SERP) APIs | Nordic APIs |, accessed February 20, 2025, <https://nordicapis.com/10-search-engine-results-page-serp-apis/>

--------------------------------------------------------------------------------

> # Grok 3

### Key Points

- For web search APIs, Brave and Exa offer the lowest cost at $0.0025 per query for keyword searches, with free tiers up to 5, 000 queries monthly.
- Google's API costs $0.005 per query after a free 100 queries daily, while Bing's price increased to $0.01 per query.
- Some APIs, like You.com, have unclear pricing, and others like DuckDuckGo lack full search capabilities officially.

### Overview

When choosing a web search API, consider cost, features, and suitability for your needs. Brave and Exa stand out for their low cost and privacy focus, while Google offers reliability at a higher price. Bing's recent price hike makes it less competitive, and some providers like You.com require further pricing details.

#### Cost Comparison

Here's a quick look at the pricing for popular web search APIs:

- **Brave and Exa**: Start at $0.0025 per query for keyword searches, with free tiers.
- **Google**: $0.005 per query after 100 free daily queries.
- **Bing**: Now $0.01 per query after a significant price increase.
- **You.com and Phind**: Pricing unclear or developer-focused, respectively.

#### Features and Suitability

- Brave and Exa offer semantic and keyword search options, ideal for privacy-conscious users.
- Google's API is customizable and widely used, suitable for branded searches.
- Bing provides comprehensive results but at a higher cost post-increase.
- DuckDuckGo lacks a full search API officially, and Yandex/Baidu rely on third-party options.

#### Surprising Detail: Bing's Price Surge

It's surprising that Bing's API cost jumped to $0.01 per query, a 10x increase, making it less competitive compared to newer options like Brave and Exa.

--------------------------------------------------------------------------------

### Detailed Survey Note: Web Search API Comparison

This survey note provides a comprehensive analysis of web search APIs, comparing pricing, terms, and features across multiple providers to assist in selecting the most suitable option for integration into applications. The analysis covers Google, Brave, Exa, Phind, You.com, Bing, DuckDuckGo, Yandex, and Baidu, focusing on their capabilities for general web searches.

#### Methodology

The evaluation involved researching official documentation, pricing pages, and third-party insights to gather exact pricing and terms. Each API's free tier, paid pricing structure, per-query cost, and additional features were analyzed to provide a holistic view. The focus was on APIs offering general web search functionality, excluding those limited to specific niches like developer-focused or instant answers.

#### Provider Analysis

##### Google Custom Search API

- **Pricing Structure**: Offers a free tier of 100 queries per day. Paid tier costs $5 per 1000 queries, with a daily limit of 10, 000 queries.
- **Per Query Cost**: $0.005 for paid queries, calculated as $5/1000.
- **Features**: Highly customizable, allows branding, and provides comprehensive search results. Suitable for applications needing branded search experiences.
- **Terms**: Official Google API, reliable with extensive documentation. Usage is subject to Google's terms, including potential daily limits.
- **Source**: [Custom Search JSON API overview](https://developers.google.com/custom-search/v1/overview)

##### Brave Search API

- **Pricing Structure**: Free tier ranges from 2, 000 to 5, 000 queries per month. Paid tiers vary by search type and results:

  - Auto/Neural (1-25 results): $5 per 1000 queries
  - Auto/Neural (26-100 results): $25 per 1000 queries
  - Keyword (any results): $2.5 per 1000 queries

- **Per Query Cost**:

  - Auto/Neural (1-25 results): $0.005
  - Auto/Neural (26-100 results): $0.025
  - Keyword: $0.0025

- **Features**: Independent index, privacy-focused, offers semantic (Auto/Neural) and traditional (Keyword) search options. Ideal for applications prioritizing user privacy.

- **Terms**: Official API from Brave, known for no profiling, with documentation available at [Brave Search API pricing](https://api.search.brave.com/app/pricing).
- **Notes**: The cost increases with more results for Auto/Neural, but Keyword remains cost-effective at $0.0025 per query.

##### Exa Search API

- **Pricing Structure**: Similar to Brave, with a pay-as-you-go model. Starts with $10 in free credits, no credit card required.
- **Paid Tiers**:

  - Search (per 1k requests):

    - Auto: $5 (1-25 results), $25 (26-100 results)
    - Neural: $5 (1-25 results), $25 (26-100 results)
    - Keyword: $2.5 (1-25 and 26-100 results)

  - Contents (per 1k pages): Text, Highlights, Summary at $1 each

  - Answer (per 1k answers): $5

- **Per Query Cost**: Matches Brave for search types, with Keyword at $0.0025 per query.

- **Features**: AI-powered, uses embeddings for semantic search, designed for LLMs. Offers additional content retrieval options like summaries.
- **Terms**: Official API from Exa, focused on AI applications, with custom plans for high volume. Documentation and pricing at [Exa pricing](https://exa.ai/pricing).
- **Notes**: Discounts available for startups and education, contact required for custom pricing.

##### Phind API

- **Pricing Structure**: Unofficial API available on RapidAPI. Free tier of 50 monthly requests, Pro package at $25/month for 10, 000 requests.
- **Per Query Cost**: $0.0025, calculated as $25/10, 000.
- **Features**: Developer-focused search engine, combines web results with AI for coding queries. May have limited general web search capabilities.
- **Terms**: Not an official API, potentially less reliable for general searches. Documentation via [Phind API on RapidAPI](https://rapidapi.com/umgbhallaphind-search).
- **Notes**: Suitable for developer applications, but not ideal for broad web searches due to its niche focus.

##### You.com API (Sonar)

- **Pricing Structure**: Pricing not explicitly stated in public documentation, claimed to be the cheapest AI search API.
- **Per Query Cost**: Unclear, requires contacting api@you.com for details.
- **Features**: AI-powered search for LLMs, provides real-time web access with extensive snippets. Designed for chatbot and LLM integration.
- **Terms**: Official API from You.com, with documentation at [You.com API documentation](https://documentation.you.com/welcome). May require enterprise agreements for pricing.
- **Notes**: Lack of clear pricing makes comparison challenging, but positioned as cost-effective for AI applications.

##### Bing Search API

- **Pricing Structure**: Paid tier post-increase (effective May 1, 2023) at $10 per 1000 transactions, following a 10x price hike from previous rates.
- **Per Query Cost**: $0.01, based on transaction definition as successful API calls.
- **Features**: Comprehensive search results including web, images, news, and videos. Part of Microsoft's ecosystem, with local customization options.
- **Terms**: Official API from Microsoft, with detailed documentation. Recent price surge noted in [Bing API pricing update](https://www.ghacks.net/2023/02/17/microsoft-increases-bing-search-api-pricing-by-up-to-1000/).
- **Notes**: Significant price increase makes it less competitive, previously at $0.001 per query.

##### DuckDuckGo API

- **Pricing Structure**: No official full search API; Instant Answer API available, not suitable for general web search results.
- **Per Query Cost**: Not applicable for full search, third-party APIs like SerpApi offer access but with separate pricing.
- **Features**: Privacy-focused, provides instant answers, not comprehensive search results. Third-party options at [DuckDuckGo Search API by SerpApi](https://serpapi.com/duckduckgo-search-api).
- **Terms**: Official API limited to instant answers, no full syndication rights. Third-party APIs may have additional costs and terms.

##### Yandex and Baidu

- **Pricing Structure**: No official APIs for general web search. Third-party services like SerpApi provide access, with pricing varying by provider.
- **Per Query Cost**: Not applicable officially, third-party costs depend on service (e.g., SerpApi at [Yandex Search API](https://serpapi.com/yandex-search-api)).
- **Features**: Regional search engines, Yandex for Russia, Baidu for China. Limited global relevance, third-party APIs scrape results.
- **Terms**: No official APIs, reliance on third-party scraping may have legal and reliability issues.

#### Comparative Analysis

To facilitate comparison, the following table summarizes key metrics:

Provider     | Free Tier                   | Paid Tier (Per Query Cost) | Key Features
------------ | --------------------------- | -------------------------- | --------------------------------------
Google       | 100 queries/day             | $0.005                     | Customizable, branded, comprehensive
Brave        | 2, 000-5, 000 queries/month | $0.0025 (Keyword)          | Privacy-focused, semantic options
Exa          | $10 free credits            | $0.0025 (Keyword)          | AI-powered, LLM-friendly
Phind        | 50 queries/month            | $0.0025                    | Developer-focused, limited web search
You.com      | Not clear                   | Not clear                  | AI for LLMs, real-time web access
Bing         | None                        | $0.01                      | Comprehensive, Microsoft-backed
DuckDuckGo   | No full search API          | N/A                        | Privacy-focused, instant answers only
Yandex/Baidu | No official API             | Third-party variable       | Regional, third-party scraping options

#### Discussion

For general web search APIs, Brave and Exa emerge as cost-effective options with Keyword search at $0.0025 per query, significantly lower than Google's $0.005 and Bing's post-increase $0.01\. Both offer free tiers, making them accessible for initial testing. Their privacy focus (Brave) and AI capabilities (Exa) add value, particularly for applications needing semantic search or LLM integration.

Google's API, while reliable and widely used, is more expensive, suitable for applications requiring branding and extensive customization. Bing's recent 10x price increase to $0.01 per query, as noted in [Microsoft's pricing update](https://www.ghacks.net/2023/02/17/microsoft-increases-bing-search-api-pricing-by-up-to-1000/), reduces its competitiveness. You.com's pricing remains unclear, requiring direct inquiry, which may delay decision-making.

Phind, at $0.0025 per query, is the cheapest but developer-focused, potentially limiting its use for general web searches. DuckDuckGo's lack of a full search API officially, and reliance on third-party options, makes it less viable. Yandex and Baidu, without official APIs, depend on third-party scraping, which may pose legal and reliability concerns.

#### Recommendations

Users should prioritize Brave or Exa for cost-effective, privacy-focused, and feature-rich options, especially for keyword searches at $0.0025 per query. Google is recommended for reliability and branding needs, despite higher costs. For high-volume applications, contact You.com for pricing, and note Bing's increased cost. Consider application-specific needs, such as semantic search (Exa, Brave Auto/Neural) or developer focus (Phind).

#### Key Citations

- [Custom Search JSON API overview](https://developers.google.com/custom-search/v1/overview)
- [Brave Search API pricing](https://api.search.brave.com/app/pricing)
- [Exa pricing](https://exa.ai/pricing)
- [Phind API on RapidAPI](https://rapidapi.com/umgbhallaphind-search)
- [You.com API documentation](https://documentation.you.com/welcome)
- [Bing API pricing update](https://www.ghacks.net/2023/02/17/microsoft-increases-bing-search-api-pricing-by-up-to-1000/)
- [DuckDuckGo Search API by SerpApi](https://serpapi.com/duckduckgo-search-api)
- [Yandex Search API](https://serpapi.com/yandex-search-api)
</file>

<file path="examples/check_llm_plugins.py">
"""Check which LLM plugins are installed and display results in a table."""

from __future__ import annotations

import importlib
import time
from dataclasses import dataclass
from typing import TYPE_CHECKING

from fire import Fire
from rich.console import Console
from rich.table import Table

if TYPE_CHECKING:
    from collections.abc import Sequence


@dataclass
class PackageResult:
    """Result of checking a package installation."""

    name: str
    status: str
    duration: float
    details: str = ""


def check_package(package: str) -> PackageResult:
    """Check if a package can be imported.

    Args:
        package: Name of package to check

    Returns:
        PackageResult containing status and timing information
    """
    start_time = time.time()
    try:
        importlib.import_module(package)
        duration = time.time() - start_time
        return PackageResult(package, "SUCCESS", duration)
    except ImportError as e:
        duration = time.time() - start_time
        return PackageResult(package, "FAILED", duration, str(e))


def create_results_table(results: Sequence[PackageResult]) -> Table:
    """Create formatted table of package check results.

    Args:
        results: Sequence of package check results

    Returns:
        Rich Table object with formatted results
    """
    table = Table(show_header=True, header_style="bold magenta")
    table.add_column("Package")
    table.add_column("Status", justify="center")
    table.add_column("Time (s)", justify="right")
    table.add_column("Details")

    for result in sorted(results, key=lambda x: x.duration, reverse=True):
        status_color = "green" if result.status == "SUCCESS" else "red"
        table.add_row(
            result.name,
            f"[{status_color}]{result.status}[/{status_color}]",
            f"{result.duration:.4f}",
            result.details,
        )
    return table


def check_llm_plugins(packages: Sequence[str] | None = None) -> None:
    """Check which LLM plugins are installed and display results in a table.

    Args:
        packages: Optional sequence of package names to check. If None, checks default LLM plugins.
    """
    if packages is None:
        packages = [
            "llm",
            "llm_claude_3",
            "llm_cmd",
            "llm_cmd_comp",
            "llm_consortium",
            "llm_fireworks",
            "llm_gemini",
            "llm_groq",
            "llm_groq_whisper",
            "llm_horde",
            "llm_huggingface",
            "llm_hyperbolic",
            "llm_inference",
            "llm_interpolate",
            "llm_jina_api",
            "llm_jq",
            "llm_judge",
            "llm_mistral",
            "llm_mlc",
            "llm_nomic_api_embed",
            "llm_openrouter",
            "llm_perplexity",
            "llm_python",
            "llm_reka",
            "llm_replicate",
            "llm_sambanova",
            "llm_structure",
            "llm_together",
            "llm_topology",
            "llm_utils",
            "llm_whisper_api",
        ]

    results = [check_package(pkg) for pkg in packages]
    table = create_results_table(results)
    console = Console()
    console.print(table)


if __name__ == "__main__":
    Fire(check_llm_plugins)
</file>

<file path="examples/example_people_script.py">
import requests

url = "https://nubela.co/proxycurl/api/v2/linkedin?url=https%3A%2F%2Flinkedin.com%2Fin%2Fjohnrmarty"

# Define headers as None for this example, or provide actual headers if available
headers = None  # Or your actual headers dictionary
response = requests.get(url, headers=headers, timeout=10)

print(response.json())  # noqa: T201
</file>

<file path="examples/example_serp_script.py">
import requests

url = "https://webknox-search.p.rapidapi.com/media/images/search"

querystring = {"keyword": "fontlab"}

headers = {
    "x-rapidapi-key": "fbc450445amshee120728b154b67p1505e8jsn9bdde3f1ea1d",
    "x-rapidapi-host": "webknox-search.p.rapidapi.com",
}

response = requests.get(url, headers=headers, params=querystring, timeout=10)

print(response.json())  # noqa: T201
</file>

<file path="examples/funchain_example.py">
#!/usr/bin/env python3


from mallmo import ask_chain


def tolower(s):
    return s.lower()


output = ask_chain(
    data="My name is Adam Twardoch",
    steps=[
        "Convert the full name to all caps in: $input",
        "Translate into Polish:",
        tolower,
    ],
)
</file>

<file path="src/twat_llm/__init__.py">
# SPDX-FileCopyrightText: 2024 Adam Twardoch <adam+github@twardoch.com>
#
# SPDX-License-Identifier: MIT

"""
LLM integration for twat.
"""

from __future__ import annotations

try:
    from .__version__ import __version__
except ImportError:
    # Package not installed, or __version__.py not generated by hatch-vcs
    # This is a fallback for development mode when running from source
    # without a proper build/install that generates __version__.py.
    # It's also useful if someone copies the source code without build metadata.
    __version__ = "0.0.0.dev0+unknown" # Placeholder version

# Expose main functionalities if any, e.g.:
# from .twat_llm import process_data, ActionConfig, PersonEnrichmentParams, WebSearchParams
# __all__ = ["__version__", "process_data", "ActionConfig", "PersonEnrichmentParams", "WebSearchParams"]

__all__ = ["__version__"]
</file>

<file path="src/twat_llm/mallmo.py">
#!/usr/bin/env python3
from __future__ import annotations

import io
import os
import sys
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path
from typing import TYPE_CHECKING, Any, Callable, cast

import cv2
import llm
from fire import Fire
from PIL import Image
from PIL.Image import Resampling
from tenacity import (
    RetryError,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

if TYPE_CHECKING:
    from collections.abc import Iterable, Sequence


DEFAULT_RETRY_ATTEMPTS: int = 2
MAX_STEP_TUPLE_LENGTH: int = 2


DEFAULT_FALLBACK_MODELS: list[str] = [
    "gpt-4o-mini",  # Default OpenAI model
    "openrouter/google/gemini-flash-1.5",
    "openrouter/openai/gpt-4o-mini",
    "claude-3-haiku-20240307",  # Updated Claude Haiku model ID
]


class LLMError(Exception):
    """Base exception class for LLM errors."""


class MediaProcessingError(LLMError):
    """Exception for errors during media processing."""


class ModelInvocationError(LLMError):
    """Exception for errors when invoking an LLM model."""


class BatchProcessingError(LLMError):
    """Exception for errors during batch processing."""


def _resize_image(image: Image.Image, max_size: tuple[int, int] = (512, 512)) -> bytes:
    """Resize an image while maintaining aspect ratio."""
    try:
        # Ensure image is in RGB mode for JPEG saving
        if image.mode != "RGB":
            image = image.convert("RGB")
        image.thumbnail(max_size, Resampling.LANCZOS)
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format="JPEG")
        return img_byte_arr.getvalue()
    except Exception as e:
        raise MediaProcessingError(f"Failed to resize image: {e!s}") from e


def _extract_middle_frame(video_path: Path) -> Image.Image:
    """Extract the middle frame from a video file."""
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        raise MediaProcessingError(f"Unable to open video file: {video_path}")

    try:
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames == 0:
            raise MediaProcessingError(f"Video file has no frames: {video_path}")
        middle_frame_index = total_frames // 2

        cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame_index)
        ret, frame = cap.read()

        if not ret or frame is None:
            raise MediaProcessingError(
                f"Unable to read frame at index {middle_frame_index} from video: {video_path}"
            )
        # Convert BGR (OpenCV default) to RGB for PIL
        return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    except Exception as e:
        raise MediaProcessingError(f"Failed to extract frame from video {video_path}: {e!s}") from e
    finally:
        cap.release()


def _prepare_media(path: Path) -> bytes:
    """Process image or video file and return resized image bytes."""
    file_ext = path.suffix.lower()
    try:
        if file_ext in {".mp4", ".avi", ".mov", ".mkv"}:
            image = _extract_middle_frame(path)
        elif file_ext in {".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp", ".tiff"}:
            image = Image.open(path)
        else:
            raise MediaProcessingError(f"Unsupported media file type: {file_ext} for {path}")
        return _resize_image(image)
    except FileNotFoundError as e:
        raise MediaProcessingError(f"Media file not found: {path}") from e
    except MediaProcessingError: # Re-raise specific media errors
        raise
    except Exception as e:
        raise MediaProcessingError(f"Error processing media file {path}: {e!s}") from e


@retry(
    retry=retry_if_exception_type(ModelInvocationError),
    stop=stop_after_attempt(DEFAULT_RETRY_ATTEMPTS + 1), # +1 because first attempt is not a "retry"
    wait=wait_exponential(multiplier=1, min=4, max=10),
    reraise=True,
)
def _try_model(
    prompt: str, model_id: str, attachments: list[llm.Attachment] | None = None
) -> str:
    """Try a single model with retry logic."""
    try:
        model = llm.get_model(model_id)
        if not model.supports_multimodal and attachments: # Corrected attribute name
             raise ModelInvocationError(f"Model {model_id} does not support multimodal inputs, but attachments were provided.")
        response = model.prompt(prompt, attachments=attachments)
        text_response = response.text()
        if text_response is None: # Handle cases where response.text() might be None
            raise ModelInvocationError(f"Model {model_id} returned an empty response.")
        return text_response
    except llm.UnknownModelError as e:
        raise ModelInvocationError(f"Unknown model: {model_id}. {e!s}") from e
    except RetryError: # Let tenacity's RetryError propagate if all attempts fail
        raise
    except Exception as e:
        raise ModelInvocationError(f"Error invoking model {model_id}: {e!s}") from e


def _process_step(
    step: str | Callable[..., Any] | tuple[str | Callable[..., Any], dict[str, Any]],
    current_data: str,
) -> str:
    """Process a single step in the chain and return the result."""
    processor: str | Callable[..., Any]
    kwargs: dict[str, Any] = {}

    if isinstance(step, str) or callable(step):
        processor = step
    elif isinstance(step, tuple) and 1 <= len(step) <= MAX_STEP_TUPLE_LENGTH:
        processor = step[0]
        if len(step) == MAX_STEP_TUPLE_LENGTH:
            if not isinstance(step[1], dict):
                raise TypeError(
                    f"Optional second element in step tuple must be a dictionary, got {type(step[1])}"
                )
            kwargs = cast(dict[str, Any], step[1]) # Cast for type checker
    else:
        raise TypeError(
            "Step must be a string, function, or a 1-2 element tuple (processor, kwargs_dict)."
        )

    if callable(processor):
        result = processor(current_data, **kwargs)
    elif isinstance(processor, str):
        # Type checker might complain here if 'ask' is not yet fully defined or imported in a way it can resolve.
        # For now, assuming 'ask' will be available in the scope this function is used.
        result = ask(prompt=processor, data=current_data, **kwargs) # type: ignore[misc]
    else:
        raise TypeError( # Should be unreachable due to earlier checks
            f"Step processor must be either a function or string, got {type(processor)}"
        )

    return str(result)


def ask_chain(data: str, steps: Iterable[str | Callable[..., Any] | tuple[str | Callable[..., Any], dict[str, Any]]]) -> str:
    """
    Process a chain of steps where each step is either a function call or a prompt.

    Args:
        data: The initial input string to process.
        steps: Iterable of steps. Each step can be:
            - A string (prompt).
            - A function (Callable[[str, ...], Any]).
            - A tuple: (processor: str | Callable, kwargs: dict[str, Any]).

    Returns:
        The final processed output as a string.

    Raises:
        TypeError: If step format is invalid.
        LLMError: If any LLM-related error occurs during processing.
    """
    current_data = data
    for step_item in steps: # Renamed to avoid conflict with outer 'step' if this were nested
        current_data = _process_step(step_item, current_data)
    return current_data


def ask(
    prompt: str,
    data: str | None = None,
    model_ids: Sequence[str] | None = None,
    media_paths: Sequence[Path | str] | None = None,
) -> str:
    """
    Send a prompt to LLM with optional media attachments and fallback models.

    Args:
        prompt: The text prompt to send.
        data: Optional input data to be incorporated into the prompt.
        model_ids: List of model IDs to try in order. Defaults to DEFAULT_FALLBACK_MODELS.
        media_paths: Optional list of paths to image or video files to attach.

    Returns:
        String response from the LLM.

    Raises:
        LLMError: If all models fail or media processing fails.
        MediaProcessingError: If there's an issue with a media file.
        ModelInvocationError: If a model fails to process the request.
    """
    if data:
        prompt = (
            prompt.replace("$input", data)
            if "$input" in prompt
            else f"{prompt}:\n\n<input>{data}</input>"
        )

    models_to_try_list = list(model_ids) if model_ids is not None else DEFAULT_FALLBACK_MODELS
    if not models_to_try_list: # Ensure there's at least one model to try
        raise ValueError("No model_ids provided and no default models configured.")


    attachments_list: list[llm.Attachment] | None = None
    if media_paths:
        attachments_list = []
        for p_item in media_paths:
            path = Path(p_item) # Ensure it's a Path object
            try:
                image_bytes = _prepare_media(path)
                # Assuming JPEG for all prepared media for simplicity, adjust if other formats are possible
                attachments_list.append(llm.Attachment(content=image_bytes, content_type="image/jpeg"))
            except MediaProcessingError:
                raise
            except Exception as e:
                raise MediaProcessingError(f"Unexpected error processing media file {path}: {e!s}") from e


    last_error: Exception | None = None
    for model_id in models_to_try_list:
        try:
            return _try_model(prompt, model_id, attachments_list)
        except ModelInvocationError as e:
            last_error = e
            # print(f"Model {model_id} failed: {e!s}", file=sys.stderr) # Optional: log attempt failure
            continue
        except Exception as e: # Should ideally not happen if _try_model handles its errors well
            last_error = e
            # print(f"Unexpected error with model {model_id}: {e!s}", file=sys.stderr) # Optional: log attempt failure
            continue

    if last_error:
        raise LLMError(f"All models failed. Last error: {last_error!s}") from last_error
    # This case should be rare if models_to_try_list was not empty initially
    raise LLMError("All models failed, but no specific error was captured (this indicates an issue with the model list or retry logic).")


def _process_single_prompt_for_batch(prompt_tuple: tuple[str, Sequence[str] | None]) -> str:
    """
    Helper function for parallel processing, takes a tuple of (prompt, model_ids).
    Media paths are not supported in this batch version for simplicity.
    """
    prompt, model_ids = prompt_tuple
    return ask(prompt=prompt, model_ids=model_ids, media_paths=None)


def ask_batch(
    prompts: Sequence[str],
    model_ids: Sequence[str] | None = None,
    num_processes: int | None = None,
) -> list[str]:
    """
    Process multiple prompts in parallel using concurrent.futures.ProcessPoolExecutor.
    Note: This batch version does not support media attachments per prompt.

    Args:
        prompts: List of text prompts to process.
        model_ids: Optional list of model IDs to try for each prompt.
        num_processes: Optional number of processes to use (defaults to CPU count).

    Returns:
        List of responses corresponding to the input prompts.

    Raises:
        BatchProcessingError: If parallel processing fails or any prompt processing fails.
    """
    if not prompts:
        return []

    actual_num_processes = num_processes if num_processes is not None else os.cpu_count()
    if actual_num_processes is None: # os.cpu_count() can return None
        actual_num_processes = 1 # Default to 1 if cpu_count is not available

    # Prepare arguments for map: list of (prompt, model_ids) tuples
    args_for_map = [(prompt, model_ids) for prompt in prompts]

    results: list[str]
    try:
        with ProcessPoolExecutor(max_workers=actual_num_processes) as executor:
            # executor.map processes items in order and collects results
            # It will raise the first exception encountered in a worker.
            results = list(executor.map(_process_single_prompt_for_batch, args_for_map))
        return results
    except Exception as e:
        raise BatchProcessingError(f"Batch processing failed: {e!s}") from e


def cli(
    prompt: str | None = None,
    model: str | None = None,
    media: list[str] | None = None,
    batch_prompts_file: str | None = None,
    output_file: str | None = None,
    processes: int | None = None,
) -> None:
    """
    CLI interface for LLM interactions.

    Args:
        prompt: The text prompt to send to the LLM.
        model: Optional specific model ID to use.
        media: Optional list of media file paths.
        batch_prompts_file: Path to a file with prompts (one per line) for batch processing.
        output_file: Path to save batch output (one response per line).
        processes: Number of processes for batch processing.
    """
    model_ids_list: Sequence[str] | None = [model] if model else None
    media_paths_list: Sequence[Path] | None = [Path(p) for p in media] if media else None

    try:
        if batch_prompts_file:
            try:
                with open(batch_prompts_file, "r", encoding="utf-8") as f:
                    prompts_for_batch = [line.strip() for line in f if line.strip()]
            except FileNotFoundError:
                print(f"Error: Batch prompts file not found: {batch_prompts_file}", file=sys.stderr)
                sys.exit(1)
            except Exception as e: # pylint: disable=broad-except
                print(f"Error reading batch prompts file {batch_prompts_file}: {e!s}", file=sys.stderr)
                sys.exit(1)

            if not prompts_for_batch:
                print("No prompts found in batch file.", file=sys.stderr)
                sys.exit(0)

            print(f"Processing {len(prompts_for_batch)} prompts in batch mode...")
            responses = ask_batch(
                prompts_for_batch,
                model_ids=model_ids_list,
                num_processes=processes,
            )
            if output_file:
                try:
                    with open(output_file, "w", encoding="utf-8") as f:
                        for response_text in responses:
                            f.write(response_text + "\n")
                    print(f"Batch output written to {output_file}")
                except Exception as e: # pylint: disable=broad-except
                    print(f"Error writing batch output to {output_file}: {e!s}", file=sys.stderr)
                    sys.exit(1)
            else:
                for i, response_text in enumerate(responses):
                    print(f"Response for prompt {i+1}:\n{response_text}\n---")

        elif prompt:
            response_text = ask(
                prompt,
                model_ids=model_ids_list,
                media_paths=media_paths_list,
            )
            print(response_text)
        else:
            print("Error: You must provide a 'prompt' or use '--batch_prompts_file'.", file=sys.stderr)
            sys.exit(1)

    except LLMError as e:
        print(f"LLM Error: {e!s}", file=sys.stderr)
        sys.exit(1)
    except Exception as e: # pylint: disable=broad-except
        print(f"An unexpected error occurred: {e!s}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    Fire(cli)
</file>

<file path="src/twat_llm/twat_llm.py">
#!/usr/bin/env python3
"""twat_llm:

Created by Adam Twardoch
"""

from __future__ import annotations

import logging
import os
from typing import Any, Literal, Union, Annotated # Added Annotated

import httpx
from pydantic import BaseModel, Field, HttpUrl, model_validator # Added model_validator
from pydantic_settings import BaseSettings, SettingsConfigDict

from .mallmo import ask  # Import from local mallmo module

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# --- Pydantic Models for Configuration ---
class ApiKeySettings(BaseSettings):
    """Manages API keys from environment variables."""

    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8", extra="ignore")

    proxycurl_api_key: str | None = Field(None, env="PROXYCURL_API_KEY")
    search_api_key: str | None = Field(None, env="SEARCH_API_KEY") # For chosen search provider


class PersonEnrichmentParams(BaseModel):
    """Parameters for enriching a person's profile."""
    action_type: Literal["enrich_person"] = "enrich_person"
    linkedin_profile_url: HttpUrl | None = None
    email: str | None = None
    name: str | None = None
    # Add other relevant identifiers as needed, e.g., company, location
    company_name: str | None = None
    location: str | None = None


class WebSearchParams(BaseModel):
    """Parameters for performing a web search."""
    action_type: Literal["search_web"] = "search_web"
    query: str = Field(..., description="The search query.")


AnyParams = Annotated[Union[PersonEnrichmentParams, WebSearchParams], Field(discriminator="action_type")]

class ActionConfig(BaseModel):
    """Defines an action to be performed and its parameters."""

    action_type: Literal["enrich_person", "search_web"] = Field(
        ..., description="The type of action to perform."
    )
    parameters: AnyParams
    api_keys: ApiKeySettings = Field(default_factory=ApiKeySettings)

    @model_validator(mode='before')
    @classmethod
    def _add_action_type_to_params(cls, values: Any) -> Any:
        if isinstance(values, dict):
            action_type = values.get('action_type')
            parameters = values.get('parameters')
            if action_type and isinstance(parameters, dict) and 'action_type' not in parameters:
                # Create a new dict for parameters to avoid modifying the original
                # if it's shared or immutable, though Pydantic usually handles this.
                updated_parameters = parameters.copy()
                updated_parameters['action_type'] = action_type
                values['parameters'] = updated_parameters
        return values


def process_data(config: ActionConfig, *, debug: bool = False) -> dict[str, Any]:
    """Process the input data according to the provided action configuration.

    Args:
        config: The Pydantic model containing action type, parameters, and API keys.
        debug: Enable debug mode.

    Returns:
        Processed data as a dictionary.

    Raises:
        ValueError: If configuration is invalid or action type is unsupported.
        NotImplementedError: If the specified action is not yet implemented.
    """
    if debug:
        logger.setLevel(logging.DEBUG)
        logger.debug(f"Debug mode enabled. Received config: {config.model_dump_json(indent=2)}")

    # API keys can be accessed via config.api_keys.proxycurl_api_key, etc.
    # These will be loaded from environment variables if available.

    if config.action_type == "enrich_person":
        if not isinstance(config.parameters, PersonEnrichmentParams):
            msg = "Invalid parameters for 'enrich_person' action."
            raise ValueError(msg)

        params: PersonEnrichmentParams = config.parameters
        api_key = config.api_keys.proxycurl_api_key

        if not api_key:
            msg = "PROXYCURL_API_KEY is required for person enrichment but not found."
            logger.error(msg)
            raise ValueError(msg)

        if not params.linkedin_profile_url:
            # TODO: Implement lookup by name/email if URL is not provided
            # For MVP, require LinkedIn URL.
            msg = "LinkedIn profile URL is required for person enrichment in this MVP version."
            logger.error(msg)
            raise ValueError(msg)

        headers = {"Authorization": f"Bearer {api_key}"}
        # Proxycurl Person Profile Endpoint
        # See: https://nubela.co/proxycurl/docs#people-api-person-profile-endpoint
        proxycurl_endpoint_url = "https://nubela.co/proxycurl/api/linkedin/person-profile"

        api_params = {
            "url": str(params.linkedin_profile_url),
            # Add other Proxycurl parameters as needed, e.g.:
            # 'fallback_to_cache': 'on-error',
            # 'use_cache': 'if-present',
            # 'skills': 'include',
            # 'inferred_salary': 'include',
            # 'personal_email': 'include',
            # 'personal_contact_number': 'include',
            # 'twitter_profile_id': 'include',
            # 'facebook_profile_id': 'include',
            # 'github_profile_id': 'include',
            # 'extra': 'include',
        }

        logger.info(f"Contacting Proxycurl API for LinkedIn URL: {params.linkedin_profile_url}")
        try:
            with httpx.Client() as client:
                response = client.get(proxycurl_endpoint_url, headers=headers, params=api_params, timeout=30.0)
                response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)

            profile_data = response.json()

            # Summarize with mallmo.ask
            # Construct a prompt for summarization
            # Ensure profile_data is converted to a string, perhaps JSON string
            import json
            profile_json_str = json.dumps(profile_data, indent=2)
            summary_prompt = (
                "Based on the following JSON data of a person's profile, "
                "provide a concise summary (2-3 sentences) highlighting their current role, key skills, and experience. "
                "The summary should be suitable for understanding how to improve communication with them."
                f"\n\nProfile Data:\n{profile_json_str}"
            )

            logger.info("Sending profile data to LLM for summarization.")
            summary = ask(prompt=summary_prompt) # Uses default model in mallmo

            return {
                "status": "success",
                "action": "enrich_person",
                "linkedin_url": str(params.linkedin_profile_url),
                "raw_profile_data": profile_data, # Consider if returning full raw data is always desired
                "summary": summary,
            }

        except httpx.HTTPStatusError as e:
            logger.error(f"Proxycurl API request failed: {e!s} - Response: {e.response.text}")
            # Potentially return error details or raise a custom app exception
            raise ValueError(f"Failed to retrieve data from Proxycurl: {e!s}") from e
        except httpx.RequestError as e:
            logger.error(f"Proxycurl API request failed: {e!s}")
            raise ValueError(f"Failed to connect to Proxycurl: {e!s}") from e
        except Exception as e:
            logger.exception(f"An unexpected error occurred during person enrichment: {e!s}")
            raise ValueError(f"An unexpected error occurred: {e!s}") from e

    elif config.action_type == "search_web":
        if not isinstance(config.parameters, WebSearchParams):
            msg = "Invalid parameters for 'search_web' action."
            raise ValueError(msg)

        params: WebSearchParams = config.parameters
        api_key = config.api_keys.search_api_key # Using the generic name from ApiKeySettings

        if not api_key:
            msg = "SEARCH_API_KEY is required for web search but not found."
            logger.error(msg)
            raise ValueError(msg)

        # Using Brave Search API as an example
        # Endpoint: https://api.search.brave.com/res/v1/web/search
        # Requires header: X-Subscription-Token
        brave_search_api_url = "https://api.search.brave.com/res/v1/web/search"
        headers = {"X-Subscription-Token": api_key, "Accept": "application/json"}
        api_params = {"q": params.query} # Brave uses 'q' for query

        logger.info(f"Contacting Brave Search API for query: {params.query}")
        try:
            with httpx.Client() as client:
                response = client.get(brave_search_api_url, headers=headers, params=api_params, timeout=20.0)
                response.raise_for_status()

            search_results = response.json() # Structure depends on Brave API (likely a list of results)

            # Create a string representation of search results for the LLM
            # This might involve picking top N results and formatting them.
            # For simplicity, just use the raw JSON string of all results for now.
            # A more sophisticated approach would extract titles, snippets, URLs.
            # Example:
            # formatted_results = ""
            # if "web" in search_results and "results" in search_results["web"]:
            #     for i, result in enumerate(search_results["web"]["results"][:5]): # Top 5
            #         formatted_results += f"Result {i+1}:\nTitle: {result.get('title')}\nSnippet: {result.get('description')}\nURL: {result.get('url')}\n\n"
            # else:
            #     formatted_results = "No detailed web results found."

            import json
            results_json_str = json.dumps(search_results, indent=2)

            summary_prompt = (
                "Based on the following web search results, provide a concise answer or summary "
                f"for the query: '{params.query}'. Focus on the most relevant information."
                f"\n\nSearch Results (JSON):\n{results_json_str}"
            )

            logger.info("Sending search results to LLM for summarization.")
            summary = ask(prompt=summary_prompt)

            return {
                "status": "success",
                "action": "search_web",
                "query": params.query,
                "raw_search_results": search_results, # Consider if returning all raw results is always desired
                "summary": summary,
            }

        except httpx.HTTPStatusError as e:
            logger.error(f"Brave Search API request failed: {e!s} - Response: {e.response.text}")
            raise ValueError(f"Failed to retrieve data from Search API: {e!s}") from e
        except httpx.RequestError as e:
            logger.error(f"Brave Search API request failed: {e!s}")
            raise ValueError(f"Failed to connect to Search API: {e!s}") from e
        except Exception as e:
            logger.exception(f"An unexpected error occurred during web search: {e!s}")
            raise ValueError(f"An unexpected error occurred: {e!s}") from e

    else:
        msg = f"Unsupported action type: {config.action_type}"
        raise ValueError(msg)

    # Dummy result for now
    # result: dict[str, Any] = {"status": "processing_started", "details": config.model_dump()}
    # return result


def main() -> None:
    """Main entry point for demonstrating twat_llm."""
    logger.info("Starting twat_llm demonstration...")

    # Attempt to load API keys from environment (or .env file if present)
    # For demonstration, we'll create configs. In real use, these might come from user input or another system.

    # Example 1: Enrich Person
    logger.info("\n--- Example: Person Enrichment ---")
    try:
        enrich_config = ActionConfig(
            action_type="enrich_person",
            parameters=PersonEnrichmentParams(
                linkedin_profile_url="http://linkedin.com/in/johndoe",
                name="John Doe",
                email="john.doe@example.com"
            )
        )
        logger.info(f"Attempting Person Enrichment with config: {enrich_config.model_dump_json(indent=2)}")
        enrich_result = process_data(enrich_config, debug=True)
        logger.info(f"Person Enrichment Result: {enrich_result}")
    except ValueError as e:
        logger.warning(f"Person Enrichment failed (e.g., missing API key or bad input): {e}")
    except Exception as e:
        logger.exception(f"An unexpected error occurred during person enrichment demo: {e!s}")

    # Example 2: Web Search
    logger.info("\n--- Example: Web Search ---")
    try:
        search_config = ActionConfig(
            action_type="search_web",
            parameters=WebSearchParams(query="latest advancements in large language models")
        )
        logger.info(f"Attempting Web Search with config: {search_config.model_dump_json(indent=2)}")
        search_result = process_data(search_config, debug=True)
        logger.info(f"Web Search Result: {search_result}")
    except ValueError as e:
        logger.warning(f"Web Search failed (e.g., missing API key or bad input): {e}")
    except Exception as e:
        logger.exception(f"An unexpected error occurred during web search demo: {e!s}")

    logger.info("\nTwat_llm demonstration finished.")

    # Illustrate API key loading (optional, for clarity)
    loaded_api_keys = ApiKeySettings()
    if loaded_api_keys.proxycurl_api_key:
        logger.info("Proxycurl API key found (from env or .env).")
    else:
        logger.warning(
            "Proxycurl API key not found. Set PROXYCURL_API_KEY environment variable or add to .env file."
        )
    if loaded_api_keys.search_api_key:
        logger.info("Search API key found (from env or .env).")
    else:
        logger.warning(
            "Search API key not found. Set SEARCH_API_KEY environment variable or add to .env file."
        )

    # Example of how to handle missing data error from original process_data
    # try:
    #     process_data([], config=None) # This call is no longer valid with new ActionConfig
    # except ValueError as e:
    #    logger.warning(f"Caught expected ValueError for empty data (old API): {e}")

    # Note: The original `process_data` took a `data: list[Any]` argument.
    # This has been removed in the new Pydantic-based `ActionConfig` approach,
    # as parameters are now part of the config. If `data` is still needed,
    # it should be incorporated into the Pydantic models. For now, it's omitted.
    # The following except block was causing issues, ensuring it's correctly structured or removed if not needed.
    # For now, I will assume the previous try-except blocks cover all intended error handling for main.
    # If specific error handling is needed for the main function's direct operations (not the examples),
    # it can be added back carefully.


if __name__ == "__main__":
    try:
        main()
    except Exception as e: # General catch for main, though specific examples have their own
        logger.exception(f"A general error occurred in main: {e!s}")
        raise
</file>

<file path="tests/test_mallmo.py">
"""Test suite for twat_llm.mallmo."""
import pytest
from unittest.mock import patch, MagicMock, mock_open
from pathlib import Path
import io
import sys # For capturing stdout/stderr in CLI tests
import os # For os.cpu_count

from PIL import Image
import llm # Import the llm module itself for llm.Attachment and llm.UnknownModelError

# Module to test
from twat_llm import mallmo # Assuming mallmo is now part of twat_llm package
from twat_llm.mallmo import (
    LLMError,
    MediaProcessingError,
    ModelInvocationError,
    BatchProcessingError,
    _resize_image,
    _extract_middle_frame,
    _prepare_media,
    _try_model,
    _process_step,
    ask,
    ask_chain,
    ask_batch,
    cli,
)

# A known good small PNG (1x1 pixel, black) as bytes
# (base64: iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=)
BLACK_PIXEL_PNG_BYTES = b'\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00\x01\x00\x00\x00\x01\x08\x04\x00\x00\x00\xb5\x1c\x0c\x02\x00\x00\x00\x0bIDAT\x08\x99c`\x00\x00\x00\x06\x00\x01\x00\x00 \xae\xce\xbf\x00\x00\x00\x00IEND\xaeB`\x82'

# --- Test Helper Functions ---

def test_resize_image():
    """Test image resizing functionality."""
    # Create a dummy PIL Image object
    img = Image.new('RGB', (100, 100), color = 'red')
    resized_bytes = _resize_image(img, max_size=(50, 50))
    assert isinstance(resized_bytes, bytes)
    # Further checks could involve decoding the bytes and checking dimensions
    # For simplicity, we're just checking if it returns bytes.

    # Test with RGBA image
    img_rgba = Image.new('RGBA', (100, 100), color = (255,0,0,128))
    resized_bytes_rgba = _resize_image(img_rgba, max_size=(50, 50))
    assert isinstance(resized_bytes_rgba, bytes)
    # Check if it converted to RGB (JPEG doesn't support alpha)
    img_from_bytes = Image.open(io.BytesIO(resized_bytes_rgba))
    assert img_from_bytes.mode == 'RGB'


@patch('twat_llm.mallmo.cv2.VideoCapture')
@patch('twat_llm.mallmo.cv2.cvtColor') # Mock cvtColor as well
def test_extract_middle_frame_success(mock_cvt_color, mock_video_capture):
    """Test successful extraction of a middle frame from a video."""
    mock_cap_instance = MagicMock()
    mock_cap_instance.isOpened.return_value = True
    mock_cap_instance.get.return_value = 100  # Total frames
    mock_frame_data = MagicMock() # Simulate a numpy array for the frame
    mock_cap_instance.read.return_value = (True, mock_frame_data)
    mock_video_capture.return_value = mock_cap_instance

    mock_rgb_frame = MagicMock()
    mock_cvt_color.return_value = mock_rgb_frame

    mock_image_fromarray = MagicMock(return_value="mock_pil_image")
    with patch('twat_llm.mallmo.Image.fromarray', mock_image_fromarray):
        frame = _extract_middle_frame(Path("dummy.mp4"))
        assert frame == "mock_pil_image"
        mock_cap_instance.set.assert_called_once_with(mallmo.cv2.CAP_PROP_POS_FRAMES, 50)
        mock_cvt_color.assert_called_once_with(mock_frame_data, mallmo.cv2.COLOR_BGR2RGB)
        mock_image_fromarray.assert_called_once_with(mock_rgb_frame)
        mock_cap_instance.release.assert_called_once()

@patch('twat_llm.mallmo.cv2.VideoCapture')
def test_extract_middle_frame_open_fail(mock_video_capture):
    """Test failure when video cannot be opened."""
    mock_cap_instance = MagicMock()
    mock_cap_instance.isOpened.return_value = False
    mock_video_capture.return_value = mock_cap_instance
    with pytest.raises(MediaProcessingError, match="Unable to open video file"):
        _extract_middle_frame(Path("dummy.mp4"))

@patch('twat_llm.mallmo.cv2.VideoCapture')
def test_extract_middle_frame_read_fail(mock_video_capture):
    """Test failure when frame cannot be read."""
    mock_cap_instance = MagicMock()
    mock_cap_instance.isOpened.return_value = True
    mock_cap_instance.get.return_value = 100
    mock_cap_instance.read.return_value = (False, None) # Simulate read failure
    mock_video_capture.return_value = mock_cap_instance
    with pytest.raises(MediaProcessingError, match="Unable to read frame at index 50"):
        _extract_middle_frame(Path("dummy.mp4"))

@patch('twat_llm.mallmo.Image.open')
@patch('twat_llm.mallmo._resize_image')
def test_prepare_media_image(mock_resize, mock_image_open):
    """Test preparing an image file."""
    mock_img_instance = MagicMock(spec=Image.Image)
    mock_img_instance.mode = 'RGB'
    mock_image_open.return_value = mock_img_instance
    mock_resize.return_value = b"resized_image_bytes"

    result = _prepare_media(Path("test.jpg"))
    assert result == b"resized_image_bytes"
    mock_image_open.assert_called_once_with(Path("test.jpg"))
    mock_resize.assert_called_once_with(mock_img_instance)

@patch('twat_llm.mallmo._extract_middle_frame')
@patch('twat_llm.mallmo._resize_image')
def test_prepare_media_video(mock_resize, mock_extract_frame):
    """Test preparing a video file."""
    mock_pil_image = MagicMock(spec=Image.Image)
    mock_extract_frame.return_value = mock_pil_image
    mock_resize.return_value = b"resized_video_frame_bytes"

    result = _prepare_media(Path("test.mp4"))
    assert result == b"resized_video_frame_bytes"
    mock_extract_frame.assert_called_once_with(Path("test.mp4"))
    mock_resize.assert_called_once_with(mock_pil_image)

def test_prepare_media_unsupported_type():
    """Test preparing an unsupported file type."""
    with pytest.raises(MediaProcessingError, match="Unsupported media file type: .txt"):
        _prepare_media(Path("test.txt"))

@patch('pathlib.Path.is_file', return_value=False)
@patch('twat_llm.mallmo.Image.open', side_effect=FileNotFoundError("File not found"))
def test_prepare_media_file_not_found(mock_image_open, mock_is_file):
    """Test preparing a non-existent file."""
    with pytest.raises(MediaProcessingError, match="Media file not found"):
        _prepare_media(Path("nonexistent.jpg"))


# --- Test Core LLM Interaction ---

@patch('twat_llm.mallmo.llm.get_model')
def test_try_model_success(mock_get_model):
    """Test successful model invocation."""
    mock_llm_model_instance = MagicMock(spec=llm.Model)
    mock_response = MagicMock(spec=llm.Response)
    mock_response.text.return_value = "Successful response"
    mock_llm_model_instance.prompt.return_value = mock_response
    mock_llm_model_instance.supports_multimodal = True
    mock_get_model.return_value = mock_llm_model_instance

    result = _try_model("Test prompt", "test-model")
    assert result == "Successful response"
    mock_get_model.assert_called_once_with("test-model")
    mock_llm_model_instance.prompt.assert_called_once_with("Test prompt", attachments=None)

@patch('twat_llm.mallmo.llm.get_model')
def test_try_model_failure_unknown_model(mock_get_model):
    """Test model invocation failure due to unknown model."""
    mock_get_model.side_effect = llm.UnknownModelError("Model not found")
    with pytest.raises(ModelInvocationError, match="Unknown model: test-model"):
        _try_model("Test prompt", "test-model")

@patch('twat_llm.mallmo.llm.get_model')
def test_try_model_failure_prompt_error(mock_get_model):
    """Test model invocation failure during prompt execution."""
    mock_llm_model_instance = MagicMock(spec=llm.Model)
    mock_llm_model_instance.prompt.side_effect = Exception("Prompt error")
    mock_llm_model_instance.supports_multimodal = True
    mock_get_model.return_value = mock_llm_model_instance

    with pytest.raises(ModelInvocationError, match="Error invoking model test-model: Prompt error"):
        _try_model("Test prompt", "test-model")

@patch('twat_llm.mallmo.llm.get_model')
def test_try_model_multimodal_check(mock_get_model):
    """Test model invocation with multimodal check."""
    mock_llm_model_instance = MagicMock(spec=llm.Model)
    mock_llm_model_instance.supports_multimodal = False
    mock_get_model.return_value = mock_llm_model_instance

    mock_attachment = MagicMock(spec=llm.Attachment)

    with pytest.raises(ModelInvocationError, match="Model test-model does not support multimodal inputs"):
        _try_model("Test prompt", "test-model", attachments=[mock_attachment])

@patch('twat_llm.mallmo._try_model')
def test_ask_success(mock_try_model):
    """Test ask function with a single successful model."""
    mock_try_model.return_value = "LLM response"
    result = ask("Hello", model_ids=["model1"])
    assert result == "LLM response"
    mock_try_model.assert_called_once_with("Hello", "model1", None)

@patch('twat_llm.mallmo._try_model')
def test_ask_fallback(mock_try_model):
    """Test ask function with model fallback."""
    mock_try_model.side_effect = [
        ModelInvocationError("Failed model1"),
        "Success from model2",
    ]
    result = ask("Hello", model_ids=["model1", "model2"])
    assert result == "Success from model2"
    assert mock_try_model.call_count == 2

@patch('twat_llm.mallmo._try_model', side_effect=ModelInvocationError("Model failed"))
def test_ask_all_models_fail(mock_try_model):
    """Test ask function when all models fail."""
    with pytest.raises(LLMError, match="All models failed. Last error: Model failed"):
        ask("Hello", model_ids=["model1", "model2"])
    # Each model is tried (DEFAULT_RETRY_ATTEMPTS + 1) times.
    # Since _try_model itself is mocked here and raises immediately,
    # it will be called once for each model in the list.
    assert mock_try_model.call_count == 2

@patch('twat_llm.mallmo._prepare_media', return_value=b"imagedata")
@patch('twat_llm.mallmo._try_model')
@patch('twat_llm.mallmo.llm.Attachment') # Mock the Attachment class itself
def test_ask_with_media(MockAttachment, mock_try_model, mock_prepare_media): # Renamed mock
    """Test ask function with media paths."""
    mock_try_model.return_value = "Response with media"
    media_path = Path("dummy.jpg")

    mock_attachment_instance = MagicMock() # Removed spec=llm.Attachment
    MockAttachment.return_value = mock_attachment_instance # Mock constructor

    result = ask("Describe this image", media_paths=[media_path])
    assert result == "Response with media"
    mock_prepare_media.assert_called_once_with(media_path)
    MockAttachment.assert_called_once_with(content=b"imagedata", content_type="image/jpeg") # Check constructor call
    mock_try_model.assert_called_once_with("Describe this image", mallmo.DEFAULT_FALLBACK_MODELS[0], [mock_attachment_instance])


def test_process_step_string():
    """Test _process_step with a string (prompt)."""
    with patch('twat_llm.mallmo.ask', return_value="Processed: Hello") as mock_ask_func:
        result = _process_step("Hello", "initial_data")
        assert result == "Processed: Hello"
        mock_ask_func.assert_called_once_with(prompt="Hello", data="initial_data")

def test_process_step_function():
    """Test _process_step with a callable."""
    mock_func = MagicMock(return_value="Function output")
    result = _process_step(mock_func, "initial_data")
    assert result == "Function output"
    mock_func.assert_called_once_with("initial_data")

def test_process_step_tuple():
    """Test _process_step with a tuple (processor, kwargs)."""
    mock_func = MagicMock(return_value="Function output with kwargs")
    kwargs = {"key": "value"}
    result = _process_step((mock_func, kwargs), "initial_data")
    assert result == "Function output with kwargs"
    mock_func.assert_called_once_with("initial_data", key="value")

def test_process_step_invalid_type():
    """Test _process_step with an invalid type."""
    with pytest.raises(TypeError, match="Step must be a string, function, or a 1-2 element tuple"):
        _process_step(123, "initial_data") # type: ignore

def test_process_step_invalid_tuple_format():
    """Test _process_step with an invalid tuple format."""
    with pytest.raises(TypeError, match="Optional second element in step tuple must be a dictionary"):
        _process_step(("prompt", "not_a_dict"), "initial_data") # type: ignore

def test_ask_chain_simple():
    """Test ask_chain with simple string prompts."""
    with patch('twat_llm.mallmo.ask') as mock_ask_func:
        def side_effect_func(prompt, data, **kwargs): # Add **kwargs to match 'ask' signature
            return f"{prompt} processed with {data}"
        mock_ask_func.side_effect = side_effect_func

        steps = ["Step1", "Step2"]
        result = ask_chain("Initial", steps)
        assert result == "Step2 processed with Step1 processed with Initial"
        assert mock_ask_func.call_count == 2
        mock_ask_func.assert_any_call(prompt="Step1", data="Initial")
        mock_ask_func.assert_any_call(prompt="Step2", data="Step1 processed with Initial")

@patch('twat_llm.mallmo.ProcessPoolExecutor')
def test_ask_batch_success(MockProcessPoolExecutor):
    """Test ask_batch successful processing."""
    prompts = ["prompt1", "prompt2"]
    expected_responses = ["response1", "response2"]

    mock_executor_instance = MagicMock()
    mock_executor_instance.map.return_value = iter(expected_responses) # map returns an iterator
    MockProcessPoolExecutor.return_value.__enter__.return_value = mock_executor_instance

    responses = ask_batch(prompts, model_ids=["test-model"])

    assert responses == expected_responses
    mock_executor_instance.map.assert_called_once()
    # Check arguments passed to map
    map_args = mock_executor_instance.map.call_args[0]
    assert map_args[0] == mallmo._process_single_prompt_for_batch
    # Convert generator to list for comparison
    assert list(map_args[1]) == [("prompt1", ["test-model"]), ("prompt2", ["test-model"])]


def test_ask_batch_empty_prompts():
    """Test ask_batch with no prompts."""
    responses = ask_batch([])
    assert responses == []

@patch('twat_llm.mallmo.ProcessPoolExecutor')
def test_ask_batch_processing_error_in_worker(MockProcessPoolExecutor):
    """Test ask_batch when a worker in ProcessPoolExecutor raises an error."""
    prompts = ["prompt1", "prompt2"]

    mock_executor_instance = MagicMock()
    # Simulate one of the map calls raising an exception
    def map_side_effect(func, iterables):
        results = []
        for item in iterables:
            if item[0] == "prompt2": # Let's say processing prompt2 fails
                raise ValueError("Worker failed")
            results.append(f"processed_{item[0]}")
        return iter(results)

    mock_executor_instance.map.side_effect = BatchProcessingError("Worker failed") # More direct way to test this
    MockProcessPoolExecutor.return_value.__enter__.return_value = mock_executor_instance

    with pytest.raises(BatchProcessingError, match="Batch processing failed: Worker failed"):
        ask_batch(prompts, model_ids=["test-model"])


# --- CLI Tests ---

@patch('twat_llm.mallmo.ask')
def test_cli_simple_prompt(mock_ask_func, capsys):
    """Test CLI with a simple prompt."""
    mock_ask_func.return_value = "CLI response"
    cli(prompt="Hello CLI", model="test-cli-model")
    captured = capsys.readouterr()
    assert "CLI response" in captured.out
    mock_ask_func.assert_called_once_with("Hello CLI", model_ids=["test-cli-model"], media_paths=None)

@patch('twat_llm.mallmo.ask_batch')
def test_cli_batch_prompt_file(mock_ask_batch, capsys, tmp_path):
    """Test CLI with batch prompts from a file."""
    prompt_file = tmp_path / "prompts.txt"
    prompt_file.write_text("Prompt 1\nPrompt 2")

    mock_ask_batch.return_value = ["Response 1", "Response 2"]

    cli(batch_prompts_file=str(prompt_file), model="test-batch-model")

    captured = capsys.readouterr()
    assert "Processing 2 prompts in batch mode..." in captured.out
    assert "Response for prompt 1:\nResponse 1" in captured.out
    assert "Response for prompt 2:\nResponse 2" in captured.out
    mock_ask_batch.assert_called_once_with(
        ["Prompt 1", "Prompt 2"],
        model_ids=["test-batch-model"],
        num_processes=None
    )

@patch('twat_llm.mallmo.ask_batch')
def test_cli_batch_prompt_output_file(mock_ask_batch, tmp_path, capsys):
    """Test CLI with batch prompts and output to a file."""
    prompt_file = tmp_path / "prompts.txt"
    prompt_file.write_text("Prompt 1\nPrompt 2")
    output_file_path = tmp_path / "output.txt"

    mock_ask_batch.return_value = ["Response 1", "Response 2"]

    # Capture print statements to stdout for verification
    with patch('sys.stdout', new_callable=io.StringIO) as mock_stdout:
        cli(batch_prompts_file=str(prompt_file), output_file=str(output_file_path))

    mock_ask_batch.assert_called_once_with(
        ["Prompt 1", "Prompt 2"],
        model_ids=None,
        num_processes=None
    )
    assert output_file_path.read_text() == "Response 1\nResponse 2\n"
    assert f"Batch output written to {str(output_file_path)}" in mock_stdout.getvalue()


def test_cli_no_prompt_or_batch_file(capsys):
    """Test CLI when neither prompt nor batch_prompts_file is provided."""
    with pytest.raises(SystemExit) as e:
        cli()
    assert e.value.code == 1
    captured = capsys.readouterr()
    assert "Error: You must provide a 'prompt' or use '--batch_prompts_file'." in captured.err

@patch('twat_llm.mallmo.ask', side_effect=LLMError("Test LLM Error"))
def test_cli_llm_error(mock_ask, capsys):
    """Test CLI handling of LLMError."""
    with pytest.raises(SystemExit) as e:
        cli(prompt="test")
    assert e.value.code == 1
    captured = capsys.readouterr()
    assert "LLM Error: Test LLM Error" in captured.err

@patch('builtins.open', new_callable=mock_open)
@patch('twat_llm.mallmo.ask_batch') # This mock is not used but good for consistency
def test_cli_batch_file_read_error(mock_ask_batch, mock_file_open, capsys):
    """Test CLI with batch prompts file that causes a read error."""
    mock_file_open.side_effect = IOError("Cannot read file")

    with pytest.raises(SystemExit) as e:
        cli(batch_prompts_file="nonexistent_or_unreadable.txt")
    assert e.value.code == 1
    captured = capsys.readouterr()
    assert "Error reading batch prompts file" in captured.err
    mock_ask_batch.assert_not_called()


@patch('twat_llm.mallmo.ask_batch')
def test_cli_batch_empty_file(mock_ask_batch, capsys, tmp_path):
    """Test CLI with an empty batch prompts file."""
    prompt_file = tmp_path / "empty_prompts.txt"
    prompt_file.write_text("") # Empty file

    # The cli function exits with 0 if the batch file is empty but valid.
    # We need to capture sys.exit to test this.
    with pytest.raises(SystemExit) as e:
        cli(batch_prompts_file=str(prompt_file))
    assert e.value.code == 0

    captured = capsys.readouterr()
    assert "No prompts found in batch file." in captured.err
    mock_ask_batch.assert_not_called()
</file>

<file path="tests/test_twat_llm.py">
"""Test suite for twat_llm."""

import pytest
from unittest.mock import patch, MagicMock
import pydantic # Added for ValidationError
import httpx # Added for HTTPStatusError and RequestError

# Assuming your Pydantic models and process_data are in src.twat_llm.twat_llm
# Adjust the import path if your structure is different after hatch build/install
from twat_llm.twat_llm import (
    ActionConfig,
    ApiKeySettings,
    PersonEnrichmentParams,
    WebSearchParams,
    process_data,
)


def test_version():
    """Verify package exposes version."""
    import twat_llm
    assert twat_llm.__version__

# --- Pydantic Model Tests ---

def test_action_config_enrich_person_valid():
    """Test valid ActionConfig for person enrichment."""
    config_data = {
        "action_type": "enrich_person",
        "parameters": {"name": "John Doe", "email": "john@example.com"}
    }
    config = ActionConfig(**config_data)
    assert config.action_type == "enrich_person"
    assert isinstance(config.parameters, PersonEnrichmentParams)
    assert config.parameters.name == "John Doe"

def test_action_config_search_web_valid():
    """Test valid ActionConfig for web search."""
    config_data = {
        "action_type": "search_web",
        "parameters": {"query": "test query"}
    }
    config = ActionConfig(**config_data)
    assert config.action_type == "search_web"
    assert isinstance(config.parameters, WebSearchParams)
    assert config.parameters.query == "test query"

def test_action_config_invalid_action_type():
    """Test ActionConfig with an invalid action type."""
    config_data = {
        "action_type": "unknown_action",
        "parameters": {"query": "test query"}
    }
    with pytest.raises(ValueError): # Pydantic raises ValueError for invalid Literal
        ActionConfig(**config_data)

def test_action_config_enrich_person_missing_params():
    """Test ActionConfig for enrich_person with missing required parameters (if any)."""
    # PersonEnrichmentParams has all optional fields for now, so this might pass.
    # If fields become mandatory, this test would need adjustment.
    config_data = {
        "action_type": "enrich_person",
        "parameters": {}
    }
    config = ActionConfig(**config_data)
    assert isinstance(config.parameters, PersonEnrichmentParams)

def test_action_config_search_web_missing_query():
    """Test ActionConfig for search_web with missing query."""
    config_data = {
        "action_type": "search_web",
        "parameters": {} # Missing 'query'
    }
    with pytest.raises(pydantic.ValidationError): # Pydantic raises ValidationError for missing fields
        ActionConfig(**config_data)

# --- ApiKeySettings Tests ---

def test_api_key_settings_load_from_env(monkeypatch):
    """Test ApiKeySettings loads from environment variables."""
    monkeypatch.setenv("PROXYCURL_API_KEY", "test_proxycurl_key")
    monkeypatch.setenv("SEARCH_API_KEY", "test_search_key")

    settings = ApiKeySettings()
    assert settings.proxycurl_api_key == "test_proxycurl_key"
    assert settings.search_api_key == "test_search_key"

def test_api_key_settings_no_env():
    """Test ApiKeySettings when no environment variables are set."""
    # Assuming no relevant env vars are set by default in the test environment
    settings = ApiKeySettings()
    assert settings.proxycurl_api_key is None
    assert settings.search_api_key is None

# --- process_data Tests (to be expanded) ---

# Placeholder for further tests - we will build these out
# For now, the structure for process_data tests will be set up.

@patch("twat_llm.twat_llm.httpx.Client")
@patch("twat_llm.twat_llm.ask")
def test_process_data_enrich_person_success(mock_mallmo_ask, MockHttpClient, monkeypatch):
    """Test successful person enrichment."""
    monkeypatch.setenv("PROXYCURL_API_KEY", "fake_proxy_key")

    # Mock Proxycurl API response
    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {"linkedin_profile": "data", "full_name": "John Doe"}

    # Configure the mock client instance
    mock_client_instance = MockHttpClient.return_value.__enter__.return_value
    mock_client_instance.get.return_value = mock_response

    # Mock mallmo.ask response
    mock_mallmo_ask.return_value = "LLM summary of profile"

    config = ActionConfig(
        action_type="enrich_person",
        parameters=PersonEnrichmentParams(linkedin_profile_url="http://linkedin.com/in/johndoe")
    )

    result = process_data(config)

    assert result["status"] == "success"
    assert result["summary"] == "LLM summary of profile"
    assert result["raw_profile_data"]["full_name"] == "John Doe"
    mock_client_instance.get.assert_called_once()
    # Ensure the prompt sent to mallmo.ask is reasonable
    call_kwargs = mock_mallmo_ask.call_args.kwargs
    assert "John Doe" in call_kwargs['prompt']  # Check if some profile data is in the prompt
    mock_mallmo_ask.assert_called_once()

def test_process_data_enrich_person_missing_api_key(monkeypatch):
    """Test person enrichment when PROXYCURL_API_KEY is missing."""
    monkeypatch.delenv("PROXYCURL_API_KEY", raising=False) # Ensure it's not set
    config = ActionConfig(
        action_type="enrich_person",
        parameters=PersonEnrichmentParams(linkedin_profile_url="http://linkedin.com/in/johndoe") # type: ignore
    )
    with pytest.raises(ValueError, match="PROXYCURL_API_KEY is required"):
        process_data(config)

def test_process_data_enrich_person_missing_linkedin_url(monkeypatch):
    """Test person enrichment when LinkedIn URL is missing (as per MVP)."""
    monkeypatch.setenv("PROXYCURL_API_KEY", "fake_proxy_key")
    config = ActionConfig(
        action_type="enrich_person",
        parameters=PersonEnrichmentParams(name="John Doe") # No URL
    )
    with pytest.raises(ValueError, match="LinkedIn profile URL is required"):
        process_data(config)

@patch("twat_llm.twat_llm.httpx.Client")
def test_process_data_enrich_person_api_http_error(MockHttpClient, monkeypatch):
    """Test person enrichment when Proxycurl API returns an HTTP error."""
    monkeypatch.setenv("PROXYCURL_API_KEY", "fake_proxy_key")

    mock_response = MagicMock()
    mock_response.status_code = 401 # Unauthorized
    mock_response.text = "Invalid API key"
    mock_response.raise_for_status.side_effect = httpx.HTTPStatusError(
        "Client error '401 Unauthorized'", request=MagicMock(), response=mock_response
    )

    mock_client_instance = MockHttpClient.return_value.__enter__.return_value
    mock_client_instance.get.return_value = mock_response

    config = ActionConfig(
        action_type="enrich_person",
        parameters=PersonEnrichmentParams(linkedin_profile_url="http://linkedin.com/in/johndoe") # type: ignore
    )

    with pytest.raises(ValueError, match="Failed to retrieve data from Proxycurl"):
        process_data(config)

@patch("twat_llm.twat_llm.httpx.Client")
def test_process_data_enrich_person_api_request_error(MockHttpClient, monkeypatch):
    """Test person enrichment when there's a request error (e.g., network)."""
    monkeypatch.setenv("PROXYCURL_API_KEY", "fake_proxy_key")

    mock_client_instance = MockHttpClient.return_value.__enter__.return_value
    mock_client_instance.get.side_effect = httpx.RequestError("Network error", request=MagicMock())

    config = ActionConfig(
        action_type="enrich_person",
        parameters=PersonEnrichmentParams(linkedin_profile_url="http://linkedin.com/in/johndoe") # type: ignore
    )

    with pytest.raises(ValueError, match="Failed to connect to Proxycurl"):
        process_data(config)

@patch("twat_llm.twat_llm.httpx.Client")
@patch("twat_llm.twat_llm.ask")
def test_process_data_enrich_person_mallmo_error(mock_mallmo_ask, MockHttpClient, monkeypatch):
    """Test person enrichment when mallmo.ask raises an error."""
    monkeypatch.setenv("PROXYCURL_API_KEY", "fake_proxy_key")

    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {"linkedin_profile": "data", "full_name": "John Doe"}

    mock_client_instance = MockHttpClient.return_value.__enter__.return_value
    mock_client_instance.get.return_value = mock_response

    mock_mallmo_ask.side_effect = Exception("LLM unavailable")

    config = ActionConfig(
        action_type="enrich_person",
        parameters=PersonEnrichmentParams(linkedin_profile_url="http://linkedin.com/in/johndoe") # type: ignore
    )

    with pytest.raises(ValueError, match="An unexpected error occurred"): # General error wrapping
        process_data(config)


# --- Tests for search_web action ---

@patch("twat_llm.twat_llm.httpx.Client")
@patch("twat_llm.twat_llm.ask")
def test_process_data_search_web_success(mock_mallmo_ask, MockHttpClient, monkeypatch):
    """Test successful web search."""
    monkeypatch.setenv("SEARCH_API_KEY", "fake_search_key")

    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {"web": {"results": [{"title": "Test Result"}]}}

    mock_client_instance = MockHttpClient.return_value.__enter__.return_value
    mock_client_instance.get.return_value = mock_response

    mock_mallmo_ask.return_value = "LLM summary of search results"

    config = ActionConfig(
        action_type="search_web",
        parameters=WebSearchParams(query="test query")
    )

    result = process_data(config)

    assert result["status"] == "success"
    assert result["summary"] == "LLM summary of search results"
    assert result["raw_search_results"]["web"]["results"][0]["title"] == "Test Result"
    mock_client_instance.get.assert_called_once()
    # Ensure the prompt sent to mallmo.ask is reasonable
    call_kwargs = mock_mallmo_ask.call_args.kwargs
    assert "Test Result" in call_kwargs['prompt']  # Check if some search data is in the prompt
    mock_mallmo_ask.assert_called_once()

def test_process_data_search_web_missing_api_key(monkeypatch):
    """Test web search when SEARCH_API_KEY is missing."""
    monkeypatch.delenv("SEARCH_API_KEY", raising=False)
    config = ActionConfig(
        action_type="search_web",
        parameters=WebSearchParams(query="test query")
    )
    with pytest.raises(ValueError, match="SEARCH_API_KEY is required"):
        process_data(config)

@patch("twat_llm.twat_llm.httpx.Client")
def test_process_data_search_web_api_http_error(MockHttpClient, monkeypatch):
    """Test web search when Search API returns an HTTP error."""
    monkeypatch.setenv("SEARCH_API_KEY", "fake_search_key")

    mock_response = MagicMock()
    mock_response.status_code = 403 # Forbidden
    mock_response.text = "Subscription inactive"
    mock_response.raise_for_status.side_effect = httpx.HTTPStatusError(
        "Client error '403 Forbidden'", request=MagicMock(), response=mock_response
    )

    mock_client_instance = MockHttpClient.return_value.__enter__.return_value
    mock_client_instance.get.return_value = mock_response

    config = ActionConfig(
        action_type="search_web",
        parameters=WebSearchParams(query="test query")
    )

    with pytest.raises(ValueError, match="Failed to retrieve data from Search API"):
        process_data(config)

@patch("twat_llm.twat_llm.httpx.Client")
def test_process_data_search_web_api_request_error(MockHttpClient, monkeypatch):
    """Test web search when there's a request error."""
    monkeypatch.setenv("SEARCH_API_KEY", "fake_search_key")

    mock_client_instance = MockHttpClient.return_value.__enter__.return_value
    mock_client_instance.get.side_effect = httpx.RequestError("Connection refused", request=MagicMock())

    config = ActionConfig(
        action_type="search_web",
        parameters=WebSearchParams(query="test query")
    )

    with pytest.raises(ValueError, match="Failed to connect to Search API"):
        process_data(config)

@patch("twat_llm.twat_llm.httpx.Client")
@patch("twat_llm.twat_llm.ask")
def test_process_data_search_web_mallmo_error(mock_mallmo_ask, MockHttpClient, monkeypatch):
    """Test web search when mallmo.ask raises an error."""
    monkeypatch.setenv("SEARCH_API_KEY", "fake_search_key")

    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {"web": {"results": [{"title": "Test Result"}]}}

    mock_client_instance = MockHttpClient.return_value.__enter__.return_value
    mock_client_instance.get.return_value = mock_response

    mock_mallmo_ask.side_effect = Exception("LLM processing failed")

    config = ActionConfig(
        action_type="search_web",
        parameters=WebSearchParams(query="test query")
    )

    with pytest.raises(ValueError, match="An unexpected error occurred"):
        process_data(config)
</file>

<file path=".gitignore">
*_autogen/
.DS_Store
__version__.py
__pycache__/
_Chutzpah*
_deps
_NCrunch_*
_pkginfo.txt
_Pvt_Extensions
_ReSharper*/
_TeamCity*
_UpgradeReport_Files/
!?*.[Cc]ache/
!.axoCover/settings.json
!.vscode/extensions.json
!.vscode/launch.json
!.vscode/settings.json
!.vscode/tasks.json
!**/[Pp]ackages/build/
!Directory.Build.rsp
.*crunch*.local.xml
.axoCover/*
.builds
.cr/personal
.fake/
.history/
.ionide/
.localhistory/
.mfractor/
.ntvs_analysis.dat
.paket/paket.exe
.sass-cache/
.vs/
.vscode
.vscode/*
.vshistory/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
[Bb]in/
[Bb]uild[Ll]og.*
[Dd]ebug/
[Dd]ebugPS/
[Dd]ebugPublic/
[Ee]xpress/
[Ll]og/
[Ll]ogs/
[Oo]bj/
[Rr]elease/
[Rr]eleasePS/
[Rr]eleases/
[Tt]est[Rr]esult*/
[Ww][Ii][Nn]32/
*_h.h
*_i.c
*_p.c
*_wpftmp.csproj
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl
*- [Bb]ackup.rdl
*.[Cc]ache
*.[Pp]ublish.xml
*.[Rr]e[Ss]harper
*.a
*.app
*.appx
*.appxbundle
*.appxupload
*.aps
*.azurePubxml
*.bim_*.settings
*.bim.layout
*.binlog
*.btm.cs
*.btp.cs
*.build.csdef
*.cab
*.cachefile
*.code-workspace
*.coverage
*.coveragexml
*.d
*.dbmdl
*.dbproj.schemaview
*.dll
*.dotCover
*.DotSettings.user
*.dsp
*.dsw
*.dylib
*.e2e
*.exe
*.gch
*.GhostDoc.xml
*.gpState
*.ilk
*.iobj
*.ipdb
*.jfm
*.jmconfig
*.la
*.lai
*.ldf
*.lib
*.lo
*.log
*.mdf
*.meta
*.mm.*
*.mod
*.msi
*.msix
*.msm
*.msp
*.ncb
*.ndf
*.nuget.props
*.nuget.targets
*.nupkg
*.nvuser
*.o
*.obj
*.odx.cs
*.opendb
*.opensdf
*.opt
*.out
*.pch
*.pdb
*.pfx
*.pgc
*.pgd
*.pidb
*.plg
*.psess
*.publishproj
*.publishsettings
*.pubxml
*.pyc
*.rdl.data
*.rptproj.bak
*.rptproj.rsuser
*.rsp
*.rsuser
*.sap
*.sbr
*.scc
*.sdf
*.sln.docstates
*.sln.iml
*.slo
*.smod
*.snupkg
*.so
*.suo
*.svclog
*.tlb
*.tlh
*.tli
*.tlog
*.tmp
*.tmp_proj
*.tss
*.user
*.userosscache
*.userprefs
*.vbp
*.vbw
*.VC.db
*.VC.VC.opendb
*.VisualState.xml
*.vsp
*.vspscc
*.vspx
*.vssscc
*.xsd.cs
**/[Pp]ackages/*
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.HTMLClient/GeneratedArtifacts
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
*~
~$*
$tf/
AppPackages/
artifacts/
ASALocalRun/
AutoTest.Net/
Backup*/
BenchmarkDotNet.Artifacts/
bld/
BundleArtifacts/
ClientBin/
cmake_install.cmake
CMakeCache.txt
CMakeFiles
CMakeLists.txt.user
CMakeScripts
CMakeUserPresets.json
compile_commands.json
coverage*.info
coverage*.json
coverage*.xml
csx/
CTestTestfile.cmake
dlldata.c
DocProject/buildhelp/
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/*.HxC
DocProject/Help/*.HxT
DocProject/Help/html
DocProject/Help/Html2
ecf/
FakesAssemblies/
FodyWeavers.xsd
Generated_Code/
Generated\ Files/
healthchecksdb
install_manifest.txt
ipch/
Makefile
MigrationBackup/
mono_crash.*
nCrunchTemp_*
node_modules/
nunit-*.xml
OpenCover/
orleans.codegen.cs
Package.StoreAssociation.xml
paket-files/
project.fragment.lock.json
project.lock.json
publish/
PublishScripts/
rcf/
ScaffoldingReadMe.txt
ServiceFabricBackup/
StyleCopReport.xml
Testing
TestResult.xml
UpgradeLog*.htm
UpgradeLog*.XML
x64/
x86/
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Distribution / packaging
!dist/.gitkeep

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
.ruff_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
__version__.py
_private

# Generated files
CLEANUP.txt
REPO_CONTENT.txt
</file>

<file path=".pre-commit-config.yaml">
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.5.0 # Updated Ruff version
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.11.0 # Use a recent version of mypy
    hooks:
      - id: mypy
        args: [--install-types, --non-interactive]
        # additional_dependencies are managed by hatch/uv in the main env
        # but for pre-commit, specify necessary types here if not picked up automatically
        # For example:
        # additional_dependencies: ['types-Pillow', 'types-tenacity']
        # However, it's often better to rely on mypy finding these from the project's venv
        # if pre-commit is run within an activated venv or if mypy is configured project-wide.
        # For now, keeping it simple. MyPy will use project's pyproject.toml for config.
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.6.0 # Updated pre-commit-hooks version
    hooks:
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf]
      - id: end-of-file-fixer
      - id: trailing-whitespace
</file>

<file path="cleanup.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = [
#   "ruff>=0.9.6",
#   "pytest>=8.3.4",
#   "mypy>=1.15.0",
# ]
# ///
# this_file: cleanup.py

"""
Cleanup tool for managing repository tasks and maintaining code quality.

This script provides a comprehensive set of commands for repository maintenance:

When to use each command:

- `cleanup.py status`: Use this FIRST when starting work to check the current state
  of the repository. It shows file structure, git status, and runs all code quality
  checks. Run this before making any changes to ensure you're starting from a clean state.

- `cleanup.py venv`: Run this when setting up the project for the first time or if
  your virtual environment is corrupted/missing. Creates a new virtual environment
  using uv.

- `cleanup.py install`: Use after `venv` or when dependencies have changed. Installs
  the package and all development dependencies in editable mode.

- `cleanup.py update`: Run this when you've made changes and want to commit them.
  It will:
  1. Show current status (like `status` command)
  2. Stage and commit any changes with a generic message
  Use this for routine maintenance commits.

- `cleanup.py push`: Run this after `update` when you want to push your committed
  changes to the remote repository.

Workflow Example:
1. Start work: `cleanup.py status`
2. Make changes to code
3. Commit changes: `cleanup.py update`
4. Push to remote: `cleanup.py push`

The script maintains a CLEANUP.txt file that records all operations with timestamps.
It also includes content from README.md at the start and TODO.md at the end of logs
for context.

Required Files:
- LOG.md: Project changelog
- README.md: Project documentation
- TODO.md: Pending tasks and future plans
"""

import subprocess
import os
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import NoReturn
from shutil import which

# Configuration
IGNORE_PATTERNS = [
    ".git",
    ".venv",
    "__pycache__",
    "*.pyc",
    "dist",
    "build",
    "*.egg-info",
]
REQUIRED_FILES = ["LOG.md", ".cursor/rules/0project.mdc", "TODO.md"]
LOG_FILE = Path("CLEANUP.txt")

# Ensure we're working from the script's directory
os.chdir(Path(__file__).parent)


def new() -> None:
    """Remove existing log file."""
    if LOG_FILE.exists():
        LOG_FILE.unlink()


def prefix() -> None:
    """Write README.md content to log file."""
    readme = Path(".cursor/rules/0project.mdc")
    if readme.exists():
        log_message("\n=== PROJECT STATEMENT ===")
        content = readme.read_text()
        log_message(content)


def suffix() -> None:
    """Write TODO.md content to log file."""
    todo = Path("TODO.md")
    if todo.exists():
        log_message("\n=== TODO.md ===")
        content = todo.read_text()
        log_message(content)


def log_message(message: str) -> None:
    """Log a message to file and console with timestamp."""
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
    log_line = f"{timestamp} - {message}\n"
    with LOG_FILE.open("a") as f:
        f.write(log_line)


def run_command(cmd: list[str], check: bool = True) -> subprocess.CompletedProcess:
    """Run a shell command and return the result."""
    try:
        result = subprocess.run(
            cmd,
            check=check,
            capture_output=True,
            text=True,
            shell=False,  # Explicitly set shell=False for security
        )
        if result.stdout:
            log_message(result.stdout)
        return result
    except subprocess.CalledProcessError as e:
        log_message(f"Command failed: {' '.join(cmd)}")
        log_message(f"Error: {e.stderr}")
        if check:
            raise
        return subprocess.CompletedProcess(cmd, 1, "", str(e))


def check_command_exists(cmd: str) -> bool:
    """Check if a command exists in the system."""
    try:
        return which(cmd) is not None
    except Exception:
        return False


class Cleanup:
    """Main cleanup tool class."""

    def __init__(self) -> None:
        self.workspace = Path.cwd()

    def _print_header(self, message: str) -> None:
        """Print a section header."""
        log_message(f"\n=== {message} ===")

    def _check_required_files(self) -> bool:
        """Check if all required files exist."""
        missing = False
        for file in REQUIRED_FILES:
            if not (self.workspace / file).exists():
                log_message(f"Error: {file} is missing")
                missing = True
        return not missing

    def _generate_tree(self) -> None:
        """Generate and display tree structure of the project."""
        if not check_command_exists("tree"):
            log_message("Warning: 'tree' command not found. Skipping tree generation.")
            return None

        try:
            # Create/overwrite the file with YAML frontmatter
            rules_dir = Path(".cursor/rules")
            rules_dir.mkdir(parents=True, exist_ok=True)
            # Get tree output
            tree_result = run_command(
                ["tree", "-a", "-I", ".git", "--gitignore", "-n", "-h", "-I", "*_cache"]
            )
            tree_text = tree_result.stdout
            # Write frontmatter and tree output to file
            with open(rules_dir / "filetree.mdc", "w") as f:
                f.write("---\ndescription: File tree of the project\nglobs: \n---\n")
                f.write(tree_text)

            # Log the contents
            log_message("\nProject structure:")
            log_message(tree_text)

        except Exception as e:
            log_message(f"Failed to generate tree: {e}")
        return None

    def _git_status(self) -> bool:
        """Check git status and return True if there are changes."""
        result = run_command(["git", "status", "--porcelain"], check=False)
        return bool(result.stdout.strip())

    def _venv(self) -> None:
        """Create and activate virtual environment using uv."""
        log_message("Setting up virtual environment")
        try:
            run_command(["uv", "venv"])
            # Activate the virtual environment
            venv_path = self.workspace / ".venv" / "bin" / "activate"
            if venv_path.exists():
                os.environ["VIRTUAL_ENV"] = str(self.workspace / ".venv")
                os.environ["PATH"] = (
                    f"{self.workspace / '.venv' / 'bin'}{os.pathsep}{os.environ['PATH']}"
                )
                log_message("Virtual environment created and activated")
            else:
                log_message("Virtual environment created but activation failed")
        except Exception as e:
            log_message(f"Failed to create virtual environment: {e}")

    def _install(self) -> None:
        """Install package in development mode with all extras."""
        log_message("Installing package with all extras")
        try:
            self._venv()
            run_command(["uv", "pip", "install", "-e", ".[test,dev]"])
            log_message("Package installed successfully")
        except Exception as e:
            log_message(f"Failed to install package: {e}")

    def _run_checks(self) -> None:
        """Run code quality checks using ruff and pytest."""
        log_message("Running code quality checks")

        try:
            # Run ruff checks
            log_message(">>> Running code fixes...")
            run_command(
                [
                    "python",
                    "-m",
                    "ruff",
                    "check",
                    "--fix",
                    "--unsafe-fixes",
                    "src",
                    "tests",
                ],
                check=False,
            )
            run_command(
                [
                    "python",
                    "-m",
                    "ruff",
                    "format",
                    "--respect-gitignore",
                    "src",
                    "tests",
                ],
                check=False,
            )

            # Run type checks
            log_message(">>>Running type checks...")
            run_command(["python", "-m", "mypy", "src", "tests"], check=False)

            # Run tests
            log_message(">>> Running tests...")
            run_command(["python", "-m", "pytest", "tests"], check=False)

            log_message("All checks completed")
        except Exception as e:
            log_message(f"Failed during checks: {e}")

    def status(self) -> None:
        """Show current repository status: tree structure, git status, and run checks."""
        prefix()  # Add README.md content at start
        self._print_header("Current Status")

        # Check required files
        self._check_required_files()

        # Show tree structure
        self._generate_tree()

        # Show git status
        result = run_command(["git", "status"], check=False)
        log_message(result.stdout)

        # Run additional checks
        self._print_header("Environment Status")
        self._venv()
        self._install()
        self._run_checks()

        suffix()  # Add TODO.md content at end

    def venv(self) -> None:
        """Create and activate virtual environment."""
        self._print_header("Virtual Environment Setup")
        self._venv()

    def install(self) -> None:
        """Install package with all extras."""
        self._print_header("Package Installation")
        self._install()

    def update(self) -> None:
        """Show status and commit any changes if needed."""
        # First show current status
        self.status()

        # Then handle git changes if any
        if self._git_status():
            log_message("Changes detected in repository")
            try:
                # Add all changes
                run_command(["git", "add", "."])
                # Commit changes
                commit_msg = "Update repository files"
                run_command(["git", "commit", "-m", commit_msg])
                log_message("Changes committed successfully")
            except Exception as e:
                log_message(f"Failed to commit changes: {e}")
        else:
            log_message("No changes to commit")

    def push(self) -> None:
        """Push changes to remote repository."""
        self._print_header("Pushing Changes")
        try:
            run_command(["git", "push"])
            log_message("Changes pushed successfully")
        except Exception as e:
            log_message(f"Failed to push changes: {e}")


def repomix(
    *,
    compress: bool = True,
    remove_empty_lines: bool = True,
    ignore_patterns: str = ".specstory/**/*.md,.venv/**,_private/**,CLEANUP.txt,**/*.json,*.lock",
    output_file: str = "REPO_CONTENT.txt",
) -> None:
    """Combine repository files into a single text file.

    Args:
        compress: Whether to compress whitespace in output
        remove_empty_lines: Whether to remove empty lines
        ignore_patterns: Comma-separated glob patterns of files to ignore
        output_file: Output file path
    """
    try:
        # Build command
        cmd = ["repomix"]
        if compress:
            cmd.append("--compress")
        if remove_empty_lines:
            cmd.append("--remove-empty-lines")
        if ignore_patterns:
            cmd.append("-i")
            cmd.append(ignore_patterns)
        cmd.extend(["-o", output_file])

        # Run repomix
        run_command(cmd)
        log_message(f"Repository content mixed into {output_file}")

    except Exception as e:
        log_message(f"Failed to mix repository: {e}")


def print_usage() -> None:
    """Print usage information."""
    log_message("Usage:")
    log_message("  cleanup.py status   # Show current status and run all checks")
    log_message("  cleanup.py venv     # Create virtual environment")
    log_message("  cleanup.py install  # Install package with all extras")
    log_message("  cleanup.py update   # Update and commit changes")
    log_message("  cleanup.py push     # Push changes to remote")


def main() -> NoReturn:
    """Main entry point."""
    new()  # Clear log file

    if len(sys.argv) < 2:
        print_usage()
        sys.exit(1)

    command = sys.argv[1]
    cleanup = Cleanup()

    try:
        if command == "status":
            cleanup.status()
        elif command == "venv":
            cleanup.venv()
        elif command == "install":
            cleanup.install()
        elif command == "update":
            cleanup.update()
        elif command == "push":
            cleanup.push()
        else:
            print_usage()
    except Exception as e:
        log_message(f"Error: {e}")
    repomix()
    sys.stdout.write(Path("CLEANUP.txt").read_text())
    sys.exit(0)  # Ensure we exit with a status code


if __name__ == "__main__":
    main()
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="LOG.md">
---
this_file: LOG.md
---

# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [v0.0.1] - 2025-02-15

### Added

- Initial release of the project
- Created `mallmo.py` with LLM interaction functionality:
  - Core `ask()` function for LLM prompting with media support
  - `ask_batch()` for parallel processing of multiple prompts
  - `ask_chain()` for chaining multiple prompts or functions
  - Support for multiple fallback models
  - Media file processing (images and video frames)
  - CLI interface
- Added `llm_plugins.py` for checking installed LLM plugins
- Created `funchain.py` as a simple example of chain functionality
- Basic project structure with Python package setup

### Changed

- Moved `twat_llm.py` to `src/twat_llm/` directory
- Enhanced code quality with type hints and modern Python features
- Improved error handling and logging

### Fixed

- Added missing newline at end of files
- Updated `.gitignore` to exclude `_private` directory

## [Unreleased]

### To Do

- Implement core data processing logic in `twat_llm.py`
- Add comprehensive test coverage
- Enhance documentation with usage examples
- Consider adding more LLM providers and models

[unreleased]: https://github.com/twardoch/twat-llm/compare/v0.0.1...HEAD
[v0.0.1]: https://github.com/twardoch/twat-llm/releases/tag/v0.0.1
</file>

<file path="pyproject.toml">
# this_file: pyproject.toml
#==============================================================================
# TWAT_LLM PACKAGE CONFIGURATION
# This pyproject.toml defines the package metadata, dependencies, build system,
# and development environment for the twat_llm package.
#==============================================================================
#------------------------------------------------------------------------------
# BUILD SYSTEM CONFIGURATION
# Defines the tools required to build the package and the build backend.
#------------------------------------------------------------------------------
[build-system]
requires = [
    "hatchling>=1.27.0", # Core build backend for Hatch
    "hatch-vcs>=0.4.0", # Version Control System plugin for Hatch

]
build-backend = "hatchling.build" # Use Hatchling as the build backend

#------------------------------------------------------------------------------
# HATCH BUILD CONFIGURATION
# Configures the build process, specifying which packages to include and
# how to handle versioning.
#------------------------------------------------------------------------------
[tool.hatch.build.targets.wheel]
packages = ["src/twat_llm"]

#------------------------------------------------------------------------------
# PROJECT METADATA
# Core package information used by PyPI and package managers.
#------------------------------------------------------------------------------
[project]
name = "twat-llm"
dynamic = ["version"] # Version is determined dynamically from VCS
description = "LLM integration for twat"
readme = "README.md"
requires-python = ">=3.10" # Minimum Python version required
license = "MIT"
keywords = ["llm", "ai", "nlp", "text-processing"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Programming Language :: Python",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: Implementation :: CPython",
    "Programming Language :: Python :: Implementation :: PyPy",
    "Operating System :: OS Independent",
    "License :: OSI Approved :: MIT License",
    "Intended Audience :: Developers",
]
# Runtime Dependencies
dependencies = [
    "twat>=1.8.1", # Main twat package
    "pydantic>=2.0.0", # For data validation and settings management
    "pydantic-settings>=2.0.0", # For settings management (e.g., .env files)
    "rich>=13.0.0", # For rich text and beautiful formatting in the terminal
    "httpx>=0.25.0", # HTTP client for making API requests
    "llm>=0.8", # LLM library by Simon Willison
    "Pillow>=9.0.0", # For image processing (PIL)
    "tenacity>=8.0.0", # For retrying operations
    "opencv-python-headless>=4.0.0" # For video/image processing with cv2
]

# Project Authors
[[project.authors]]
name = "Adam Twardoch"
email = "adam+github@twardoch.com"

# Project URLs
[project.urls]
Documentation = "https://github.com/twardoch/twat-llm#readme"
Issues = "https://github.com/twardoch/twat-llm/issues"
Source = "https://github.com/twardoch/twat-llm"

#------------------------------------------------------------------------------
# OPTIONAL DEPENDENCIES
# Additional dependencies for optional features, development, and testing.
#------------------------------------------------------------------------------
[project.optional-dependencies]
# Testing tools and frameworks
test = [
    "pytest>=8.3.4",
    "pytest-cov>=6.0.0",
    "pytest-xdist>=3.6.1", # For parallel test execution
    "pytest-benchmark[histogram]>=5.1.0", # For performance testing
]
# Development tools
dev = [
    "pre-commit>=4.1.0", # Git pre-commit hooks
    "ruff>=0.9.6", # Fast Python linter
    "mypy>=1.15.0", # Static type checker
    "types-Pillow", # Type stubs for Pillow
    "types-tenacity", # Type stubs for tenacity
    # Consider adding cv2-stubs if specific typing issues persist with opencv
    # Consider adding types-fire if CLI typing issues persist
]
# All optional dependencies combined
all = [
    "twat>=1.8.1", # Main twat package
    "llm>=0.8",
    "Pillow>=9.0.0",
    "tenacity>=8.0.0",
    "opencv-python-headless>=4.0.0",
]

#------------------------------------------------------------------------------
# HATCH CONFIGURATION
# Configuration for the Hatch build system and environments.
#------------------------------------------------------------------------------
[tool.hatch.version]
source = "vcs"

[tool.hatch.version.raw-options]
version_scheme = "post-release"

[tool.hatch.build.hooks.vcs]
version-file = "src/twat_llm/__version__.py"

[tool.hatch.envs.default]
dependencies = [
    "pytest", # Testing framework
    "pytest-cov", # Coverage reporting
    "mypy>=1.15.0", # Static type checker
    "ruff>=0.9.6", # Fast Python linter

]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/twat_llm --cov=tests {args:tests}"
type-check = "mypy src/twat_llm tests"
lint = ["ruff check src/twat_llm tests", "ruff format src/twat_llm tests"]

[[tool.hatch.envs.all.matrix]]
python = ["3.10", "3.11", "3.12"]

[tool.hatch.envs.lint]
detached = true # Run in isolated environment
dependencies = [
    "mypy>=1.15.0", # Static type checker
    "ruff>=0.9.6", # Fast Python linter

]

[tool.hatch.envs.lint.scripts]
typing = "mypy --install-types --non-interactive {args:src/twat_llm tests}"
style = ["ruff check {args:.}", "ruff format {args:.}"]
fmt = ["ruff format {args:.}", "ruff check --fix {args:.}"]
fix = ["ruff check --fix --unsafe-fixes {args:.}", "ruff format {args:.}"]
all = ["style", "typing"]

#------------------------------------------------------------------------------
# CODE QUALITY TOOLS
# Configuration for linting, formatting, and code quality enforcement.
#------------------------------------------------------------------------------
[tool.ruff]
target-version = "py310"
line-length = 88

[tool.ruff.lint]
extend-select = [
    "A", # flake8-builtins
    "ARG", # flake8-unused-arguments
    "B", # flake8-bugbear
    "C", # flake8-comprehensions
    "DTZ", # flake8-datetimez
    "E", # pycodestyle errors
    "EM", # flake8-errmsg
    "F", # pyflakes
    "FBT", # flake8-boolean-trap
    "I", # isort
    "ICN", # flake8-import-conventions
    "ISC", # flake8-implicit-str-concat
    "N", # pep8-naming
    "PLC", # pylint convention
    "PLE", # pylint error
    "PLR", # pylint refactor
    "PLW", # pylint warning
    "Q", # flake8-quotes
    "RUF", # Ruff-specific rules
    "S", # flake8-bandit
    "T", # flake8-debugger
    "TID", # flake8-tidy-imports
    "UP", # pyupgrade
    "W", # pycodestyle warnings
    "YTT", # flake8-2020

]
ignore = [
    "ARG001", # Unused function argument
    "E501", # Line too long
    "I001", # Import block formatting

]

[tool.ruff.lint.isort]
known-first-party = ["twat_llm"] # Treat as first-party imports for sorting

[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = "all" # Ban all relative imports for consistency

[tool.ruff.lint.per-file-ignores]
# Tests can use magic values, assertions, and relative imports
"tests/**/*" = [
    "PLR2004", # Allow magic values in tests for readability
    "S101", # Allow assertions in tests
    "TID252"
    # Allow relative imports in tests for convenience
]

#------------------------------------------------------------------------------
# MYPY CONFIGURATION
# Configuration for type checking with mypy.
#------------------------------------------------------------------------------
[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

#------------------------------------------------------------------------------
# COVERAGE CONFIGURATION
# Settings for test coverage measurement and reporting.
#------------------------------------------------------------------------------
[tool.coverage.run]
source_pkgs = ["twat_llm", "tests"]
branch = true
parallel = true
omit = ["src/twat_llm/__about__.py"]

[tool.coverage.paths]
twat_llm = ["src/twat_llm", "*/twat-llm/src/twat_llm"]
tests = ["tests", "*/twat-llm/tests"]

[tool.coverage.report]
exclude_lines = ["no cov", "if __name__ == .__main__.:", "if TYPE_CHECKING:"]

#------------------------------------------------------------------------------
# TEST ENVIRONMENTS AND CONFIGURATION
# Settings for test execution and benchmarking.
#------------------------------------------------------------------------------
[tool.hatch.envs.test]
dependencies = [
  "twat-llm[test]",
]

[tool.hatch.envs.test.scripts]
test = "python -m pytest -n auto {args:tests}"
test-cov = "python -m pytest -n auto --cov-report=term-missing --cov-config=pyproject.toml --cov=src/twat_llm --cov=tests {args:tests}"
bench = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only"
bench-save = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json"

[tool.pytest.ini_options]
markers = ["benchmark: marks tests as benchmarks (select with '-m benchmark')"]
addopts = "-v -p no:briefcase"
testpaths = ["tests"]
python_files = ["test_*.py"]
filterwarnings = ["ignore::DeprecationWarning", "ignore::UserWarning"]
asyncio_mode = "auto"

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = "file"
save-data = true
compare = [
    "min", # Minimum time
    "max", # Maximum time
    "mean", # Mean time
    "stddev", # Standard deviation
    "median", # Median time
    "iqr", # Inter-quartile range
    "ops", # Operations per second
    "rounds", # Number of rounds

]
</file>

<file path="README.md">
# twat-llm

**twat-llm** is a Python library designed to simplify interactions with various Large Language Models (LLMs) and external data services. It provides a unified interface for tasks like data enrichment (e.g., fetching professional profiles) and web searching, augmented by LLM-powered summarization and processing. The library also includes robust utilities for handling media, chaining prompts, and batch processing LLM requests.

## Features

-   **Modern Python Development**: Built with Python 3.10+, using modern tooling like Hatch for project management and `pyproject.toml` for PEP 621 compliance.
-   **LLM Abstraction**: Easily interact with different LLMs through a consistent API (powered by the `llm` library).
-   **Data Enrichment**:
    -   Fetch and summarize professional profiles using services like Proxycurl.
    -   Perform web searches using services like Brave Search API and summarize results.
-   **Media Handling**: Utilities for processing images and extracting frames from videos for multimodal LLM inputs.
-   **Flexible Prompting**: Supports direct prompting, chained prompts for complex workflows, and batch processing for efficiency.
-   **Configurable Actions**: Use Pydantic models for clear and validated action configurations.
-   **Extensive Quality Assurance**: Includes a comprehensive test suite (pytest), linting (Ruff), and static type checking (MyPy).
-   **CI/CD Ready**: GitHub Actions for automated testing, building, and releasing.
-   **Semantic Versioning**: Versioning managed by `hatch-vcs` based on Git tags.

## Rationale

The primary goal of `twat-llm` is to provide a streamlined and robust toolkit for developers looking to integrate LLM capabilities with external data sources. Many applications require fetching data from APIs (like social media enrichment or web search) and then using LLMs to process, summarize, or derive insights from that data. `twat-llm` aims to:

-   Reduce boilerplate code for common LLM interaction patterns.
-   Offer a flexible way to define and execute data processing actions involving LLMs.
-   Provide helpful utilities for tasks often associated with LLM applications, such as media handling.
-   Encourage best practices in terms of code quality, testing, and dependency management.

## Installation

You can install `twat-llm` using pip:

```bash
pip install twat-llm
```

This will install the core package and its runtime dependencies.

### Extras

The package defines optional dependencies for development and testing:

-   `dev`: Includes tools like `ruff`, `mypy`, and `pre-commit` for development.
-   `test`: Includes `pytest`, `pytest-cov`, etc., for running tests.

To install with these extras:
```bash
pip install "twat-llm[dev,test]"
```
Or for all extras:
```bash
pip install "twat-llm[all]"
```

### API Keys

Some functionalities, like person enrichment (Proxycurl) and web search (e.g., Brave Search), require API keys. These are managed via environment variables. You can set them directly in your environment or place them in a `.env` file in your project root:

```env
# .env file example
PROXYCURL_API_KEY="your_proxycurl_api_key"
SEARCH_API_KEY="your_search_api_key"
# Add other API keys as needed by the llm library (e.g., OPENAI_API_KEY)
```

The library uses `pydantic-settings` to load these variables.

## Usage

The library offers two main ways to interact with LLMs and data services:
1.  High-level `process_data` function for predefined actions.
2.  Granular functions from the `mallmo` module for direct LLM calls, chaining, and batching.

### Using `process_data`

The `process_data` function uses an `ActionConfig` model to define what operation to perform.

```python
from twat_llm import process_data, ActionConfig, PersonEnrichmentParams, WebSearchParams

# Example 1: Enrich Person profile (requires PROXYCURL_API_KEY)
enrich_config = ActionConfig(
    action_type="enrich_person",
    parameters=PersonEnrichmentParams(
        linkedin_profile_url="https://www.linkedin.com/in/exampleprofile",
        name="John Doe", # Optional, can be used if URL is unknown (future enhancement)
        email="john.doe@example.com" # Optional
    )
)
try:
    enriched_data = process_data(enrich_config, debug=True)
    print("Enriched Person Data:", enriched_data)
except ValueError as e:
    print(f"Error enriching person: {e}")

# Example 2: Search Web (requires SEARCH_API_KEY for Brave Search, or configure other provider)
search_config = ActionConfig(
    action_type="search_web",
    parameters=WebSearchParams(query="latest advancements in AI")
)
try:
    search_summary = process_data(search_config, debug=True)
    print("Web Search Summary:", search_summary)
except ValueError as e:
    print(f"Error searching web: {e}")

```

### Using `mallmo` module directly

The `twat_llm.mallmo` module provides more direct control over LLM interactions.

```python
from pathlib import Path
from twat_llm import mallmo

# Simple prompt to an LLM (uses default models if not specified)
try:
    response = mallmo.ask("What is the capital of France?")
    print(f"LLM Response: {response}")
except mallmo.LLMError as e:
    print(f"Error with LLM: {e}")

# Prompt with input data
data_to_summarize = "Python is a versatile and widely-used programming language..."
summary_prompt = "Summarize the following text in one sentence: $input"
try:
    summary = mallmo.ask(prompt=summary_prompt, data=data_to_summarize)
    print(f"Summary: {summary}")
except mallmo.LLMError as e:
    print(f"Error summarizing: {e}")

# Prompt with media (ensure you have an image file 'example.jpg')
# Create a dummy image for the example if you don't have one
try:
    from PIL import Image
    dummy_image = Image.new('RGB', (60, 30), color = 'red')
    dummy_image.save("dummy_image.jpg")

    media_response = mallmo.ask(
        prompt="What is in this image?",
        media_paths=[Path("dummy_image.jpg")]
    )
    print(f"Media Response: {media_response}")
except mallmo.LLMError as e:
    print(f"Error with media prompt: {e}")
except FileNotFoundError:
    print("Dummy image not found. Please create 'dummy_image.jpg' to run this example.")


# Chaining prompts and functions
def uppercase_text(text: str) -> str:
    return text.upper()

try:
    chain_steps = [
        "What is the capital of France?",
        uppercase_text,
        "Translate the following to Spanish: $input"
    ]
    final_result = mallmo.ask_chain("Initial data (not used by first prompt)", chain_steps)
    print(f"Chained result: {final_result}")
except mallmo.LLMError as e:
    print(f"Error in chain: {e}")


# Batch processing prompts (media not supported in batch via this simple function)
prompts_list = [
    "What is 2+2?",
    "What is the color of the sky?"
]
try:
    batch_responses = mallmo.ask_batch(prompts_list)
    for i, res in enumerate(batch_responses):
        print(f"Batch Response {i+1}: {res}")
except mallmo.BatchProcessingError as e:
    print(f"Error in batch processing: {e}")

```

## Development

This project uses [Hatch](https://hatch.pypa.io/) for environment and project management, and [uv](https://github.com/astral-sh/uv) for faster package installation and resolution within Hatch.

### Setup Development Environment

1.  **Install Hatch and uv**:
    If you don't have them, install Hatch and uv. Pipx is recommended for CLI tools:
    ```bash
    pipx install hatch
    pipx install uv
    ```
    Alternatively, use pip:
    ```bash
    pip install --user hatch uv
    ```

2.  **Activate Hatch Environment**:
    Navigate to the project root directory and run:
    ```bash
    hatch shell
    ```
    This will create a virtual environment (using `uv` if available) and install all dependencies, including development tools.

### Running Quality Checks and Tests

The following commands can be run from within the activated Hatch environment (after `hatch shell`) or by prefixing with `hatch run <env>:<script_name>` (e.g., `hatch run default:test`).

-   **Run tests**:
    ```bash
    pytest
    # or
    hatch run test
    ```

-   **Run tests with coverage**:
    ```bash
    hatch run test:test-cov
    ```

-   **Run linters and formatters (Ruff)**:
    ```bash
    hatch run lint:style  # Runs ruff check and ruff format
    # For auto-fixing:
    hatch run lint:fmt
    ```

-   **Run type checking (MyPy)**:
    ```bash
    hatch run lint:typing
    ```

-   **Run all linters and type checker**:
    ```bash
    hatch run lint:all
    ```

-   **`cleanup.py` Script**:
    A utility script `cleanup.py` is provided for various maintenance tasks. It's configured to run with `uv`.
    ```bash
    ./cleanup.py status  # Check current status and run checks
    ./cleanup.py update  # Run checks and commit changes
    ```
    Make sure it's executable (`chmod +x cleanup.py`).

### Pre-commit Hooks

This project uses pre-commit hooks to automatically check and format code before commits. To install:
```bash
pip install pre-commit
pre-commit install
```
This will run Ruff and MyPy on staged files during `git commit`.

## Codebase Structure

-   **`src/twat_llm/`**: Main source code directory.
    -   `__init__.py`: Makes the directory a package and exports `__version__`.
    -   `twat_llm.py`: Contains the high-level `process_data` function and Pydantic models for action configurations (`ActionConfig`, `PersonEnrichmentParams`, `WebSearchParams`, `ApiKeySettings`). This is the primary entry point for structured LLM-driven data processing tasks.
    -   `mallmo.py`: Provides core LLM interaction utilities like `ask` (for single prompts with media support), `ask_chain` (for sequential processing steps), and `ask_batch` (for parallel prompt execution). It also includes helper functions for media processing.
    -   `__version__.py`: Automatically generated by `hatch-vcs` during the build process to store the package version.
-   **`tests/`**: Contains unit and integration tests.
    -   `test_twat_llm.py`: Tests for `twat_llm.py`.
    -   `test_mallmo.py`: Tests for `mallmo.py`.
-   **`examples/`**: Contains example scripts demonstrating usage of the library.
-   **`docs/`**: Documentation files.
    -   `research/`: Contains research notes and API explorations (may be integrated into more formal docs later).
-   **`pyproject.toml`**: Defines project metadata, dependencies, and tool configurations (Hatch, Ruff, MyPy, Pytest).
-   **`.pre-commit-config.yaml`**: Configuration for pre-commit hooks.
-   **`LICENSE`**: MIT License file.
-   **`README.md`**: This file.
-   **`cleanup.py`**: Utility script for development tasks.

## Contribution Guidelines

Contributions are welcome! Please follow these guidelines:

1.  **Fork and Clone**: Fork the repository and clone it locally.
2.  **Create a Branch**: Create a new branch for your feature or bug fix: `git checkout -b feature/your-feature-name` or `fix/your-bug-fix`.
3.  **Develop**: Make your changes. Ensure your code adheres to the existing style (Ruff will help enforce this). Add type hints for new functions and classes.
4.  **Test**:
    *   Write new tests for any new functionality.
    *   Ensure all tests pass by running `hatch run test:test-cov`. Aim for high test coverage.
5.  **Lint and Format**:
    *   Run `hatch run lint:all` to check for linting issues and type errors. Fix any reported problems.
6.  **Commit**: Use clear and descriptive commit messages. Consider using [Conventional Commits](https://www.conventionalcommits.org/) if you are familiar with it.
7.  **Push**: Push your changes to your fork: `git push origin feature/your-feature-name`.
8.  **Create a Pull Request**: Open a pull request against the `main` branch of the original repository. Provide a clear description of your changes.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
</file>

</files>
</file>

<file path="LOG.md">
---
this_file: LOG.md
---

# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [v0.0.1] - 2025-02-15

### Added

- Initial release of the project
- Created `mallmo.py` with LLM interaction functionality:
  - Core `ask()` function for LLM prompting with media support
  - `ask_batch()` for parallel processing of multiple prompts
  - `ask_chain()` for chaining multiple prompts or functions
  - Support for multiple fallback models
  - Media file processing (images and video frames)
  - CLI interface
- Added `llm_plugins.py` for checking installed LLM plugins
- Created `funchain.py` as a simple example of chain functionality
- Basic project structure with Python package setup

### Changed

- Moved `twat_llm.py` to `src/twat_llm/` directory
- Enhanced code quality with type hints and modern Python features
- Improved error handling and logging

### Fixed

- Added missing newline at end of files
- Updated `.gitignore` to exclude `_private` directory

## [Unreleased]

### To Do

- Implement core data processing logic in `twat_llm.py`
- Add comprehensive test coverage
- Enhance documentation with usage examples
- Consider adding more LLM providers and models

[unreleased]: https://github.com/twardoch/twat-llm/compare/v0.0.1...HEAD
[v0.0.1]: https://github.com/twardoch/twat-llm/releases/tag/v0.0.1
</file>

<file path="README.md">
# twat-llm

**twat-llm** is a Python library designed to simplify interactions with various Large Language Models (LLMs) and external data services. It provides a unified interface for tasks like data enrichment (e.g., fetching professional profiles) and web searching, augmented by LLM-powered summarization and processing. The library also includes robust utilities for handling media, chaining prompts, and batch processing LLM requests.

## Features

-   **Modern Python Development**: Built with Python 3.10+, using modern tooling like Hatch for project management and `pyproject.toml` for PEP 621 compliance.
-   **LLM Abstraction**: Easily interact with different LLMs through a consistent API (powered by the `llm` library).
-   **Data Enrichment**:
    -   Fetch and summarize professional profiles using services like Proxycurl.
    -   Perform web searches using services like Brave Search API and summarize results.
-   **Media Handling**: Utilities for processing images and extracting frames from videos for multimodal LLM inputs.
-   **Flexible Prompting**: Supports direct prompting, chained prompts for complex workflows, and batch processing for efficiency.
-   **Configurable Actions**: Use Pydantic models for clear and validated action configurations.
-   **Extensive Quality Assurance**: Includes a comprehensive test suite (pytest), linting (Ruff), and static type checking (MyPy).
-   **CI/CD Ready**: GitHub Actions for automated testing, building, and releasing.
-   **Semantic Versioning**: Versioning managed by `hatch-vcs` based on Git tags.

## Rationale

The primary goal of `twat-llm` is to provide a streamlined and robust toolkit for developers looking to integrate LLM capabilities with external data sources. Many applications require fetching data from APIs (like social media enrichment or web search) and then using LLMs to process, summarize, or derive insights from that data. `twat-llm` aims to:

-   Reduce boilerplate code for common LLM interaction patterns.
-   Offer a flexible way to define and execute data processing actions involving LLMs.
-   Provide helpful utilities for tasks often associated with LLM applications, such as media handling.
-   Encourage best practices in terms of code quality, testing, and dependency management.

## Installation

You can install `twat-llm` using pip:

```bash
pip install twat-llm
```

This will install the core package and its runtime dependencies.

### Extras

The package defines optional dependencies for development and testing:

-   `dev`: Includes tools like `ruff`, `mypy`, and `pre-commit` for development.
-   `test`: Includes `pytest`, `pytest-cov`, etc., for running tests.

To install with these extras:
```bash
pip install "twat-llm[dev,test]"
```
Or for all extras:
```bash
pip install "twat-llm[all]"
```

### API Keys

Some functionalities, like person enrichment (Proxycurl) and web search (e.g., Brave Search), require API keys. These are managed via environment variables. You can set them directly in your environment or place them in a `.env` file in your project root:

```env
# .env file example
PROXYCURL_API_KEY="your_proxycurl_api_key"
SEARCH_API_KEY="your_search_api_key"
# Add other API keys as needed by the llm library (e.g., OPENAI_API_KEY)
```

The library uses `pydantic-settings` to load these variables.

## Usage

The library offers two main ways to interact with LLMs and data services:
1.  High-level `process_data` function for predefined actions.
2.  Granular functions from the `mallmo` module for direct LLM calls, chaining, and batching.

### Using `process_data`

The `process_data` function uses an `ActionConfig` model to define what operation to perform.

```python
from twat_llm import process_data, ActionConfig, PersonEnrichmentParams, WebSearchParams

# Example 1: Enrich Person profile (requires PROXYCURL_API_KEY)
enrich_config = ActionConfig(
    action_type="enrich_person",
    parameters=PersonEnrichmentParams(
        linkedin_profile_url="https://www.linkedin.com/in/exampleprofile",
        name="John Doe", # Optional, can be used if URL is unknown (future enhancement)
        email="john.doe@example.com" # Optional
    )
)
try:
    enriched_data = process_data(enrich_config, debug=True)
    print("Enriched Person Data:", enriched_data)
except ValueError as e:
    print(f"Error enriching person: {e}")

# Example 2: Search Web (requires SEARCH_API_KEY for Brave Search, or configure other provider)
search_config = ActionConfig(
    action_type="search_web",
    parameters=WebSearchParams(query="latest advancements in AI")
)
try:
    search_summary = process_data(search_config, debug=True)
    print("Web Search Summary:", search_summary)
except ValueError as e:
    print(f"Error searching web: {e}")

```

### Using `mallmo` module directly

The `twat_llm.mallmo` module provides more direct control over LLM interactions.

```python
from pathlib import Path
from twat_llm import mallmo

# Simple prompt to an LLM (uses default models if not specified)
try:
    response = mallmo.ask("What is the capital of France?")
    print(f"LLM Response: {response}")
except mallmo.LLMError as e:
    print(f"Error with LLM: {e}")

# Prompt with input data
data_to_summarize = "Python is a versatile and widely-used programming language..."
summary_prompt = "Summarize the following text in one sentence: $input"
try:
    summary = mallmo.ask(prompt=summary_prompt, data=data_to_summarize)
    print(f"Summary: {summary}")
except mallmo.LLMError as e:
    print(f"Error summarizing: {e}")

# Prompt with media (ensure you have an image file 'example.jpg')
# Create a dummy image for the example if you don't have one
try:
    from PIL import Image
    dummy_image = Image.new('RGB', (60, 30), color = 'red')
    dummy_image.save("dummy_image.jpg")

    media_response = mallmo.ask(
        prompt="What is in this image?",
        media_paths=[Path("dummy_image.jpg")]
    )
    print(f"Media Response: {media_response}")
except mallmo.LLMError as e:
    print(f"Error with media prompt: {e}")
except FileNotFoundError:
    print("Dummy image not found. Please create 'dummy_image.jpg' to run this example.")


# Chaining prompts and functions
def uppercase_text(text: str) -> str:
    return text.upper()

try:
    chain_steps = [
        "What is the capital of France?",
        uppercase_text,
        "Translate the following to Spanish: $input"
    ]
    final_result = mallmo.ask_chain("Initial data (not used by first prompt)", chain_steps)
    print(f"Chained result: {final_result}")
except mallmo.LLMError as e:
    print(f"Error in chain: {e}")


# Batch processing prompts (media not supported in batch via this simple function)
prompts_list = [
    "What is 2+2?",
    "What is the color of the sky?"
]
try:
    batch_responses = mallmo.ask_batch(prompts_list)
    for i, res in enumerate(batch_responses):
        print(f"Batch Response {i+1}: {res}")
except mallmo.BatchProcessingError as e:
    print(f"Error in batch processing: {e}")

```

## Development

This project uses [Hatch](https://hatch.pypa.io/) for environment and project management, and [uv](https://github.com/astral-sh/uv) for faster package installation and resolution within Hatch.

### Setup Development Environment

1.  **Install Hatch and uv**:
    If you don't have them, install Hatch and uv. Pipx is recommended for CLI tools:
    ```bash
    pipx install hatch
    pipx install uv
    ```
    Alternatively, use pip:
    ```bash
    pip install --user hatch uv
    ```

2.  **Activate Hatch Environment**:
    Navigate to the project root directory and run:
    ```bash
    hatch shell
    ```
    This will create a virtual environment (using `uv` if available) and install all dependencies, including development tools.

### Running Quality Checks and Tests

The following commands can be run from within the activated Hatch environment (after `hatch shell`) or by prefixing with `hatch run <env>:<script_name>` (e.g., `hatch run default:test`).

-   **Run tests**:
    ```bash
    pytest
    # or
    hatch run test
    ```

-   **Run tests with coverage**:
    ```bash
    hatch run test:test-cov
    ```

-   **Run linters and formatters (Ruff)**:
    ```bash
    hatch run lint:style  # Runs ruff check and ruff format
    # For auto-fixing:
    hatch run lint:fmt
    ```

-   **Run type checking (MyPy)**:
    ```bash
    hatch run lint:typing
    ```

-   **Run all linters and type checker**:
    ```bash
    hatch run lint:all
    ```

-   **`cleanup.py` Script**:
    A utility script `cleanup.py` is provided for various maintenance tasks. It's configured to run with `uv`.
    ```bash
    ./cleanup.py status  # Check current status and run checks
    ./cleanup.py update  # Run checks and commit changes
    ```
    Make sure it's executable (`chmod +x cleanup.py`).

### Pre-commit Hooks

This project uses pre-commit hooks to automatically check and format code before commits. To install:
```bash
pip install pre-commit
pre-commit install
```
This will run Ruff and MyPy on staged files during `git commit`.

## Codebase Structure

-   **`src/twat_llm/`**: Main source code directory.
    -   `__init__.py`: Makes the directory a package and exports `__version__`.
    -   `twat_llm.py`: Contains the high-level `process_data` function and Pydantic models for action configurations (`ActionConfig`, `PersonEnrichmentParams`, `WebSearchParams`, `ApiKeySettings`). This is the primary entry point for structured LLM-driven data processing tasks.
    -   `mallmo.py`: Provides core LLM interaction utilities like `ask` (for single prompts with media support), `ask_chain` (for sequential processing steps), and `ask_batch` (for parallel prompt execution). It also includes helper functions for media processing.
    -   `__version__.py`: Automatically generated by `hatch-vcs` during the build process to store the package version.
-   **`tests/`**: Contains unit and integration tests.
    -   `test_twat_llm.py`: Tests for `twat_llm.py`.
    -   `test_mallmo.py`: Tests for `mallmo.py`.
-   **`examples/`**: Contains example scripts demonstrating usage of the library.
-   **`docs/`**: Documentation files.
    -   `research/`: Contains research notes and API explorations (may be integrated into more formal docs later).
-   **`pyproject.toml`**: Defines project metadata, dependencies, and tool configurations (Hatch, Ruff, MyPy, Pytest).
-   **`.pre-commit-config.yaml`**: Configuration for pre-commit hooks.
-   **`LICENSE`**: MIT License file.
-   **`README.md`**: This file.
-   **`cleanup.py`**: Utility script for development tasks.

## Contribution Guidelines

Contributions are welcome! Please follow these guidelines:

1.  **Fork and Clone**: Fork the repository and clone it locally.
2.  **Create a Branch**: Create a new branch for your feature or bug fix: `git checkout -b feature/your-feature-name` or `fix/your-bug-fix`.
3.  **Develop**: Make your changes. Ensure your code adheres to the existing style (Ruff will help enforce this). Add type hints for new functions and classes.
4.  **Test**:
    *   Write new tests for any new functionality.
    *   Ensure all tests pass by running `hatch run test:test-cov`. Aim for high test coverage.
5.  **Lint and Format**:
    *   Run `hatch run lint:all` to check for linting issues and type errors. Fix any reported problems.
6.  **Commit**: Use clear and descriptive commit messages. Consider using [Conventional Commits](https://www.conventionalcommits.org/) if you are familiar with it.
7.  **Push**: Push your changes to your fork: `git push origin feature/your-feature-name`.
8.  **Create a Pull Request**: Open a pull request against the `main` branch of the original repository. Provide a clear description of your changes.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
</file>

<file path="TODO.md">
# TODO List for twat-llm v1.0 MVP Streamlining

This list summarizes the actionable tasks derived from `PLAN.md`.

## Phase 1: Documentation & Non-Core File Cleanup

-   [ ] **`docs/research/` Cleanup:**
    -   [ ] Condense `people-api.md`: Retain a summary of chosen APIs (Proxycurl) and key considerations. Archive or remove detailed comparisons.
    -   [ ] Condense `web-search-api.md`: Retain a summary of chosen APIs (Brave Search) and key considerations. Archive or remove detailed comparisons.
    -   [ ] Review `people-api-tldr.md`: Integrate relevant parts into README or condensed `people-api.md`; remove if redundant.
    -   [ ] Remove internal review files: `review-copilot.md`, `review-cursor.md`, `review-o3.md`, `review-trae.md`.
-   [ ] **`.specstory/` & `.cursor/rules/`:** Confirm they are handled by `.gitignore` or are development-only aids; no direct code changes to library.
-   [ ] **Generated Files (`CLEANUP.txt`, `REPO_CONTENT.txt`):** Verify they are in `.gitignore` and `cleanup.py` manages them appropriately.

## Phase 2: Example Scripts & Utility Scripts Review

-   [ ] **`examples/` Review:**
    -   [ ] `check_llm_plugins.py`: Confirm it's useful and functional.
    -   [ ] `example_people_script.py`: Simplify, ensure clear demonstration of `process_data` with `PersonEnrichmentParams`, and guide on API key handling.
    -   [ ] `example_serp_script.py`: Simplify, ensure clear demonstration of `process_data` with `WebSearchParams`, and guide on API key handling.
    -   [ ] `funchain_example.py`: Review for clarity and relevance to `ask_chain` as an MVP feature.
-   [ ] **`cleanup.py` Review:**
    -   [ ] Verify all its functions (`status`, `venv`, `install`, `update`, `push`, `repomix`) are working and streamlined.
    -   [ ] Consider if `repomix` call needs to be conditional or a separate command. (For now, assume current behavior is acceptable).

## Phase 3: Core Code Refinement (`src/`)

-   [ ] **`src/twat_llm/twat_llm.py` Review:**
    -   [ ] `process_data` function: Confirm robustness for MVP. Defer major refactoring of common logic unless critical.
    -   [ ] Ensure summarization prompts are adequate for MVP.
    -   [ ] Verify error handling (`httpx.HTTPStatusError`, `httpx.RequestError`).
    -   [ ] `ApiKeySettings`: Confirm correct loading for all services.
    -   [ ] `ActionConfig` and related Pydantic models: Confirm `model_validator` and discriminator logic work as expected.
    -   [ ] `main()` function: Move demonstration logic to a new example script in `examples/`.
-   [ ] **`src/twat_llm/mallmo.py` Review:**
    -   [ ] Core Functions (`ask`, `_try_model`, `_prepare_media`): Ensure robustness and error handling.
    -   [ ] Media preparation (`_prepare_media`, `_resize_image`, `_extract_middle_frame`): Verify efficiency and correctness.
    -   [ ] Advanced Features (`ask_chain`, `ask_batch`): Confirm they are needed for MVP and function correctly. (Current assumption: retain).
    -   [ ] CLI in `mallmo.py`: Confirm it's functional.
-   [ ] **`src/twat_llm/__init__.py` Review:**
    -   [ ] Update `__all__` to export the public API for the library (e.g., `ActionConfig`, `process_data`, `ask`).

## Phase 4: Dependencies & Configuration Review

-   [ ] **`pyproject.toml` Review:**
    -   [ ] `opencv-python-headless`: Confirm if video processing is critical for MVP. If not, consider making it optional or deferring. (Current assumption: retain).
    -   [ ] Review other dependency versions.
-   [ ] **`.gitignore` Review:**
    -   [ ] Confirm all necessary generated files and directories are ignored.

## Phase 5: Final Review, Testing & Changelog

-   [ ] Run all quality checks: `ruff`, `mypy`.
-   [ ] Run all tests: `pytest`. Ensure full coverage for MVP features.
-   [ ] Manually review all changes.
-   [ ] Update `PLAN.md` and `TODO.md` to reflect completed work.
-   [ ] Finalize `CHANGELOG.md`.

## Phase 6: Submission

-   [ ] Commit all changes with a descriptive message.
-   [ ] Use an appropriate branch name for submission.
</file>

<file path=".cursor/rules/filetree.mdc">
---
description: File tree of the project
globs:
---
[4.0K]  .
├── [4.0K]  .cursor
│   └── [4.0K]  rules
│       ├── [ 821]  0project.mdc
│       ├── [ 516]  cleanup.mdc
│       ├── [1.7K]  filetree.mdc
│       └── [2.0K]  quality.mdc
├── [4.0K]  .github
│   └── [4.0K]  workflows
│       ├── [2.9K]  push.yml
│       └── [1.4K]  release.yml
├── [3.6K]  .gitignore
├── [1.3K]  .pre-commit-config.yaml
├── [4.0K]  .specstory
│   └── [4.0K]  history
│       ├── [2.0K]  .what-is-this.md
│       ├── [4.0K]  reviewing-the-reviewers-a-critical-analysis.md
│       └── [4.4K]  writeup-review-and-rating-analysis.md
├── [3.2K]  CHANGELOG.md
├── [1.0K]  LICENSE
├── [1.5K]  LOG.md
├── [ 12K]  README.md
├── [4.0K]  TODO.md
├── [ 13K]  cleanup.py
├── [4.0K]  docs
│   └── [4.0K]  research
│       ├── [1.6K]  people-api.md
│       └── [2.1K]  web-search-api.md
├── [4.0K]  examples
│   ├── [3.2K]  check_llm_plugins.py
│   ├── [4.9K]  example_people_script.py
│   ├── [4.2K]  example_serp_script.py
│   ├── [2.7K]  funchain_example.py
│   └── [5.8K]  run_action_config_example.py
├── [265K]  llms.txt
├── [9.7K]  pyproject.toml
├── [4.0K]  src
│   └── [4.0K]  twat_llm
│       ├── [1.2K]  __init__.py
│       ├── [ 15K]  mallmo.py
│       └── [8.7K]  twat_llm.py
└── [4.0K]  tests
    ├── [ 16K]  test_mallmo.py
    └── [ 13K]  test_twat_llm.py

13 directories, 31 files
</file>

<file path="docs/research/people-api.md">
# Person Profile API Integration (Summary)

This document outlines the approach for integrating person profile enrichment APIs within the `twat-llm` library.

## Primary API Choice: Proxycurl

For the MVP, `twat-llm` primarily utilizes **Proxycurl** for fetching detailed professional profile data, especially from LinkedIn.

**Key Features Leveraged:**
*   Resolving LinkedIn profiles from URLs.
*   Extracting information such as job history, education, skills, and contact details.

**Usage within `twat-llm`:**
*   The `enrich_person` action in `src/twat_llm/twat_llm.py` handles interaction with Proxycurl.
*   Requires `PROXYCURL_API_KEY` to be set in the environment or a `.env` file.
*   Fetched data is then summarized by an LLM via the `mallmo.py` module to provide a concise overview.

## Considerations
*   **Data Privacy & Terms of Service:** Users of `twat-llm` should be aware of Proxycurl's terms of service and data usage policies, as well as any privacy implications (e.g., GDPR, CCPA) when processing personal data.
*   **API Key Management:** Securely manage your Proxycurl API key.
*   **Error Handling:** The library includes error handling for API request failures.

## Future Enhancements (Post-MVP)
*   Support for other person data enrichment services.
*   More sophisticated lookup mechanisms (e.g., finding profiles by name/email if a LinkedIn URL isn't provided).
*   Allowing users to customize the LLM summarization prompt for profile data.

*This summary replaces a more detailed internal research document comparing various People APIs. For specific details on Proxycurl's API capabilities, refer to the official Proxycurl documentation.*
</file>

<file path="docs/research/web-search-api.md">
# Web Search API Integration (Summary)

This document outlines the approach for integrating web search APIs within the `twat-llm` library.

## Primary API Choice (Example): Brave Search API

For the MVP, `twat-llm` demonstrates web search capabilities using the **Brave Search API** as a primary example. The implementation can be adapted for other search providers.

**Key Features Leveraged:**
*   Performing web searches based on user queries.
*   Retrieving a list of search results, including titles, snippets, and URLs.

**Usage within `twat-llm`:**
*   The `search_web` action in `src/twat_llm/twat_llm.py` handles interaction with the chosen search API (e.g., Brave Search).
*   Requires a `SEARCH_API_KEY` (specific to the chosen provider, e.g., Brave's `X-Subscription-Token`) to be set in the environment or a `.env` file.
*   Fetched search results are then summarized by an LLM via the `mallmo.py` module to provide a concise answer or overview related to the query.

## Considerations
*   **API Provider Choice:** While Brave Search is used as an example, users might configure other providers. The `SEARCH_API_KEY` and specific API endpoint/parameters in `twat_llm.py` would need adjustment.
*   **Terms of Service:** Users must adhere to the terms of service of their chosen web search API provider.
*   **Rate Limits & Quotas:** Be mindful of API rate limits and query quotas.
*   **Result Quality:** The relevance and quality of search results depend on the chosen API provider.
*   **Cost:** Web search APIs typically have costs associated with their usage.

## Future Enhancements (Post-MVP)
*   Easier configuration for multiple search API providers.
*   More sophisticated parsing and ranking of search results before LLM summarization.
*   Allowing users to customize the LLM summarization prompt for search results.

*This summary replaces a more detailed internal research document comparing various Web Search APIs. For specific details on the Brave Search API, refer to the official Brave Search API documentation. If using another provider, consult their respective documentation.*
</file>

<file path="examples/example_people_script.py">
"""
Example script to demonstrate person enrichment using twat_llm.
This script shows how to use the `process_data` function with `ActionConfig`
and `PersonEnrichmentParams` to fetch and summarize a person's professional profile.
Requirements:
- Ensure the `twat-llm` package is installed.
- Set the PROXYCURL_API_KEY environment variable. You can get an API key from
  https://nubela.co/proxycurl. For example, create a .env file in your project root:
  PROXYCURL_API_KEY="your_actual_api_key"
- Ensure you have LLM models configured for the `llm` library, as `mallmo.ask`
  (used by `process_data`) will use them for summarization.
  See https://llm.datasette.io/en/stable/models/index.html for model setup.
"""
⋮----
# Configure basic logging for the example
⋮----
logger = logging.getLogger(__name__)
⋮----
# It's good practice to handle potential ImportError if the package structure changes
# or if it's run from a place where the package isn't correctly installed/visible.
⋮----
def enrich_person_example(linkedin_url: str)
⋮----
"""
    Demonstrates enriching a person's profile using their LinkedIn URL.
    Args:
        linkedin_url: The LinkedIn profile URL to enrich.
    """
⋮----
# 1. Create PersonEnrichmentParams
# For MVP, we primarily use linkedin_profile_url.
# Name and email are optional and might be used for future lookup strategies.
person_params = PersonEnrichmentParams(
⋮----
# name="John Doe", # Optional
# email="john.doe@example.com" # Optional
⋮----
# 2. Create ActionConfig
# The ApiKeySettings will be automatically populated from environment variables
# (e.g., PROXYCURL_API_KEY from .env file or environment).
enrich_action_config = ActionConfig(
# 3. Call process_data
⋮----
result = process_data(enrich_action_config, debug=True) # Enable debug for more verbose logging
⋮----
print(f"LinkedIn URL: {result.get('linkedin_url')}") # noqa: T201
print(f"\nLLM Summary:\n{result.get('summary')}") # noqa: T201
# print(f"\nRaw Profile Data (excerpt):\n{str(result.get('raw_profile_data', {}))[:500]}...") # Optional: print some raw data
⋮----
print(f"Enrichment failed. Details: {result.get('details', 'No details provided.')}") # noqa: T201
⋮----
print(f"Error: {ve}") # noqa: T201
print("Please ensure PROXYCURL_API_KEY is set correctly in your environment or .env file.") # noqa: T201
⋮----
print(f"An unexpected error occurred: {e}") # noqa: T201
⋮----
# Replace with a public LinkedIn profile URL for testing
# Note: Scraping LinkedIn profiles, even via an API like Proxycurl,
# should be done in accordance with LinkedIn's ToS and Proxycurl's terms.
# Using a well-known public figure or a test account is advisable.
example_linkedin_url = "https://www.linkedin.com/in/satyanadella/" # Example public profile
if "your_actual_api_key" in open(__file__).read(): # Basic check
print( # noqa: T201
⋮----
# You can try another example if you have another URL
# example_linkedin_url_2 = "https://linkedin.com/in/williamhgates"
# if example_linkedin_url_2:
#     print("\n")
#     enrich_person_example(example_linkedin_url_2)
print("\nPerson enrichment example finished.") # noqa: T201
print("Ensure PROXYCURL_API_KEY is set in your environment or .env file.") # noqa: T201
print("Ensure your `llm` library is configured with models for summarization.") # noqa: T201
# To run this example:
# 1. Make sure twat-llm is installed (`pip install .` or `pip install twat-llm`)
# 2. Create a .env file in the same directory as this script (or project root)
#    with your PROXYCURL_API_KEY:
#    PROXYCURL_API_KEY="your_key_here"
# 3. Run `python examples/example_people_script.py`
</file>

<file path="examples/example_serp_script.py">
"""
Example script to demonstrate web search using twat_llm.
This script shows how to use the `process_data` function with `ActionConfig`
and `WebSearchParams` to perform a web search and get an LLM-generated summary.
Requirements:
- Ensure the `twat-llm` package is installed.
- Set the SEARCH_API_KEY environment variable. The current `twat_llm.py` implementation
  is configured for the Brave Search API (https://brave.com/search/api/).
  You'll need a Brave Search API key (X-Subscription-Token).
  Create a .env file in your project root:
  SEARCH_API_KEY="your_brave_search_api_key"
- Ensure you have LLM models configured for the `llm` library, as `mallmo.ask`
  (used by `process_data`) will use them for summarization.
  See https://llm.datasette.io/en/stable/models/index.html for model setup.
"""
⋮----
# Configure basic logging for the example
⋮----
logger = logging.getLogger(__name__)
⋮----
def web_search_example(query: str)
⋮----
"""
    Demonstrates performing a web search and summarizing the results.
    Args:
        query: The search query string.
    """
⋮----
# 1. Create WebSearchParams
search_params = WebSearchParams(query=query)
# 2. Create ActionConfig
# ApiKeySettings will be automatically populated from environment variables
# (e.g., SEARCH_API_KEY for Brave Search from .env file or environment).
search_action_config = ActionConfig(
# 3. Call process_data
⋮----
result = process_data(search_action_config, debug=True) # Enable debug for more verbose logging
⋮----
print(f"Query: {result.get('query')}") # noqa: T201
print(f"\nLLM Summary/Answer:\n{result.get('summary')}") # noqa: T201
# You can also inspect raw search results if needed:
# print(f"\nRaw Search Results (excerpt):\n{str(result.get('raw_search_results', {}))[:500]}...") # Optional
⋮----
print(f"Web search failed. Details: {result.get('details', 'No details provided.')}") # noqa: T201
⋮----
print(f"Error: {ve}") # noqa: T201
print("Please ensure SEARCH_API_KEY (e.g., for Brave Search) is set correctly in your environment or .env file.") # noqa: T201
⋮----
print(f"An unexpected error occurred: {e}") # noqa: T201
⋮----
example_query = "latest advancements in large language models"
if "your_brave_search_api_key" in open(__file__).read(): # Basic check
print( # noqa: T201
⋮----
# Example with a different query
# example_query_2 = "What is the weather in London?"
# if example_query_2:
#     print("\n")
#     web_search_example(example_query_2)
print("\nWeb search example finished.") # noqa: T201
print("Ensure SEARCH_API_KEY (e.g., for Brave Search) is set in your environment or .env file.") # noqa: T201
print("Ensure your `llm` library is configured with models for summarization.") # noqa: T201
# To run this example:
# 1. Make sure twat-llm is installed (`pip install .` or `pip install twat-llm`)
# 2. Create a .env file in the same directory as this script (or project root)
#    with your SEARCH_API_KEY (e.g., Brave Search API token):
#    SEARCH_API_KEY="your_key_here"
# 3. Run `python examples/example_serp_script.py`
</file>

<file path="examples/funchain_example.py">
#!/usr/bin/env python3
"""
Example script to demonstrate the ask_chain functionality from twat_llm.mallmo.
This script shows how to chain multiple processing steps, where each step can be
an LLM prompt (string) or a Python callable.
Requirements:
- Ensure the `twat-llm` package is installed.
- Ensure you have LLM models configured for the `llm` library, as `ask_chain`
  will use them for string-based prompt steps.
  See https://llm.datasette.io/en/stable/models/index.html for model setup.
"""
⋮----
# Configure basic logging for the example
⋮----
logger = logging.getLogger(__name__)
⋮----
def to_lower_case(text_input: str) -> str
⋮----
"""Converts a string to lower case. Simple callable example."""
⋮----
def main()
⋮----
"""Runs the ask_chain example."""
initial_data = "My name is Adam Twardoch"
⋮----
# Define the processing steps
# Step 1: LLM prompt to convert to all caps
# Step 2: LLM prompt to translate the result into Polish
# Step 3: Python callable to convert the Polish translation to lower case
processing_steps = [
⋮----
"Convert the full name to all caps in: $input",  # $input will be initial_data
"Translate into Polish:",  # Input will be the result of the previous step
to_lower_case,             # Input will be the Polish translation
⋮----
final_output = ask_chain(
⋮----
print(f"Initial Data: {initial_data}") # noqa: T201
print(f"Steps: {processing_steps}") # noqa: T201
print(f"Final Output: {final_output}") # noqa: T201
⋮----
print(f"LLM Error: {e}") # noqa: T201
print("Please ensure your `llm` models are correctly configured.") # noqa: T201
⋮----
print(f"An unexpected error occurred: {e}") # noqa: T201
⋮----
print("\nFunChain example finished.") # noqa: T201
print("Ensure your `llm` library is configured with models for the prompt steps.") # noqa: T201
</file>

<file path="src/twat_llm/__init__.py">
# SPDX-FileCopyrightText: 2024 Adam Twardoch <adam+github@twardoch.com>
#
# SPDX-License-Identifier: MIT
"""
LLM integration for twat.
"""
⋮----
# Package not installed, or __version__.py not generated by hatch-vcs
# This is a fallback for development mode when running from source
# without a proper build/install that generates __version__.py.
# It's also useful if someone copies the source code without build metadata.
__version__ = "0.0.0.dev0+unknown"  # Placeholder version
# Expose main functionalities for the library user
⋮----
__all__ = [
</file>

<file path="src/twat_llm/mallmo.py">
#!/usr/bin/env python3
⋮----
# import cv2 # Removed: opencv-python-headless dependency removed for MVP
⋮----
DEFAULT_RETRY_ATTEMPTS: int = 2
MAX_STEP_TUPLE_LENGTH: int = 2
DEFAULT_FALLBACK_MODELS: list[str] = [
⋮----
"gpt-4o-mini",  # Default OpenAI model
⋮----
"claude-3-haiku-20240307",  # Updated Claude Haiku model ID
⋮----
class LLMError(Exception)
⋮----
"""Base exception class for LLM errors."""
class MediaProcessingError(LLMError)
⋮----
"""Exception for errors during media processing."""
class ModelInvocationError(LLMError)
⋮----
"""Exception for errors when invoking an LLM model."""
class BatchProcessingError(LLMError)
⋮----
"""Exception for errors during batch processing."""
def _resize_image(image: Image.Image, max_size: tuple[int, int] = (512, 512)) -> bytes
⋮----
"""Resize an image while maintaining aspect ratio."""
⋮----
# Ensure image is in RGB mode for JPEG saving
⋮----
image = image.convert("RGB")
⋮----
img_byte_arr = io.BytesIO()
⋮----
msg = f"Failed to resize image: {e!s}"
⋮----
# def _extract_middle_frame(video_path: Path) -> Image.Image:
#     """Extract the middle frame from a video file. (Removed for MVP - cv2 dependency)"""
#     # cap = cv2.VideoCapture(str(video_path))
#     # if not cap.isOpened():
#     #     raise MediaProcessingError(f"Unable to open video file: {video_path}")
#     # try:
#     #     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#     #     if total_frames == 0:
#     #         raise MediaProcessingError(f"Video file has no frames: {video_path}")
#     #     middle_frame_index = total_frames // 2
#     #     cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame_index)
#     #     ret, frame = cap.read()
#     #     if not ret or frame is None:
#     #         raise MediaProcessingError(
#     #             f"Unable to read frame at index {middle_frame_index} from video: {video_path}"
#     #         )
#     #     return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) # Requires cv2
#     # except Exception as e:
#     #     raise MediaProcessingError(f"Failed to extract frame from video {video_path}: {e!s}") from e
#     # finally:
#     #     cap.release()
#     raise NotImplementedError("Video processing was removed for MVP streamlining.")
def _prepare_media(path: Path) -> bytes
⋮----
"""Process image file and return resized image bytes. Video support removed for MVP."""
file_ext = path.suffix.lower()
supported_image_formats = {
⋮----
image = Image.open(path)
⋮----
# If video processing is to be re-added, it would go here.
# For now, explicitly state it's not supported.
msg = (
⋮----
msg = f"Unsupported media file type: {file_ext} for {path}. Only images are supported."
⋮----
msg = f"Media file not found: {path}"
⋮----
except MediaProcessingError:  # Re-raise specific media errors
⋮----
msg = f"Error processing media file {path}: {e!s}"
⋮----
),  # +1 because first attempt is not a "retry"
⋮----
"""Try a single model with retry logic."""
⋮----
model = llm.get_model(model_id)
if not model.supports_multimodal and attachments:  # Corrected attribute name
msg = f"Model {model_id} does not support multimodal inputs, but attachments were provided."
⋮----
response = model.prompt(prompt, attachments=attachments)
text_response = response.text()
if text_response is None:  # Handle cases where response.text() might be None
msg = f"Model {model_id} returned an empty response."
⋮----
msg = f"Unknown model: {model_id}. {e!s}"
⋮----
except RetryError:  # Let tenacity's RetryError propagate if all attempts fail
⋮----
msg = f"Error invoking model {model_id}: {e!s}"
⋮----
"""Process a single step in the chain and return the result."""
processor: str | Callable[..., Any]
kwargs: dict[str, Any] = {}
⋮----
processor = step
⋮----
processor = step[0]
⋮----
msg = f"Optional second element in step tuple must be a dictionary, got {type(step[1])}"
⋮----
kwargs = cast(dict[str, Any], step[1])  # Cast for type checker
⋮----
msg = "Step must be a string, function, or a 1-2 element tuple (processor, kwargs_dict)."
⋮----
result = processor(current_data, **kwargs)
⋮----
# Type checker might complain here if 'ask' is not yet fully defined or imported in a way it can resolve.
# For now, assuming 'ask' will be available in the scope this function is used.
result = ask(prompt=processor, data=current_data, **kwargs)  # type: ignore[misc]
⋮----
raise TypeError(  # Should be unreachable due to earlier checks
⋮----
"""
    Process a chain of steps where each step is either a function call or a prompt.
    Args:
        data: The initial input string to process.
        steps: Iterable of steps. Each step can be:
            - A string (prompt).
            - A function (Callable[[str, ...], Any]).
            - A tuple: (processor: str | Callable, kwargs: dict[str, Any]).
    Returns:
        The final processed output as a string.
    Raises:
        TypeError: If step format is invalid.
        LLMError: If any LLM-related error occurs during processing.
    """
current_data = data
⋮----
) in steps:  # Renamed to avoid conflict with outer 'step' if this were nested
current_data = _process_step(step_item, current_data)
⋮----
"""
    Send a prompt to LLM with optional media attachments and fallback models.
    Args:
        prompt: The text prompt to send.
        data: Optional input data to be incorporated into the prompt.
        model_ids: List of model IDs to try in order. Defaults to DEFAULT_FALLBACK_MODELS.
        media_paths: Optional list of paths to image or video files to attach.
    Returns:
        String response from the LLM.
    Raises:
        LLMError: If all models fail or media processing fails.
        MediaProcessingError: If there's an issue with a media file.
        ModelInvocationError: If a model fails to process the request.
    """
⋮----
prompt = (
models_to_try_list = (
if not models_to_try_list:  # Ensure there's at least one model to try
msg = "No model_ids provided and no default models configured."
⋮----
attachments_list: list[llm.Attachment] | None = None
⋮----
attachments_list = []
⋮----
path = Path(p_item)  # Ensure it's a Path object
⋮----
image_bytes = _prepare_media(path)
# Assuming JPEG for all prepared media for simplicity, adjust if other formats are possible
⋮----
msg = f"Unexpected error processing media file {path}: {e!s}"
⋮----
last_error: Exception | None = None
⋮----
last_error = e
# print(f"Model {model_id} failed: {e!s}", file=sys.stderr) # Optional: log attempt failure
⋮----
) as e:  # Should ideally not happen if _try_model handles its errors well
⋮----
# print(f"Unexpected error with model {model_id}: {e!s}", file=sys.stderr) # Optional: log attempt failure
⋮----
msg = f"All models failed. Last error: {last_error!s}"
⋮----
# This case should be rare if models_to_try_list was not empty initially
msg = "All models failed, but no specific error was captured (this indicates an issue with the model list or retry logic)."
⋮----
"""
    Helper function for parallel processing, takes a tuple of (prompt, model_ids).
    Media paths are not supported in this batch version for simplicity.
    """
⋮----
"""
    Process multiple prompts in parallel using concurrent.futures.ProcessPoolExecutor.
    Note: This batch version does not support media attachments per prompt.
    Args:
        prompts: List of text prompts to process.
        model_ids: Optional list of model IDs to try for each prompt.
        num_processes: Optional number of processes to use (defaults to CPU count).
    Returns:
        List of responses corresponding to the input prompts.
    Raises:
        BatchProcessingError: If parallel processing fails or any prompt processing fails.
    """
⋮----
actual_num_processes = (
if actual_num_processes is None:  # os.cpu_count() can return None
actual_num_processes = 1  # Default to 1 if cpu_count is not available
# Prepare arguments for map: list of (prompt, model_ids) tuples
args_for_map = [(prompt, model_ids) for prompt in prompts]
results: list[str]
⋮----
# executor.map processes items in order and collects results
# It will raise the first exception encountered in a worker.
results = list(executor.map(_process_single_prompt_for_batch, args_for_map))
⋮----
msg = f"Batch processing failed: {e!s}"
⋮----
"""
    CLI interface for LLM interactions.
    Args:
        prompt: The text prompt to send to the LLM.
        model: Optional specific model ID to use.
        media: Optional list of media file paths.
        batch_prompts_file: Path to a file with prompts (one per line) for batch processing.
        output_file: Path to save batch output (one response per line).
        processes: Number of processes for batch processing.
    """
model_ids_list: Sequence[str] | None = [model] if model else None
media_paths_list: Sequence[Path] | None = (
⋮----
prompts_for_batch = [line.strip() for line in f if line.strip()]
⋮----
except Exception:  # pylint: disable=broad-except
⋮----
responses = ask_batch(
⋮----
except Exception:  # pylint: disable=broad-except
⋮----
response_text = ask(
⋮----
except Exception:  # pylint: disable=broad-except
</file>

<file path="src/twat_llm/twat_llm.py">
#!/usr/bin/env python3
"""twat_llm:
Created by Adam Twardoch
"""
⋮----
import json  # Moved import json to top level
from typing import Any, Literal, Annotated  # Added Annotated
⋮----
from pydantic import BaseModel, Field, HttpUrl, model_validator  # Added model_validator
⋮----
from twat_llm.mallmo import ask  # Import from local mallmo module
# Configure logging
⋮----
logger = logging.getLogger(__name__)
# --- Pydantic Models for Configuration ---
class ApiKeySettings(BaseSettings)
⋮----
"""Manages API keys from environment variables."""
model_config = SettingsConfigDict(
proxycurl_api_key: str | None = Field(None, env="PROXYCURL_API_KEY")
search_api_key: str | None = Field(
⋮----
)  # For chosen search provider
class PersonEnrichmentParams(BaseModel)
⋮----
"""Parameters for enriching a person's profile."""
action_type: Literal["enrich_person"] = "enrich_person"
linkedin_profile_url: HttpUrl | None = None
email: str | None = None
name: str | None = None
# Add other relevant identifiers as needed, e.g., company, location
company_name: str | None = None
location: str | None = None
class WebSearchParams(BaseModel)
⋮----
"""Parameters for performing a web search."""
action_type: Literal["search_web"] = "search_web"
query: str = Field(..., description="The search query.")
AnyParams = Annotated[
class ActionConfig(BaseModel)
⋮----
"""Defines an action to be performed and its parameters."""
action_type: Literal["enrich_person", "search_web"] = Field(
parameters: AnyParams
api_keys: ApiKeySettings = Field(default_factory=ApiKeySettings)
⋮----
@model_validator(mode="before")
@classmethod
    def _add_action_type_to_params(cls, values: Any) -> Any
⋮----
action_type = values.get("action_type")
parameters = values.get("parameters")
⋮----
# Create a new dict for parameters to avoid modifying the original
# if it's shared or immutable, though Pydantic usually handles this.
updated_parameters = parameters.copy()
⋮----
"""Handles the person enrichment action."""
api_key = api_keys.proxycurl_api_key
⋮----
msg = "PROXYCURL_API_KEY is required for person enrichment but not found."
⋮----
msg = "LinkedIn profile URL is required for person enrichment in this MVP version."
⋮----
headers = {"Authorization": f"Bearer {api_key}"}
proxycurl_endpoint_url = "https://nubela.co/proxycurl/api/linkedin/person-profile"
api_params = {"url": str(params.linkedin_profile_url)}
⋮----
response = client.get(
⋮----
profile_data = response.json()
profile_json_str = json.dumps(profile_data, indent=2)
summary_prompt = (
⋮----
summary = ask(prompt=summary_prompt)
⋮----
msg = f"Failed to retrieve data from Proxycurl: {e!s}"
⋮----
msg = f"Failed to connect to Proxycurl: {e!s}"
⋮----
msg = f"An unexpected error occurred: {e!s}"
⋮----
"""Handles the web search action."""
api_key = api_keys.search_api_key
⋮----
msg = "SEARCH_API_KEY is required for web search but not found."
⋮----
brave_search_api_url = "https://api.search.brave.com/res/v1/web/search"
headers = {"X-Subscription-Token": api_key, "Accept": "application/json"}
api_params = {"q": params.query}
⋮----
search_results = response.json()
results_json_str = json.dumps(search_results, indent=2)
⋮----
msg = f"Failed to retrieve data from Search API: {e!s}"
⋮----
msg = f"Failed to connect to Search API: {e!s}"
⋮----
def process_data(config: ActionConfig, *, debug: bool = False) -> dict[str, Any]
⋮----
"""Process the input data according to the provided action configuration.
    Args:
        config: The Pydantic model containing action type, parameters, and API keys.
        debug: Enable debug mode.
    Returns:
        Processed data as a dictionary.
    Raises:
        ValueError: If configuration is invalid or action type is unsupported.
    """
⋮----
msg = "Invalid parameters for 'enrich_person' action."
⋮----
msg = "Invalid parameters for 'search_web' action."
⋮----
msg = f"Unsupported action type: {config.action_type}"
⋮----
#
# The main() function and its associated if __name__ == "__main__": block
# have been moved to examples/run_action_config_example.py
# to keep this file focused on library code.
</file>

<file path="tests/test_mallmo.py">
"""Test suite for twat_llm.mallmo."""
⋮----
import llm  # Import the llm module itself for llm.Attachment and llm.UnknownModelError
# Module to test
from twat_llm import mallmo  # Assuming mallmo is now part of twat_llm package
⋮----
# _extract_middle_frame, # Removed as part of cv2 dependency removal
⋮----
# A known good small PNG (1x1 pixel, black) as bytes
# (base64: iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=)
BLACK_PIXEL_PNG_BYTES = b"\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00\x01\x00\x00\x00\x01\x08\x04\x00\x00\x00\xb5\x1c\x0c\x02\x00\x00\x00\x0bIDAT\x08\x99c`\x00\x00\x00\x06\x00\x01\x00\x00 \xae\xce\xbf\x00\x00\x00\x00IEND\xaeB`\x82"
# --- Test Helper Functions ---
def test_resize_image()
⋮----
"""Test image resizing functionality."""
# Create a dummy PIL Image object
img = Image.new("RGB", (100, 100), color="red")
resized_bytes = _resize_image(img, max_size=(50, 50))
⋮----
# Further checks could involve decoding the bytes and checking dimensions
# For simplicity, we're just checking if it returns bytes.
# Test with RGBA image
img_rgba = Image.new("RGBA", (100, 100), color=(255, 0, 0, 128))
resized_bytes_rgba = _resize_image(img_rgba, max_size=(50, 50))
⋮----
# Check if it converted to RGB (JPEG doesn't support alpha)
img_from_bytes = Image.open(io.BytesIO(resized_bytes_rgba))
⋮----
# Tests for _extract_middle_frame removed as the function itself was removed.
# - test_extract_middle_frame_success
# - test_extract_middle_frame_open_fail
# - test_extract_middle_frame_read_fail
⋮----
@patch("twat_llm.mallmo.Image.open")
@patch("twat_llm.mallmo._resize_image")
def test_prepare_media_image(mock_resize, mock_image_open)
⋮----
"""Test preparing an image file."""
mock_img_instance = MagicMock(spec=Image.Image)
⋮----
result = _prepare_media(Path("test.jpg"))
⋮----
def test_prepare_media_video_unsupported()
⋮----
"""Test that _prepare_media raises an error for video files (no longer supported)."""
⋮----
def test_prepare_media_unsupported_type()
⋮----
"""Test preparing an unsupported file type."""
⋮----
@patch("pathlib.Path.is_file", return_value=False)
@patch("twat_llm.mallmo.Image.open", side_effect=FileNotFoundError("File not found"))
def test_prepare_media_file_not_found(mock_image_open, mock_is_file)
⋮----
"""Test preparing a non-existent file."""
⋮----
# --- Test Core LLM Interaction ---
⋮----
@patch("twat_llm.mallmo.llm.get_model")
def test_try_model_success(mock_get_model)
⋮----
"""Test successful model invocation."""
mock_llm_model_instance = MagicMock(spec=llm.Model)
mock_response = MagicMock(spec=llm.Response)
⋮----
result = _try_model("Test prompt", "test-model")
⋮----
@patch("twat_llm.mallmo.llm.get_model")
def test_try_model_failure_unknown_model(mock_get_model)
⋮----
"""Test model invocation failure due to unknown model."""
⋮----
@patch("twat_llm.mallmo.llm.get_model")
def test_try_model_failure_prompt_error(mock_get_model)
⋮----
"""Test model invocation failure during prompt execution."""
⋮----
@patch("twat_llm.mallmo.llm.get_model")
def test_try_model_multimodal_check(mock_get_model)
⋮----
"""Test model invocation with multimodal check."""
⋮----
mock_attachment = MagicMock(spec=llm.Attachment)
⋮----
@patch("twat_llm.mallmo._try_model")
def test_ask_success(mock_try_model)
⋮----
"""Test ask function with a single successful model."""
⋮----
result = ask("Hello", model_ids=["model1"])
⋮----
@patch("twat_llm.mallmo._try_model")
def test_ask_fallback(mock_try_model)
⋮----
"""Test ask function with model fallback."""
⋮----
result = ask("Hello", model_ids=["model1", "model2"])
⋮----
@patch("twat_llm.mallmo._try_model", side_effect=ModelInvocationError("Model failed"))
def test_ask_all_models_fail(mock_try_model)
⋮----
"""Test ask function when all models fail."""
⋮----
# Each model is tried (DEFAULT_RETRY_ATTEMPTS + 1) times.
# Since _try_model itself is mocked here and raises immediately,
# it will be called once for each model in the list.
⋮----
@patch("twat_llm.mallmo.llm.Attachment")  # Mock the Attachment class itself
@patch("twat_llm.mallmo._prepare_media", return_value=b"imagedata")
@patch("twat_llm.mallmo._try_model")
@patch("twat_llm.mallmo.llm.Attachment")  # Mock the Attachment class itself
def test_ask_with_media(mock_attachment_class, mock_try_model, mock_prepare_media)
⋮----
"""Test ask function with media paths."""
⋮----
media_path = Path("dummy.jpg")
mock_attachment_instance = MagicMock()
⋮----
result = ask("Describe this image", media_paths=[media_path])
⋮----
def test_process_step_string()
⋮----
"""Test _process_step with a string (prompt)."""
⋮----
result = _process_step("Hello", "initial_data")
⋮----
def test_process_step_function()
⋮----
"""Test _process_step with a callable."""
mock_func = MagicMock(return_value="Function output")
result = _process_step(mock_func, "initial_data")
⋮----
def test_process_step_tuple()
⋮----
"""Test _process_step with a tuple (processor, kwargs)."""
mock_func = MagicMock(return_value="Function output with kwargs")
kwargs = {"key": "value"}
result = _process_step((mock_func, kwargs), "initial_data")
⋮----
def test_process_step_invalid_type()
⋮----
"""Test _process_step with an invalid type."""
⋮----
_process_step(123, "initial_data")  # type: ignore
def test_process_step_invalid_tuple_format()
⋮----
"""Test _process_step with an invalid tuple format."""
⋮----
_process_step(("prompt", "not_a_dict"), "initial_data")  # type: ignore
def test_ask_chain_simple()
⋮----
"""Test ask_chain with simple string prompts."""
⋮----
):  # Add **kwargs to match 'ask' signature
⋮----
steps = ["Step1", "Step2"]
result = ask_chain("Initial", steps)
⋮----
@patch("twat_llm.mallmo.ProcessPoolExecutor")
def test_ask_batch_success(mock_process_pool_executor)
⋮----
"""Test ask_batch successful processing."""
prompts = ["prompt1", "prompt2"]
expected_responses = ["response1", "response2"]
mock_executor_instance = MagicMock()
⋮----
)  # map returns an iterator
⋮----
responses = ask_batch(prompts, model_ids=["test-model"])
⋮----
# Check arguments passed to map
map_args = mock_executor_instance.map.call_args[0]
⋮----
# Convert generator to list for comparison
⋮----
def test_ask_batch_empty_prompts()
⋮----
"""Test ask_batch with no prompts."""
responses = ask_batch([])
⋮----
@patch("twat_llm.mallmo.ProcessPoolExecutor")
def test_ask_batch_processing_error_in_worker(mock_process_pool_executor)
⋮----
"""Test ask_batch when a worker in ProcessPoolExecutor raises an error."""
⋮----
# Simulate one of the map calls raising an exception
def map_side_effect(func, iterables)
⋮----
results = []
⋮----
if item[0] == "prompt2":  # Let's say processing prompt2 fails
msg = "Worker failed"
⋮----
)  # More direct way to test this
⋮----
# --- CLI Tests ---
⋮----
@patch("twat_llm.mallmo.ask")
def test_cli_simple_prompt(mock_ask_func, capsys)
⋮----
"""Test CLI with a simple prompt."""
⋮----
captured = capsys.readouterr()
⋮----
@patch("twat_llm.mallmo.ask_batch")
def test_cli_batch_prompt_file(mock_ask_batch, capsys, tmp_path)
⋮----
"""Test CLI with batch prompts from a file."""
prompt_file = tmp_path / "prompts.txt"
⋮----
@patch("twat_llm.mallmo.ask_batch")
def test_cli_batch_prompt_output_file(mock_ask_batch, tmp_path, capsys)
⋮----
"""Test CLI with batch prompts and output to a file."""
⋮----
output_file_path = tmp_path / "output.txt"
⋮----
# Capture print statements to stdout for verification
⋮----
def test_cli_no_prompt_or_batch_file(capsys)
⋮----
"""Test CLI when neither prompt nor batch_prompts_file is provided."""
⋮----
@patch("twat_llm.mallmo.ask", side_effect=LLMError("Test LLM Error"))
def test_cli_llm_error(mock_ask, capsys)
⋮----
"""Test CLI handling of LLMError."""
⋮----
@patch("twat_llm.mallmo.ask_batch")  # This mock is not used but good for consistency
@patch("builtins.open", new_callable=mock_open)
@patch("twat_llm.mallmo.ask_batch")  # This mock is not used but good for consistency
def test_cli_batch_file_read_error(mock_ask_batch, mock_file_open, capsys)
⋮----
"""Test CLI with batch prompts file that causes a read error."""
⋮----
@patch("twat_llm.mallmo.ask_batch")
def test_cli_batch_empty_file(mock_ask_batch, capsys, tmp_path)
⋮----
"""Test CLI with an empty batch prompts file."""
prompt_file = tmp_path / "empty_prompts.txt"
prompt_file.write_text("")  # Empty file
# The cli function exits with 0 if the batch file is empty but valid.
# We need to capture sys.exit to test this.
</file>

<file path="tests/test_twat_llm.py">
"""Test suite for twat_llm."""
⋮----
import pydantic  # Added for ValidationError
import httpx  # Added for HTTPStatusError and RequestError
# Assuming your Pydantic models and process_data are in src.twat_llm.twat_llm
# Adjust the import path if your structure is different after hatch build/install
⋮----
import twat_llm  # Moved import to top-level
def test_version()
⋮----
"""Verify package exposes version."""
# import twat_llm # Moved to top-level
⋮----
# --- Pydantic Model Tests ---
def test_action_config_enrich_person_valid()
⋮----
"""Test valid ActionConfig for person enrichment."""
config_data = {
config = ActionConfig(**config_data)
⋮----
def test_action_config_search_web_valid()
⋮----
"""Test valid ActionConfig for web search."""
config_data = {"action_type": "search_web", "parameters": {"query": "test query"}}
⋮----
def test_action_config_invalid_action_type()
⋮----
"""Test ActionConfig with an invalid action type."""
⋮----
with pytest.raises(ValueError):  # Pydantic raises ValueError for invalid Literal
⋮----
def test_action_config_enrich_person_missing_params()
⋮----
"""Test ActionConfig for enrich_person with missing required parameters (if any)."""
# PersonEnrichmentParams has all optional fields for now, so this might pass.
# If fields become mandatory, this test would need adjustment.
config_data = {"action_type": "enrich_person", "parameters": {}}
⋮----
def test_action_config_search_web_missing_query()
⋮----
"""Test ActionConfig for search_web with missing query."""
⋮----
"parameters": {},  # Missing 'query'
⋮----
):  # Pydantic raises ValidationError for missing fields
⋮----
# --- ApiKeySettings Tests ---
def test_api_key_settings_load_from_env(monkeypatch)
⋮----
"""Test ApiKeySettings loads from environment variables."""
⋮----
settings = ApiKeySettings()
⋮----
def test_api_key_settings_no_env()
⋮----
"""Test ApiKeySettings when no environment variables are set."""
# Assuming no relevant env vars are set by default in the test environment
⋮----
# --- process_data Tests (to be expanded) ---
# Placeholder for further tests - we will build these out
# For now, the structure for process_data tests will be set up.
⋮----
"""Test successful person enrichment."""
⋮----
# Mock Proxycurl API response
mock_response = MagicMock()
⋮----
# Configure the mock client instance
mock_client_instance = mock_http_client.return_value.__enter__.return_value
⋮----
# Mock mallmo.ask response
⋮----
config = ActionConfig(
result = process_data(config)
⋮----
# Ensure the prompt sent to mallmo.ask is reasonable
call_kwargs = mock_mallmo_ask.call_args.kwargs
⋮----
)  # Check if some profile data is in the prompt
⋮----
def test_process_data_enrich_person_missing_api_key(monkeypatch)
⋮----
"""Test person enrichment when PROXYCURL_API_KEY is missing."""
monkeypatch.delenv("PROXYCURL_API_KEY", raising=False)  # Ensure it's not set
⋮----
),  # type: ignore
⋮----
def test_process_data_enrich_person_missing_linkedin_url(monkeypatch)
⋮----
"""Test person enrichment when LinkedIn URL is missing (as per MVP)."""
⋮----
parameters=PersonEnrichmentParams(name="John Doe"),  # No URL
⋮----
@patch("twat_llm.twat_llm.httpx.Client")
def test_process_data_enrich_person_api_http_error(mock_http_client, monkeypatch)
⋮----
"""Test person enrichment when Proxycurl API returns an HTTP error."""
⋮----
mock_response.status_code = 401  # Unauthorized
⋮----
@patch("twat_llm.twat_llm.httpx.Client")
def test_process_data_enrich_person_api_request_error(mock_http_client, monkeypatch)
⋮----
"""Test person enrichment when there's a request error (e.g., network)."""
⋮----
"""Test person enrichment when mallmo.ask raises an error."""
⋮----
):  # General error wrapping
⋮----
# --- Tests for search_web action ---
⋮----
"""Test successful web search."""
⋮----
)  # Check if some search data is in the prompt
⋮----
def test_process_data_search_web_missing_api_key(monkeypatch)
⋮----
"""Test web search when SEARCH_API_KEY is missing."""
⋮----
@patch("twat_llm.twat_llm.httpx.Client")
def test_process_data_search_web_api_http_error(mock_http_client, monkeypatch)
⋮----
"""Test web search when Search API returns an HTTP error."""
⋮----
mock_response.status_code = 403  # Forbidden
⋮----
@patch("twat_llm.twat_llm.httpx.Client")
def test_process_data_search_web_api_request_error(mock_http_client, monkeypatch)
⋮----
"""Test web search when there's a request error."""
⋮----
"""Test web search when mallmo.ask raises an error."""
</file>

<file path="cleanup.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = [
#   "ruff>=0.9.6",
#   "pytest>=8.3.4",
#   "mypy>=1.15.0",
# ]
# ///
# this_file: cleanup.py
"""
Cleanup tool for managing repository tasks and maintaining code quality.
This script provides a comprehensive set of commands for repository maintenance:
When to use each command:
- `cleanup.py status`: Use this FIRST when starting work to check the current state
  of the repository. It shows file structure, git status, and runs all code quality
  checks. Run this before making any changes to ensure you're starting from a clean state.
- `cleanup.py venv`: Run this when setting up the project for the first time or if
  your virtual environment is corrupted/missing. Creates a new virtual environment
  using uv.
- `cleanup.py install`: Use after `venv` or when dependencies have changed. Installs
  the package and all development dependencies in editable mode.
- `cleanup.py update`: Run this when you've made changes and want to commit them.
  It will:
  1. Show current status (like `status` command)
  2. Stage and commit any changes with a generic message
  Use this for routine maintenance commits.
- `cleanup.py push`: Run this after `update` when you want to push your committed
  changes to the remote repository.
Workflow Example:
1. Start work: `cleanup.py status`
2. Make changes to code
3. Commit changes: `cleanup.py update`
4. Push to remote: `cleanup.py push`
The script maintains a CLEANUP.txt file that records all operations with timestamps.
It also includes content from README.md at the start and TODO.md at the end of logs
for context.
Required Files:
- LOG.md: Project changelog
- README.md: Project documentation
- TODO.md: Pending tasks and future plans
"""
⋮----
# Configuration
IGNORE_PATTERNS = [
REQUIRED_FILES = ["LOG.md", ".cursor/rules/0project.mdc", "TODO.md"]
LOG_FILE = Path("CLEANUP.txt")
# Ensure we're working from the script's directory
⋮----
def new() -> None
⋮----
"""Remove existing log file."""
⋮----
def prefix() -> None
⋮----
"""Write README.md content to log file."""
readme = Path(".cursor/rules/0project.mdc")
⋮----
content = readme.read_text()
⋮----
def suffix() -> None
⋮----
"""Write TODO.md content to log file."""
todo = Path("TODO.md")
⋮----
content = todo.read_text()
⋮----
def log_message(message: str) -> None
⋮----
"""Log a message to file and console with timestamp."""
timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
log_line = f"{timestamp} - {message}\n"
⋮----
def run_command(cmd: list[str], check: bool = True) -> subprocess.CompletedProcess
⋮----
"""Run a shell command and return the result."""
⋮----
result = subprocess.run(
⋮----
shell=False,  # Explicitly set shell=False for security
⋮----
def check_command_exists(cmd: str) -> bool
⋮----
"""Check if a command exists in the system."""
⋮----
class Cleanup
⋮----
"""Main cleanup tool class."""
def __init__(self) -> None
def _print_header(self, message: str) -> None
⋮----
"""Print a section header."""
⋮----
def _check_required_files(self) -> bool
⋮----
"""Check if all required files exist."""
missing = False
⋮----
missing = True
⋮----
def _generate_tree(self) -> None
⋮----
"""Generate and display tree structure of the project."""
⋮----
# Create/overwrite the file with YAML frontmatter
rules_dir = Path(".cursor/rules")
⋮----
# Get tree output
tree_result = run_command(
tree_text = tree_result.stdout
# Write frontmatter and tree output to file
⋮----
# Log the contents
⋮----
def _git_status(self) -> bool
⋮----
"""Check git status and return True if there are changes."""
result = run_command(["git", "status", "--porcelain"], check=False)
⋮----
def _venv(self) -> None
⋮----
"""Create and activate virtual environment using uv."""
⋮----
# Activate the virtual environment
venv_path = self.workspace / ".venv" / "bin" / "activate"
⋮----
def _install(self) -> None
⋮----
"""Install package in development mode with all extras."""
⋮----
def _run_checks(self) -> None
⋮----
"""Run code quality checks using ruff and pytest."""
⋮----
# Run ruff checks
⋮----
# Run type checks
⋮----
# Run tests
⋮----
def status(self) -> None
⋮----
"""Show current repository status: tree structure, git status, and run checks."""
prefix()  # Add README.md content at start
⋮----
# Check required files
⋮----
# Show tree structure
⋮----
# Show git status
result = run_command(["git", "status"], check=False)
⋮----
# Run additional checks
⋮----
suffix()  # Add TODO.md content at end
def venv(self) -> None
⋮----
"""Create and activate virtual environment."""
⋮----
def install(self) -> None
⋮----
"""Install package with all extras."""
⋮----
def update(self) -> None
⋮----
"""Show status and commit any changes if needed."""
# First show current status
⋮----
# Then handle git changes if any
⋮----
# Add all changes
⋮----
# Commit changes
commit_msg = "Update repository files"
⋮----
def push(self) -> None
⋮----
"""Push changes to remote repository."""
⋮----
"""Combine repository files into a single text file.
    Args:
        compress: Whether to compress whitespace in output
        remove_empty_lines: Whether to remove empty lines
        ignore_patterns: Comma-separated glob patterns of files to ignore
        output_file: Output file path
    """
⋮----
# Build command
cmd = ["npx", "repomix"] # Added npx
⋮----
# Run repomix
⋮----
def print_usage() -> None
⋮----
"""Print usage information."""
⋮----
def main() -> NoReturn
⋮----
"""Main entry point."""
new()  # Clear log file
⋮----
command = sys.argv[1]
cleanup = Cleanup()
⋮----
sys.exit(0)  # Ensure we exit with a status code
</file>

<file path="pyproject.toml">
# this_file: pyproject.toml
#==============================================================================
# TWAT_LLM PACKAGE CONFIGURATION
# This pyproject.toml defines the package metadata, dependencies, build system,
# and development environment for the twat_llm package.
#==============================================================================
#------------------------------------------------------------------------------
# BUILD SYSTEM CONFIGURATION
# Defines the tools required to build the package and the build backend.
#------------------------------------------------------------------------------
[build-system]
requires = [
    "hatchling>=1.27.0", # Core build backend for Hatch
    "hatch-vcs>=0.4.0", # Version Control System plugin for Hatch

]
build-backend = "hatchling.build" # Use Hatchling as the build backend

#------------------------------------------------------------------------------
# HATCH BUILD CONFIGURATION
# Configures the build process, specifying which packages to include and
# how to handle versioning.
#------------------------------------------------------------------------------
[tool.hatch.build.targets.wheel]
packages = ["src/twat_llm"]

#------------------------------------------------------------------------------
# PROJECT METADATA
# Core package information used by PyPI and package managers.
#------------------------------------------------------------------------------
[project]
name = "twat-llm"
dynamic = ["version"] # Version is determined dynamically from VCS
description = "LLM integration for twat"
readme = "README.md"
requires-python = ">=3.10" # Minimum Python version required
license = "MIT"
keywords = ["llm", "ai", "nlp", "text-processing"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Programming Language :: Python",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: Implementation :: CPython",
    "Programming Language :: Python :: Implementation :: PyPy",
    "Operating System :: OS Independent",
    "License :: OSI Approved :: MIT License",
    "Intended Audience :: Developers",
]
# Runtime Dependencies
dependencies = [
    "twat>=1.8.1", # Main twat package
    "pydantic>=2.0.0", # For data validation and settings management
    "pydantic-settings>=2.0.0", # For settings management (e.g., .env files)
    "rich>=13.0.0", # For rich text and beautiful formatting in the terminal
    "httpx>=0.25.0", # HTTP client for making API requests
    "llm>=0.8", # LLM library by Simon Willison
    "Pillow>=9.0.0", # For image processing (PIL)
    "tenacity>=8.0.0", # For retrying operations
    # "opencv-python-headless>=4.0.0" # Removed for MVP streamlining; was for video processing
]

# Project Authors
[[project.authors]]
name = "Adam Twardoch"
email = "adam+github@twardoch.com"

# Project URLs
[project.urls]
Documentation = "https://github.com/twardoch/twat-llm#readme"
Issues = "https://github.com/twardoch/twat-llm/issues"
Source = "https://github.com/twardoch/twat-llm"

#------------------------------------------------------------------------------
# OPTIONAL DEPENDENCIES
# Additional dependencies for optional features, development, and testing.
#------------------------------------------------------------------------------
[project.optional-dependencies]
# Testing tools and frameworks
test = [
    "pytest>=8.3.4",
    "pytest-cov>=6.0.0",
    "pytest-xdist>=3.6.1", # For parallel test execution
    "pytest-benchmark[histogram]>=5.1.0", # For performance testing
]
# Development tools
dev = [
    "pre-commit>=4.1.0", # Git pre-commit hooks
    "ruff>=0.9.6", # Fast Python linter
    "mypy>=1.15.0", # Static type checker
    "types-Pillow", # Type stubs for Pillow
    # "types-tenacity", # Type stubs for tenacity - Removed temporarily to resolve uv issue
    # Consider adding cv2-stubs if specific typing issues persist with opencv
    # Consider adding types-fire if CLI typing issues persist
]
# All optional dependencies combined
all = [
    "twat>=1.8.1", # Main twat package
    "llm>=0.8",
    "Pillow>=9.0.0",
    "tenacity>=8.0.0",
    # "opencv-python-headless>=4.0.0", # Removed for MVP streamlining
]

#------------------------------------------------------------------------------
# HATCH CONFIGURATION
# Configuration for the Hatch build system and environments.
#------------------------------------------------------------------------------
[tool.hatch.version]
source = "vcs"

[tool.hatch.version.raw-options]
version_scheme = "post-release"

[tool.hatch.build.hooks.vcs]
version-file = "src/twat_llm/__version__.py"

[tool.hatch.envs.default]
dependencies = [
    "pytest", # Testing framework
    "pytest-cov", # Coverage reporting
    "mypy>=1.15.0", # Static type checker
    "ruff>=0.9.6", # Fast Python linter

]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/twat_llm --cov=tests {args:tests}"
type-check = "mypy src/twat_llm tests"
lint = ["ruff check src/twat_llm tests", "ruff format src/twat_llm tests"]

[[tool.hatch.envs.all.matrix]]
python = ["3.10", "3.11", "3.12"]

[tool.hatch.envs.lint]
detached = true # Run in isolated environment
dependencies = [
    "mypy>=1.15.0", # Static type checker
    "ruff>=0.9.6", # Fast Python linter

]

[tool.hatch.envs.lint.scripts]
typing = "mypy --install-types --non-interactive {args:src/twat_llm tests}"
style = ["ruff check {args:.}", "ruff format {args:.}"]
fmt = ["ruff format {args:.}", "ruff check --fix {args:.}"]
fix = ["ruff check --fix --unsafe-fixes {args:.}", "ruff format {args:.}"]
all = ["style", "typing"]

#------------------------------------------------------------------------------
# CODE QUALITY TOOLS
# Configuration for linting, formatting, and code quality enforcement.
#------------------------------------------------------------------------------
[tool.ruff]
target-version = "py310"
line-length = 88

[tool.ruff.lint]
extend-select = [
    "A", # flake8-builtins
    "ARG", # flake8-unused-arguments
    "B", # flake8-bugbear
    "C", # flake8-comprehensions
    "DTZ", # flake8-datetimez
    "E", # pycodestyle errors
    "EM", # flake8-errmsg
    "F", # pyflakes
    "FBT", # flake8-boolean-trap
    "I", # isort
    "ICN", # flake8-import-conventions
    "ISC", # flake8-implicit-str-concat
    "N", # pep8-naming
    "PLC", # pylint convention
    "PLE", # pylint error
    "PLR", # pylint refactor
    "PLW", # pylint warning
    "Q", # flake8-quotes
    "RUF", # Ruff-specific rules
    "S", # flake8-bandit
    "T", # flake8-debugger
    "TID", # flake8-tidy-imports
    "UP", # pyupgrade
    "W", # pycodestyle warnings
    "YTT", # flake8-2020

]
ignore = [
    "ARG001", # Unused function argument
    "E501", # Line too long
    "I001", # Import block formatting

]

[tool.ruff.lint.isort]
known-first-party = ["twat_llm"] # Treat as first-party imports for sorting

[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = "all" # Ban all relative imports for consistency

[tool.ruff.lint.per-file-ignores]
# Tests can use magic values, assertions, and relative imports
"tests/**/*" = [
    "PLR2004", # Allow magic values in tests for readability
    "S101", # Allow assertions in tests
    "TID252"
    # Allow relative imports in tests for convenience
]

#------------------------------------------------------------------------------
# MYPY CONFIGURATION
# Configuration for type checking with mypy.
#------------------------------------------------------------------------------
[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

#------------------------------------------------------------------------------
# COVERAGE CONFIGURATION
# Settings for test coverage measurement and reporting.
#------------------------------------------------------------------------------
[tool.coverage.run]
source_pkgs = ["twat_llm", "tests"]
branch = true
parallel = true
omit = ["src/twat_llm/__about__.py"]

[tool.coverage.paths]
twat_llm = ["src/twat_llm", "*/twat-llm/src/twat_llm"]
tests = ["tests", "*/twat-llm/tests"]

[tool.coverage.report]
exclude_lines = ["no cov", "if __name__ == .__main__.:", "if TYPE_CHECKING:"]

#------------------------------------------------------------------------------
# TEST ENVIRONMENTS AND CONFIGURATION
# Settings for test execution and benchmarking.
#------------------------------------------------------------------------------
[tool.hatch.envs.test]
dependencies = [
  "twat-llm[test]",
]

[tool.hatch.envs.test.scripts]
test = "python -m pytest -n auto {args:tests}"
test-cov = "python -m pytest -n auto --cov-report=term-missing --cov-config=pyproject.toml --cov=src/twat_llm --cov=tests {args:tests}"
bench = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only"
bench-save = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json"

[tool.pytest.ini_options]
markers = ["benchmark: marks tests as benchmarks (select with '-m benchmark')"]
addopts = "-v -p no:briefcase"
testpaths = ["tests"]
python_files = ["test_*.py"]
filterwarnings = ["ignore::DeprecationWarning", "ignore::UserWarning"]
asyncio_mode = "auto"

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = "file"
save-data = true
compare = [
    "min", # Minimum time
    "max", # Maximum time
    "mean", # Mean time
    "stddev", # Standard deviation
    "median", # Median time
    "iqr", # Inter-quartile range
    "ops", # Operations per second
    "rounds", # Number of rounds

]
</file>

</files>
